Project Path: /Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian

Source Tree:

```
simian
├── worker.py
├── server.py
├── object.py
├── render.py
├── batch.py
├── tests
│   ├── __run__.py
│   ├── postprocessing_test.py
│   ├── background_test.py
│   ├── transform_test.py
│   ├── combiner_test.py
│   ├── camera_test.py
│   ├── new_camera_test.py
│   ├── worker_test.py
│   ├── object_test.py
│   └── batch_test.py
├── background.py
├── __init__.py
├── scene.py
├── prompts.py
├── camera.py
├── distributed.py
├── transform.py
├── combiner.py
├── postprocessing.py
└── vendor
    ├── __init__.py
    └── objaverse.py

```

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/worker.py`:

```````py
import json
import logging
import os
import sys
import subprocess
import boto3
import shlex
import time
from typing import Any, Dict

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)


def run_job(
    combination_indeces: int,
    combinations: Dict[str, Any],
    width: int,
    height: int,
    output_dir: str,
    hdri_path: str,
    upload_dest: str,
    start_frame: int = 0,
    end_frame: int = 65
) -> None:
    """
    Run a rendering job with the specified combination index and settings.

    Args:
        combination_index (int): The index of the combination to render.
        combination (Dict[str, Any]): The combination dictionary.
        width (int): The width of the rendered output.
        height (int): The height of the rendered output.
        output_dir (str): The directory to save the rendered output.
        hdri_path (str): The path to the HDRI file.
        start_frame (int, optional): The starting frame number. Defaults to 0.
        end_frame (int, optional): The ending frame number. Defaults to 65.

    Returns:
        None
    """
    combination_strings = []
    for combo in combinations:
        combination_string = json.dumps(combo)
        combination_string = shlex.quote(combination_string)
        combination_strings.append(combination_string)

    # upload to Hugging Face
    if upload_dest == "hf":

        # create output directory, add time to name so each new directory is unique
        output_dir += str(time.time())
        os.makedirs(output_dir, exist_ok=True)

        # render images in batches (batches to handle rate limiting of uploads)
        batch_size = len(combination_indeces)
        for i in range(batch_size):

            args = f" --width {width} --height {height} --combination_index {combination_indeces[i]}"
            args += f" --output_dir {output_dir}"
            args += f" --hdri_path {hdri_path}"
            args += f" --start_frame {start_frame} --end_frame {end_frame}"
            args += f" --combination {combination_strings[i]}"

            command = f"{sys.executable} -m simian.render -- {args}"
            logger.info(f"Worker running simian.render")

            subprocess.run(["bash", "-c", command], check=True)

        distributask.upload_directory(output_dir)

    # upload to aws s3 bucket
    else:

        os.makedirs(output_dir, exist_ok=True)

        combination_index = combination_indeces[0]
        combination = combination_strings[0]

        args = f" --width {width} --height {height} --combination_index {combination_index}"
        args += f" --output_dir {output_dir}"
        args += f" --hdri_path {hdri_path}"
        args += f" --start_frame {start_frame} --end_frame {end_frame}"
        args += f" --combination {combination}"

        command = f"{sys.executable} -m simian.render -- {args}"
        logger.info(f"Worker running simian.render")

        subprocess.run(["bash", "-c", command], check=True)


        file_location = f"{output_dir}/{combination_index}.mp4"

        file_upload_name = f"{combination_index:05d}.mp4"

        s3_client = boto3.client('s3')
        s3_client.upload_file(file_location, os.getenv("S3_BUCKET_NAME"), file_upload_name)

    return "Task completed"


# only run this is this file was started by celery or run directly
# check if celery is in sys.argv, it could be sys.argv[0] but might not be

if __name__ == "__main__" or any("celery" in arg for arg in sys.argv):
    from distributask.distributask import create_from_config

    distributask = create_from_config()
    distributask.register_function(run_job)

    celery = distributask.app

```````

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/server.py`:

```````py
import os
import json
import chromadb
from chromadb.utils import embedding_functions
from rich.progress import Progress, TextColumn, BarColumn, TimeRemainingColumn, MofNCompleteColumn
from rich.console import Console
from rich.panel import Panel
from sentence_transformers import SentenceTransformer
from chromadb.config import Settings


def initialize_chroma_db(reset_hdri=False, reset_textures=False):
    """
    Initialize the Chroma database with the provided data files.

    Args:
        reset_hdri (bool): Whether to reset and reprocess the HDRI backgrounds.
        reset_textures (bool): Whether to reset and reprocess the textures.

    Returns:
        chroma_client (chromadb.PersistentClient): The initialized Chroma client.
    """
    db_path = "./chroma_db"
    
    # Check if the database directory exists
    if not os.path.exists(db_path):
        print("Database not found. Creating new database.")
        os.makedirs(db_path)
    
    # Initialize Chroma client
    chroma_client = chromadb.PersistentClient(path=db_path, settings=Settings(anonymized_telemetry=False))

    # Create or get collections for each data type
    object_collection = chroma_client.get_or_create_collection(name="object_captions")
    hdri_collection = chroma_client.get_or_create_collection(name="hdri_backgrounds")
    texture_collection = chroma_client.get_or_create_collection(name="textures")

    # Process collections if they are empty
    def process_if_empty(collection, file_path, description):
        if collection.count() == 0:
            if os.path.exists(file_path):
                print(f"Processing {description}...")
                process_in_batches(file_path, collection, batch_size=1000)
            else:
                print(f"File not found: {file_path}. Please check the file path.")
        else:
            print(f"{description} already processed. Skipping.")

    # Adjusted paths to reflect the correct location of the datasets folder
    datasets_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "../datasets"))

    def reset_and_process(collection, file_path, description):
        print(f"Resetting and reprocessing {description}...")
        # Get all IDs in the collection
        all_ids = collection.get(include=['embeddings'])['ids']
        if all_ids:
            # Delete all documents if there are any
            collection.delete(ids=all_ids)
        # Process the data
        process_in_batches(file_path, collection, batch_size=1000)
        
     # Reset and reprocess HDRI if requested
    if reset_hdri:
        reset_and_process(hdri_collection, os.path.join(datasets_path, 'hdri_data.json'), "HDRI backgrounds")
    elif hdri_collection.count() == 0:
        process_in_batches(os.path.join(datasets_path, 'hdri_data.json'), hdri_collection, batch_size=1000)

    # Reset and reprocess textures if requested
    if reset_textures:
        reset_and_process(texture_collection, os.path.join(datasets_path, 'texture_data.json'), "textures")
    elif texture_collection.count() == 0:
        process_in_batches(os.path.join(datasets_path, 'texture_data.json'), texture_collection, batch_size=1000)

    process_if_empty(object_collection, os.path.join(datasets_path, 'cap3d_captions.json'), "object captions")
    process_if_empty(hdri_collection, os.path.join(datasets_path, 'hdri_data.json'), "HDRI backgrounds")
    process_if_empty(texture_collection, os.path.join(datasets_path, 'texture_data.json'), "textures")

    # Check if collections are empty and process data if needed
    if object_collection.count() == 0:
        print("Processing object captions...")
        process_in_batches_objects('../datasets/cap3d_captions.json', object_collection, batch_size=1000)
    else:
        print("Object captions already processed. Skipping.")

    if hdri_collection.count() == 0:
        print("Processing HDRI backgrounds...")
        process_in_batches('../datasets/hdri_data.json', hdri_collection, batch_size=1000)
    else:
        print("HDRI backgrounds already processed. Skipping.")

    if texture_collection.count() == 0:
        print("Processing textures...")
        process_in_batches('../datasets/texture_data.json', texture_collection, batch_size=1000)
    else:
        print("Textures already processed. Skipping.")

    print("Database initialization complete.")

    return chroma_client


def process_in_batches_objects(file_path, collection, batch_size=1000):
    """
    Process the data in the specified file in batches and upsert it into the collection for objects.

    Args:
        file_path (str): The path to the file containing the data.
        collection (chromadb.Collection): The collection to upsert the data into.
        batch_size (int): The size of each batch for processing.
    
    Returns:
        None
    """
    sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name='all-MiniLM-L6-v2')
    console = Console()
    
    with open(file_path, 'r') as file:
        data = json.load(file)
    
    all_ids = list(data.keys())
    total_items = len(all_ids)

    with Progress(
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        MofNCompleteColumn(),
        TimeRemainingColumn(),
        TextColumn("Batch: {task.fields[current_batch]}"),
        console=console,
        expand=True
    ) as progress:
        task = progress.add_task(f"[green]Processing {file_path}", total=total_items, current_batch="0")

        for i in range(0, total_items, batch_size):
            batch_ids = all_ids[i:i+batch_size]
            batch_documents = [data[id] for id in batch_ids]
            
            # Compute embeddings for the batch using the embedding function
            embeddings = sentence_transformer_ef(batch_documents)
            
            # Upsert the batch into the collection
            collection.upsert(
                ids=batch_ids,
                embeddings=embeddings,
                documents=batch_documents
            )

            # Update progress
            progress.update(task, advance=len(batch_ids), current_batch=f"{i+1}-{min(i+batch_size, total_items)}")

    console.print(Panel.fit(f"Data processing complete for {file_path}!", border_style="green"))


def process_in_batches(file_path, collection, batch_size=1000):
    """
    Process the data in the specified file in batches and upsert it into the collection.

    Args:
        file_path (str): The path to the file containing the data.
        collection (chromadb.Collection): The collection to upsert the data into.
        batch_size (int): The size of each batch for processing.
    
    Returns:
        None
    """
    
    sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name='all-MiniLM-L6-v2')

    console = Console()
    
    with open(file_path, 'r') as file:
        data = json.load(file)
    
    all_ids = list(data.keys())
    total_items = len(all_ids)

    def convert_to_chroma_compatible(value):
        if isinstance(value, (str, int, float, bool)):
            return value
        elif isinstance(value, list):
            return ', '.join(map(str, value))
        elif isinstance(value, dict):
            return json.dumps(value)
        else:
            return str(value)

    with Progress(
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        MofNCompleteColumn(),
        TimeRemainingColumn(),
        TextColumn("Batch: {task.fields[current_batch]}"),
        console=console,
        expand=True
    ) as progress:
        task = progress.add_task(f"[green]Processing {file_path}", total=total_items, current_batch="0")

        for i in range(0, total_items, batch_size):
            batch_ids = all_ids[i:i+batch_size]
            batch_documents = []
            batch_metadatas = []
            
            for id in batch_ids:
                item = data[id]
                if isinstance(item, str):
                    # For object captions, keep as is
                    batch_documents.append(item)
                    batch_metadatas.append(None)
                else:
                    # For HDRIs and textures
                    name = item.get('name', id)
                    categories = ' '.join(item.get('categories', []))
                    tags = ' '.join(item.get('tags', []))
                    
                    # Create a searchable document string
                    document = f"{name} {categories} {tags}".strip()
                    batch_documents.append(document)
                    
                    # Convert metadata to Chroma-compatible format
                    compatible_metadata = {k: convert_to_chroma_compatible(v) for k, v in item.items()}
                    batch_metadatas.append(compatible_metadata)
            
            # Compute embeddings for the batch using the embedding function
            embeddings = sentence_transformer_ef(batch_documents)
            
            # Upsert the batch into the collection
            collection.upsert(
                ids=batch_ids,
                embeddings=embeddings,
                documents=batch_documents,
                metadatas=batch_metadatas
            )

            # Update progress
            progress.update(task, advance=len(batch_ids), current_batch=f"{i+1}-{min(i+batch_size, total_items)}")

    console.print(Panel.fit(f"Data processing complete for {file_path}!", border_style="green"))


def query_collection(query, collection, n_results=2):
    """
    Query the specified collection with the given query and display the results.

    Args:
        query (str): The query string to search for.
        sentence_transformer_ef (embedding_functions.SentenceTransformerEmbeddingFunction): The SentenceTransformer embedding function.
        collection (chromadb.Collection): The collection to query.
        n_results (int): The number of results to display.
    
    Returns:
        dict: The query results.
    """

    model = SentenceTransformer('all-MiniLM-L6-v2')  # or another appropriate model
    sentence_transformer_ef = model.encode
    sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name='all-MiniLM-L6-v2')

    console = Console()
    query_embedding = sentence_transformer_ef([query])
    results = collection.query(
        query_embeddings=query_embedding,
        n_results=n_results,
        include=["metadatas", "documents", "distances"]
    )
    
    def parse_metadata_value(value):
        if isinstance(value, str):
            if value.startswith('[') and value.endswith(']'):
                return value.strip('[]').split(', ')
            elif value.startswith('{') and value.endswith('}'):
                return json.loads(value)
        return value

    # Parse the metadata values
    for i, metadata in enumerate(results['metadatas'][0]):
        if metadata:
            results['metadatas'][0][i] = {k: parse_metadata_value(v) for k, v in metadata.items()}
    
    console.print(Panel(str(results), title=f"Query Results for {collection.name}", expand=False))
    return results
```````

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/object.py`:

```````py
import logging
from typing import List, Optional, Callable, Dict
import bpy
from mathutils import Vector

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)

"""
A dictionary mapping file extensions to Blender's import functions.
This allows for dynamic selection of the appropriate import function based on the file type.
Each key is a string representing the file extension, and the value is a callable Blender function.

Supported file types include:
- "obj": Imports Wavefront OBJ files.
- "glb": Imports GLB (binary glTF) files.
- "gltf": Imports glTF files.
- "usd": Imports Universal Scene Description (USD) files.
- "fbx": Imports Autodesk FBX files.
- "stl": Imports Stereolithography (STL) files.
- "usda": Imports ASCII format of Universal Scene Description files.
- "dae": Imports Collada (DAE) files.
- "ply": Imports Polygon File Format (PLY) files.
- "abc": Imports Alembic files.
- "blend": Appends or links from a Blender (.blend) file.
"""
IMPORT_FUNCTIONS: Dict[str, Callable] = {
    "obj": bpy.ops.import_scene.obj,
    "glb": bpy.ops.import_scene.gltf,
    "gltf": bpy.ops.import_scene.gltf,
    "usd": bpy.ops.import_scene.usd,
    "fbx": bpy.ops.import_scene.fbx,
    "stl": bpy.ops.import_mesh.stl,
    "usda": bpy.ops.import_scene.usda,
    "dae": bpy.ops.wm.collada_import,
    "ply": bpy.ops.import_mesh.ply,
    "abc": bpy.ops.wm.alembic_import,
    "blend": bpy.ops.wm.append,
}


def load_object(object_path: str) -> None:
    """
    Loads a model with a supported file extension into the scene.

    Args:
        object_path (str): Path to the model file.

    Raises:
        ValueError: If the file extension is not supported.

    Returns:
        None
    """
    file_extension = object_path.split(".")[-1].lower()
    if file_extension is None:
        raise ValueError(f"Unsupported file type: {object_path}")

    # load from existing import functions
    import_function = IMPORT_FUNCTIONS[file_extension]

    if file_extension == "blend":
        import_function(directory=object_path, link=False)
    elif file_extension in {"glb", "gltf"}:
        import_function(filepath=object_path, merge_vertices=True)
    else:
        import_function(filepath=object_path)


def delete_invisible_objects() -> None:
    """
    Deletes all invisible objects in the scene.

    Args:
        None

    Returns:
        None
    """
    bpy.ops.object.select_all(action="DESELECT")
    for obj in bpy.context.scene.objects:
        if obj.hide_select:
            continue
        if obj.hide_viewport or obj.hide_render:
            obj.hide_viewport = False
            obj.hide_render = False
            obj.hide_select = False
            obj.select_set(True)
    bpy.ops.object.delete()

    # Delete invisible collections
    invisible_collections = [col for col in bpy.data.collections if col.hide_viewport]
    for col in invisible_collections:
        bpy.data.collections.remove(col)


def get_hierarchy_bbox(obj) -> tuple[float, float]:
    """
    Calculate the bounding box of an object and its children.

    Args:
        obj (bpy.types.Object): The root object.

    Returns:
        tuple: A tuple containing the minimum and maximum coordinates of the bounding box.
    """
    # Ensure the object's matrix_world is updated
    bpy.context.view_layer.update()

    # Initialize min and max coordinates with extremely large and small values
    min_coord = [float("inf"), float("inf"), float("inf")]
    max_coord = [-float("inf"), -float("inf"), -float("inf")]

    # Function to update min and max coordinates
    def update_bounds(obj: bpy.types.Object) -> None:
        """
        Update the minimum and maximum coordinates based on the object's bounding box.

        Args:
            obj (bpy.types.Object): The object whose bounding box is to be updated.

        Returns:
            None
        """
        # Update the object's bounding box based on its world matrix
        bbox_corners = [obj.matrix_world @ Vector(corner) for corner in obj.bound_box]
        for corner in bbox_corners:
            for i in range(3):
                min_coord[i] = min(min_coord[i], corner[i])
                max_coord[i] = max(max_coord[i], corner[i])

    # Recursive function to process each object
    def process_object(obj: bpy.types.Object) -> None:
        """
        Recursively process each object and update bounding box coordinates.

        Args:
            obj (bpy.types.Object): The object to be processed.

        Returns:
            None
        """
        update_bounds(obj)
        for child in obj.children:
            process_object(child)

    # Start processing from the root object
    process_object(obj)

    return min_coord, max_coord


def remove_small_geometry(
    obj: bpy.types.Object, min_vertex_count: int = 10
) -> Optional[bpy.types.Object]:
    """
    Remove free-hanging geometry with fewer vertices than specified by separating by loose parts,
    deleting small ones, and re-joining them.

    Args:
        obj (bpy.types.Object): The object to process.
        min_vertex_count (int, optional): Minimum number of vertices required to keep a part of the mesh. Default is 10.

    Returns:
        bpy.types.Object or None: The processed object if successful, None otherwise.
    """
    # Ensure the object is a mesh
    if obj is not None and obj.type != "MESH":
        logger.info("Object is not a mesh.")
        return None

    # Make sure the object is active and we're in object mode
    bpy.context.view_layer.objects.active = obj
    obj.select_set(True)
    # select all children
    for child in obj.children:
        child.select_set(True)
    bpy.ops.object.mode_set(mode="OBJECT")

    # Separate by loose parts
    bpy.ops.mesh.separate(type="LOOSE")
    bpy.ops.object.mode_set(mode="OBJECT")

    # Deselect all to start clean
    bpy.ops.object.select_all(action="DESELECT")

    # Iterate over all new objects created by the separate operation
    for ob in bpy.context.selected_objects:
        # Re-select the object to make it active
        bpy.context.view_layer.objects.active = ob
        ob.select_set(True)

        # Check vertex count
        if len(ob.data.vertices) < min_vertex_count:
            # Delete the object if it doesn't meet the vertex count requirement
            bpy.ops.object.delete()

    # Optionally, re-join remaining objects if necessary
    bpy.ops.object.select_all(action="SELECT")
    bpy.ops.object.join()
    bpy.ops.object.mode_set(mode="OBJECT")

    return obj


def normalize_object_scale(
    obj: bpy.types.Object, scale_factor: float = 1.0
) -> bpy.types.Object:
    """
    Scales the object by a factor.

    Args:
        obj (bpy.types.Object): The object to scale.
        scale_factor (float, optional): The factor to scale the object by. Defaults to 1.0.

    Returns:
        bpy.types.Object: The scaled object.
    """
    # Get the bounding box of the object and its children
    bbox_min, bbox_max = get_hierarchy_bbox(obj)

    # Calculate the scale of the bounding box and scale the object if necessary
    bbox_dimensions = [bbox_max[i] - bbox_min[i] for i in range(3)]
    max_dimension = max(bbox_dimensions)
    scale = scale_factor / max_dimension
    obj.scale = (scale, scale, scale)
    # make sure object is active and apply the scale
    bpy.context.view_layer.objects.active = obj
    bpy.ops.object.transform_apply(location=False, rotation=False, scale=True)
    return obj


def get_meshes_in_hierarchy(obj: bpy.types.Object) -> List[bpy.types.Object]:
    """
    Recursively collects all mesh objects in the hierarchy starting from the given object.

    Args:
        obj (bpy.types.Object): The root object from which to start collecting mesh objects.

    Returns:
        List[bpy.types.Object]: A list of mesh objects found in the hierarchy.
    """
    # Initialize an empty list to store mesh objects
    meshes = []

    # Check if the current object is a mesh and add it to the list if it is
    if obj.type == "MESH":
        meshes.append(obj)

    # Initialize an empty list to store mesh objects from child objects
    new_meshes = []

    # Recursively call the function for each child object and collect their meshes
    for child in obj.children:
        # Add meshes from child objects to the list
        new_meshes += get_meshes_in_hierarchy(child)

    # Combine meshes from the current object and its children and return the list
    return meshes + new_meshes


def remove_blendshapes_from_hierarchy(obj: bpy.types.Object) -> None:
    """
    Recursively removes blendshapes from all models in the hierarchy under the given object.

    Args:
        obj (bpy.types.Object): The root object of the hierarchy.

    Returns:
        None
    """
    # Ensure context is correct
    bpy.ops.object.mode_set(mode="OBJECT")
    bpy.ops.object.select_all(action="DESELECT")

    def remove_blendshapes(obj):
        if obj.type == "MESH":
            # Select and make the mesh active
            bpy.context.view_layer.objects.active = obj
            obj.select_set(True)

            # Remove blendshapes
            if obj.data.shape_keys:
                obj.shape_key_clear()

    # Traverse the hierarchy and remove blendshapes
    def traverse_hierarchy(obj):
        remove_blendshapes(obj)
        for child in obj.children:
            traverse_hierarchy(child)

    # Start traversing from the given object
    traverse_hierarchy(obj)

    # Deselect everything to clean up
    bpy.ops.object.select_all(action="DESELECT")


def apply_and_remove_armatures():
    """
    Apply armature modifiers to meshes and remove armature objects.

    Args:
        None

    Returns:
        None
    """

    # Ensure context is correct
    bpy.ops.object.mode_set(mode="OBJECT")
    bpy.ops.object.select_all(action="DESELECT")

    # Iterate over all objects in the scene
    for obj in bpy.data.objects:
        # Check if the object is a mesh with an armature modifier
        if obj.type == "MESH":
            for modifier in obj.modifiers:
                if modifier.type == "ARMATURE" and modifier.object is not None:
                    try:
                        remove_blendshapes_from_hierarchy(obj)
                    except:
                        logger.info("Error removing blendshapes from object.")
                        # write a log file with the error
                        with open("error_log.txt", "a") as f:
                            f.write("Error removing blendshapes from object.\n")
                    # Select and make the mesh active
                    bpy.context.view_layer.objects.active = obj
                    obj.select_set(True)

                    # Apply the armature modifier
                    bpy.ops.object.modifier_apply(modifier=modifier.name)

                    # Deselect everything to clean up for the next iteration
                    bpy.ops.object.select_all(action="DESELECT")
    

def apply_all_modifiers(obj: bpy.types.Object):
    """
    Recursively apply all modifiers to the object and its children.

    Args:
        obj (bpy.types.Object): The root object.

    Return:
        None
    """

    # Traverse the hierarchy and apply all modifiers
    def recurse(obj):
        # Set the active object to the current object
        bpy.context.view_layer.objects.active = obj
        # Check if the object has modifiers
        if obj.modifiers:
            # Apply each modifier
            for modifier in obj.modifiers:
                bpy.ops.object.modifier_apply(modifier=modifier.name)

    # Recursively apply modifiers to children
    for child in obj.children:
        recurse(child)

    # Apply modifiers to the root object itself
    recurse(obj)


def optimize_meshes_in_hierarchy(obj: bpy.types.Object) -> None:
    """
    Recursively optimize meshes in the hierarchy by removing doubles and setting materials to double-sided.

    Args:
        obj (bpy.types.Object): The root object.

    Returns:
        None
    """

    if obj.type == "MESH":
        # Go into edit mode and select the mesh
        bpy.context.view_layer.objects.active = obj

        # Go to edit mode
        bpy.ops.object.mode_set(mode="EDIT")

        # Select all verts
        bpy.ops.mesh.select_all(action="SELECT")

        bpy.ops.mesh.remove_doubles(threshold=0.0001)
        angle_limit = 0.000174533 * 10

        # Go to lines mode
        bpy.ops.mesh.select_mode(type="EDGE")

        # Deselect all
        bpy.ops.mesh.select_all(action="DESELECT")

        # Select all edges
        bpy.ops.mesh.select_all(action="SELECT")

        # bpy.ops.mesh.dissolve_limited(angle_limit=angle_limit, use_dissolve_boundaries=True, delimit={'NORMAL', 'MATERIAL', 'SEAM', 'SHARP', 'UV'})

        # Set all materials to be double sided
        for slot in obj.material_slots:
            # slot.material.use_backface_culling = False
            if slot.material.blend_method == "BLEND":
                slot.material.blend_method = "HASHED"

        # Return to object mode
        bpy.ops.object.mode_set(mode="OBJECT")

    for child in obj.children:
        optimize_meshes_in_hierarchy(child)


def join_objects_in_hierarchy(obj: bpy.types.Object) -> None:
    """
    Joins a list of objects into a single object.

    Args:
        objects (List[bpy.types.Object]): List of objects to join.

    Returns:
        None
    """

    meshes = get_meshes_in_hierarchy(obj)

    # Select and activate meshes
    bpy.ops.object.select_all(action="DESELECT")
    for mesh in meshes:
        mesh.select_set(True)

    # Set the last selected mesh as active and check if it's valid for mode setting
    if meshes:
        bpy.context.view_layer.objects.active = meshes[0]
        if (
            bpy.context.view_layer.objects.active is not None
            and bpy.context.view_layer.objects.active.type == "MESH"
        ):
            # Use Context.temp_override() to create a temporary context override
            with bpy.context.temp_override(
                active_object=bpy.context.view_layer.objects.active,
                selected_objects=meshes,
            ):
                # Set the object mode to 'OBJECT' using the operator with the temporary context override
                bpy.ops.object.mode_set(mode="OBJECT")

                # Join meshes using the bpy.ops.object.join() operator with a custom context override
                if len(meshes) > 1:
                    bpy.ops.object.join()
                else:
                    logger.info("Not enough meshes to join.")
        else:
            logger.info("Active object is not a valid mesh.")
    else:
        logger.info("No meshes found to set as active.")


def get_terrain_height(location: Vector) -> float:
    """
    Get the height of the terrain at a specific location.
    Args:
        location (Vector): The location to get the height for.
    Returns:
        float: The height of the terrain at the specified location.
    """
    bpy.context.view_layer.update()
    ray_origin = Vector((location.x, location.y, 25))  # Ray origin below the location
    ray_direction = Vector((0, 0, -1))  # Ray direction downwards

    depsgraph = bpy.context.evaluated_depsgraph_get()
    hit_objects = ['OpaqueTerrain', 'OpaqueTerrain_fine']
    terrain_objects = [obj for obj in bpy.context.scene.objects if obj.name in hit_objects]

    result = False
    hit_location = None

    for obj in terrain_objects:
        if obj.hide_get():
            continue

        result_local, hit_location_local, normal, index, hit_obj, matrix = bpy.context.scene.ray_cast(
            depsgraph, ray_origin, ray_direction)

        if result_local and hit_obj.name in hit_objects:
            if hit_obj.name == 'OpaqueTerrain_fine':
                result = result_local
                hit_location = hit_location_local
                break
            if hit_obj.name == 'OpaqueTerrain' and not result:
                result = result_local
                hit_location = hit_location_local

    if result:
        return hit_location.z
    else:
        # logger.info("Ray did not hit any terrain")
        return 0.0  # Default to 0 if no intersection is found


def set_pivot_to_bottom(obj: bpy.types.Object) -> None:
    """
    Set the pivot of the object to the center of mass, and place it on the terrain surface.
    Args:
        obj (bpy.types.Object): The object to adjust.
    Returns:
        None
    """
    # Update the view layer to ensure the latest changes are considered
    bpy.context.view_layer.update()

    # Calculate the center of mass
    center_of_mass = obj.location

    # Calculate the bounding box bottom
    bbox_min = [obj.matrix_world @ Vector(corner) for corner in obj.bound_box][0]
    for corner in obj.bound_box:
        world_corner = obj.matrix_world @ Vector(corner)
        if world_corner.z < bbox_min.z:
            bbox_min = world_corner

    # Set origin to the center of mass
    bpy.ops.object.origin_set(type="ORIGIN_CENTER_OF_MASS", center="BOUNDS")

    # Get the terrain height at the object's bounding box minimum location
    terrain_height = get_terrain_height(bbox_min)
    # logger.info(f"Terrain height at object location: {terrain_height}")

    # Calculate the object's height
    obj_height = center_of_mass.z - bbox_min.z

    # Move the object to the terrain height plus its height
    obj.location.z = terrain_height + obj_height

    # Apply transformations
    bpy.ops.object.transform_apply(location=True, rotation=True, scale=True)
    # logger.info(f"Applied transformation to the object: {obj.location}")
 

def unparent_keep_transform(obj: bpy.types.Object) -> None:
    """
    Unparents an object but keeps its transform.

    Args:
        obj (bpy.types.Object): The object to unparent.

    Returns:
        None
    """
    # clear the parent object, but keep the transform
    bpy.ops.object.parent_clear(type="CLEAR_KEEP_TRANSFORM")
    # clear rotation and scale and apply
    bpy.ops.object.transform_apply(location=False, rotation=True, scale=True)


def delete_all_empties() -> None:
    """Deletes all empty objects in the scene.

    Args:
        None

    Returns:
        None
    """
    for obj in bpy.data.objects:
        if obj.type == "EMPTY":
            # if the object is hidden from selection, ignore
            if obj.hide_select:
                continue
            bpy.data.objects.remove(obj, do_unlink=True)


def lock_all_objects() -> None:
    """
    Locks all objects in the scene from selection and returns a list of these objects.

    Args:
        None

    Returns:
        None
    """
    locked_objects = []
    for obj in bpy.context.scene.objects:
        obj.hide_select = True
        locked_objects.append(obj)
    return locked_objects


def unlock_objects(objs: List[bpy.types.Object]) -> None:
    """
    Unlocks a given list of objects for selection.

    Args:
        objs (list): A list of Blender objects to be unlocked.

    Returns:
        None
    """
    for obj in objs:
        obj.hide_select = False

```````

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/render.py`:

```````py
import argparse
import json
import logging
import os
import ssl
import sys
import bpy
import random
from rich.console import Console

console = Console()
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)

ssl._create_default_https_context = ssl._create_unverified_context

from .camera import (
    create_camera_rig,
    position_camera,
    set_camera_animation,
    set_camera_settings,
)
from .transform import(
    find_largest_length, 
    place_objects_on_grid,
    apply_animation
)
from .object import (
    apply_all_modifiers,
    apply_and_remove_armatures,
    get_meshes_in_hierarchy,
    join_objects_in_hierarchy,
    load_object,
    lock_all_objects,
    normalize_object_scale,
    optimize_meshes_in_hierarchy,
    set_pivot_to_bottom,
    unlock_objects,
    unparent_keep_transform,
)
from .background import create_photosphere, set_background
from .scene import apply_stage_material, create_stage, initialize_scene
from .vendor import objaverse


def read_combination(combination_file: str, index: int = 0) -> dict:
    """
    Reads a specified camera combination from a JSON file.

    Args:
        combination_file (str): Path to the JSON file containing camera combinations.
        index (int): Index of the camera combination to read from the JSON file. Defaults to 0.

    Returns:
        dict: The camera combination data.
    """
    with open(combination_file, "r") as file:
        data = json.load(file)
        combinations_data = data["combinations"]
        return combinations_data[index]
    

def load_user_blend_file(user_blend_file):
    """
    Loads a user-specified Blender file as the base scene.

    Args:
        user_blend_file (str): Path to the user-specified Blender file.

    Returns:
        bool: True if the file was successfully loaded, False otherwise.
    """
    if not os.path.exists(user_blend_file):
        logger.error(f"Blender file {user_blend_file} does not exist.")
        return False

    try:
        bpy.ops.wm.open_mainfile(filepath=user_blend_file)
        # logger.info(f"Opened user-specified Blender file {user_blend_file} as the base scene.")
        return True
    except Exception as e:
        logger.error(f"Failed to load Blender file {user_blend_file}: {e}")
        return False


def select_focus_object(all_objects):
    for obj_dict in all_objects:
        obj = list(obj_dict.keys())[0]  # This is the actual Blender object
        properties = obj_dict[obj]
        if isinstance(properties, dict) and properties.get("camera_follow", {}).get("follow", False):
            return obj
    
    # If no object with camera_follow, fall back to placement 4 or the first object
    for obj_dict in all_objects:
        obj = list(obj_dict.keys())[0]
        properties = obj_dict[obj]
        if isinstance(properties, dict) and properties.get("placement") == 4:
            return obj
    
    # If no object with placement 4 is found, return the first object
    return list(all_objects[0].keys())[0] if all_objects else None


def render_scene(
    output_dir: str,
    context: bpy.types.Context,
    combination_file,
    start_frame: int = 1,
    end_frame: int = 65,
    combination_index=0,
    combination=None,
    render_images: bool =False,
    user_blend_file = None,
    animation_length: int = 100
) -> None:
    """
    Renders a scene with specified parameters.

    Args:
        output_dir (str): Path to the directory where the rendered video will be saved.
        context (bpy.types.Context): Blender context.
        combination_file (str): Path to the JSON file containing camera combinations.
        start_frame (int): Start frame of the animation. Defaults to 1.
        end_frame (int): End frame of the animation. Defaults to 65.
        combination_index (int): Index of the camera combination to use from the JSON file. Defaults to 0.
        render_images (bool): Flag to indicate if images should be rendered instead of videos.
        user_blend_file (str): Path to the user-specified Blender file to use as the base scene
        animation_length (int): Percentage animation length. Defaults to 100.

    Returns:
        None
    """

    console.print("Rendering scene with combination ", style="orange_red1", end="")
    console.print(f"{combination_index}", style="bold bright_green")

    os.makedirs(output_dir, exist_ok=True)

    initialize_scene()

    if user_blend_file:
        bpy.ops.wm.open_mainfile(filepath=user_blend_file)
        if not load_user_blend_file(user_blend_file):
            logger.error(f"Unable to load user-specified Blender file: {user_blend_file}")
            return  # Exit the function if the file could not be loaded

    context.scene.render.engine = 'BLENDER_EEVEE'

    create_camera_rig()

    scene = context.scene

    scene.frame_start = start_frame
    scene.frame_end = end_frame

    # Lock and hide all scene objects before doing any object operations
    initial_objects = lock_all_objects()

    if combination is not None:
        combination = json.loads(combination)
    else:
        combination = read_combination(combination_file, combination_index)
    all_objects = []

    focus_object = None

    for object_data in combination["objects"]:
        object_file = objaverse.load_objects([object_data["uid"]])[object_data["uid"]]

        load_object(object_file)
        obj = [obj for obj in context.view_layer.objects.selected][0]

        apply_and_remove_armatures()
        apply_all_modifiers(obj)
        join_objects_in_hierarchy(obj)
        optimize_meshes_in_hierarchy(obj)

        meshes = get_meshes_in_hierarchy(obj)
        obj = meshes[0]

        unparent_keep_transform(obj)
        set_pivot_to_bottom(obj)

        obj.scale = [object_data["scale"]["factor"] for _ in range(3)]
        normalize_object_scale(obj)
        obj.name = object_data["uid"] 

        all_objects.append({obj: object_data})

    largest_length = find_largest_length(all_objects)

    if not user_blend_file:
        set_background(args.hdri_path, combination)
        create_photosphere(args.hdri_path, combination).scale = (10, 10, 10)
        stage = create_stage(combination)
        apply_stage_material(stage, combination)
    
    unlock_objects(initial_objects)

    set_camera_settings(combination)
    set_camera_animation(combination, end_frame-start_frame, animation_length)
    
    place_objects_on_grid(all_objects, largest_length)
    yaw = combination["orientation"]["yaw"]
    
    focus_object = select_focus_object(all_objects)
    if focus_object is None or not isinstance(focus_object, bpy.types.Object):
        logger.error("No valid focus object found or focus object is not a Blender object. Cannot position camera.")
        return  

    position_camera(combination, focus_object)

    if not combination.get("no_movement", False):
        check_camera_follow = any(obj.get("camera_follow", {}).get("follow", False) for obj in combination['objects'])
        apply_animation(all_objects, focus_object, yaw, scene.frame_start, end_frame, check_camera_follow)

    sizes = [
        (1920, 1080),
        (1024, 1024),
        # able to add more options here
    ]

    if render_images:
        # Render a specific frame as an image with a random size
        middle_frame = (scene.frame_start + scene.frame_end) // 2
        size = random.choice(sizes)
        scene.frame_set(middle_frame)
        scene.render.resolution_x = size[0]
        scene.render.resolution_y = size[1]
        scene.render.resolution_percentage = 100
        render_path = os.path.join(
            output_dir,
            f"{combination_index}_frame_{middle_frame}_{size[0]}x{size[1]}.png",
        )
        scene.render.filepath = render_path
        bpy.ops.render.render(write_still=True)
        logger.info(f"Rendered image saved to {render_path}")
    else:
        # Render the entire animation as a video
        scene.render.resolution_x = 1920
        scene.render.resolution_y = 1080
        scene.render.resolution_percentage = 100
        scene.render.image_settings.file_format = "FFMPEG"
        scene.render.ffmpeg.format = "MPEG4"
        scene.render.ffmpeg.codec = "H264"
        scene.render.ffmpeg.constant_rate_factor = "PERC_LOSSLESS"
        scene.render.ffmpeg.ffmpeg_preset = "BEST"
        render_path = os.path.join(output_dir, f"{combination_index}.mp4")
        scene.render.filepath = render_path
        bpy.ops.render.render(animation=True)

        # uncomment this to prevent generation of blend files
        bpy.ops.wm.save_as_mainfile(
            filepath=os.path.join(output_dir, f"{combination_index}.blend")
        )

        logger.info(f"Rendered video saved to {render_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--output_dir",
        type=str,
        default="renders",
        required=False,
        help="Path to the directory where the rendered video or images will be saved.",
    )
    parser.add_argument(
        "--combination_file",
        type=str,
        default="combinations.json",
        help="Path to the JSON file containing camera combinations.",
    )
    parser.add_argument(
        "--hdri_path",
        type=str,
        default="backgrounds",
        help="Path to the directory where the background HDRs will be saved.",
    )
    parser.add_argument(
        "--combination_index",
        type=int,
        default=0,
        help="Index of the camera combination to use from the JSON file.",
        required=False,
    )
    parser.add_argument(
        "--start_frame",
        type=int,
        default=1,
        help="Start frame of the animation.",
        required=False,
    )
    parser.add_argument(
        "--end_frame",
        type=int,
        default=65,
        help="End frame of the animation.",
        required=False,
    )
    parser.add_argument(
        "--width", type=int, default=1920, help="Render output width.", required=False
    )
    parser.add_argument(
        "--height", type=int, default=1080, help="Render output height.", required=False
    )
    parser.add_argument(
        "--combination", type=str, default=None, help="Combination dictionary."
    )
    parser.add_argument(
        "--images",
        action="store_true",
        help="Generate images instead of videos.",
    )
    parser.add_argument(
        "--blend",
        type=str,
        default=None,
        help="Path to the user-specified Blender file to use as the base scene.",
        required=False,
    )
    parser.add_argument(
        "--animation_length",
        type=int,
        default=100,
        help="Percentage animation length. Defaults to 100%.",
        required=False
    )

    if "--" in sys.argv:
        argv = sys.argv[sys.argv.index("--") + 1 :]
    else:
        argv = []

    args = parser.parse_args(argv)

    context = bpy.context
    scene = context.scene
    render = scene.render

    if args.combination is not None:
        combination = json.loads(args.combination)
    else:
        combination = read_combination(args.combination_file, args.combination_index)

    # get the object uid from the 'object' column, which is a dictionary
    objects_column = combination["objects"]

    for object in objects_column:
        uid = object["uid"]

        downloaded = objaverse.load_objects([uid])

    # Render the images
    render_scene(
        start_frame=args.start_frame,
        end_frame=args.end_frame,
        output_dir=args.output_dir,
        context=context,
        combination_file=args.combination_file,
        combination_index=args.combination_index,
        combination=args.combination,
        render_images=args.images,
        user_blend_file=args.blend,
    )

```````

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/batch.py`:

```````py
import json
import readline
import multiprocessing
import os
import subprocess
import sys
import argparse
from questionary import Style
from questionary import Choice, select
from rich.console import Console
from typing import Any, Dict, List, Optional

from simian.prompts import generate_gemini, setup_gemini, parse_gemini_json, CAMERA_PROMPT, OBJECTS_JSON_PROMPT, OBJECTS_PROMPT, OBJECTS_JSON_IMPROVEMENT_PROMPT, CAMERA_JSON_IMPROVEMENT_PROMPT
from .server import initialize_chroma_db, query_collection
from .combiner import calculate_transformed_positions

console = Console()

def select_mode():
    """
    Prompt the user to select the rendering mode (Prompt Mode or Batch Mode).
    """

    custom_style = Style([
        ('question', 'fg:#FF9D00 bold'), 
        ('pointer', 'fg:#FF9D00 bold'),  
        ('highlighted', 'fg:#00FFFF bold'),
        ('default', 'fg:#FFFFFF'),
    ])

    options = [
        Choice("Prompt Mode", value="Prompt Mode"),
        Choice("Batch Mode", value="Batch Mode")
    ]
    
    selected = select(
        "Select Mode",
        choices=options,
        use_indicator=True,
        style=custom_style,
        instruction='Use ↑ and ↓ to navigate, Enter to select',
        qmark="", 
        pointer="▶", 
    ).ask()

    console.print(f"\nEntering {selected}", style="bold green")
    
    return selected


def render_objects(
    processes: Optional[int] = None,
    render_timeout: int = 3000,
    width: int = 1920,
    height: int = 1080,
    start_index: int = 0,
    end_index: int = -1,
    start_frame: int = 1,
    end_frame: int = 65,
    images: bool = False,
    blend_file: Optional[str] = None,
    animation_length: int = 100
) -> None:
    """
    Automates the rendering of objects using Blender based on predefined combinations.

    This function orchestrates the rendering of multiple objects within a specified range
    from the combinations DataFrame. It allows for configuration of rendering dimensions,
    use of specific GPU devices, and selection of frames for animation sequences.

    Args:
        processes (Optional[int]): Number of processes to use for multiprocessing.
        Defaults to three times the number of CPU cores.
        render_timeout (int): Maximum time in seconds for a single rendering process.
        width (int): Width of the rendering in pixels.
        height (int): Height of the rendering in pixels.
        start_index (int): Starting index for rendering from the combinations DataFrame.
        end_index (int): Ending index for rendering from the combinations DataFrame.
        start_frame (int): Starting frame number for the animation.
        end_frame (int): Ending frame number for the animation.
        images (bool): Generate images instead of videos.
        blend_file (Optional[str]): Path to the user-specified Blender file to use as the base scene.
        animation_length (int): Percentage animation length.

    Raises:
        NotImplementedError: If the operating system is not supported.
        FileNotFoundError: If Blender is not found at the specified path.

    Returns:
        None
    """
    # Set the number of processes to three times the number of CPU cores if not specified.
    if processes is None:
        processes = multiprocessing.cpu_count() * 3

    scripts_dir = os.path.dirname(os.path.realpath(__file__))
    target_directory = os.path.join(scripts_dir, "../", "renders")
    hdri_path = os.path.join(scripts_dir, "../", "backgrounds")

    # make sure renders directory exists
    os.makedirs(target_directory, exist_ok=True)

    if end_index == -1:

        # get the length of combinations.json
        with open(os.path.join(scripts_dir, "../", "combinations.json"), "r") as file:
            data = json.load(file)
            combinations_data = data["combinations"]
            num_combinations = len(combinations_data)

        end_index = num_combinations

    # Loop over each combination index to set up and run the rendering process.
    for i in range(start_index, end_index):
        if images:
            args = f"--width {width} --height {height} --combination_index {i} --start_frame {start_frame} --end_frame {end_frame} --output_dir {target_directory} --hdri_path {hdri_path} --animation_length {animation_length} --images"
        else:
            args = f"--width {width} --height {height} --combination_index {i} --start_frame {start_frame} --end_frame {end_frame} --output_dir {target_directory} --hdri_path {hdri_path} --animation_length {animation_length}"

        if blend_file:
            args += f" --blend {blend_file}"

        command = f"{sys.executable} -m simian.render -- {args}"
        subprocess.run(["bash", "-c", command], timeout=render_timeout, check=False)


def parse_args(args_list = None) -> argparse.Namespace:
    """
    Parse the command-line arguments for the rendering process.

    Args:
        args_list (Optional[List[str]]): A list of command-line arguments. Defaults to None.

    Returns:
        argparse.Namespace: The parsed command-line arguments.
    """

    parser = argparse.ArgumentParser(
        description="Automate the rendering of objects using Blender."
    )
    parser.add_argument(
        "--processes",
        type=int,
        default=None,
        help="Number of processes to use for multiprocessing. Defaults to three times the number of CPU cores.",
    )
    parser.add_argument(
        "--render_timeout",
        type=int,
        default=3000,
        help="Maximum time in seconds for a single rendering process. Defaults to 3000.",
    )
    parser.add_argument(
        "--width",
        type=int,
        default=1920,
        help="Width of the rendering in pixels. Defaults to 1920.",
    )
    parser.add_argument(
        "--height",
        type=int,
        default=1080,
        help="Height of the rendering in pixels. Defaults to 1080.",
    )
    parser.add_argument(
        "--start_index",
        type=int,
        default=0,
        help="Starting index for rendering from the combinations DataFrame. Defaults to 0.",
    )
    parser.add_argument(
        "--end_index",
        type=int,
        default=-1,
        help="Ending index for rendering from the combinations DataFrame. Defaults to -1.",
    )
    parser.add_argument(
        "--start_frame",
        type=int,
        default=1,
        help="Starting frame number for the animation. Defaults to 1.",
    )
    parser.add_argument(
        "--end_frame",
        type=int,
        default=65,
        help="Ending frame number for the animation. Defaults to 65.",
    )
    parser.add_argument(
        "--images",
        action="store_true",
        help="Generate images instead of videos.",
    )
    parser.add_argument(
        "--blend",
        type=str,
        default=None,
        help="Path to the user-specified Blender file to use as the base scene.",
        required=False,
    )
    parser.add_argument(
        "--animation_length",
        type=int,
        default=100,
        help="Percentage animation length. Defaults to 100%.",
        required=False
    )

    if args_list is None:
        args = parser.parse_args()
    else:
        args = parser.parse_args(args_list)
    
    return args


def should_apply_movement(all_objects):
    """
    Check if any object in the scene has movement defined.
    
    Args:
        all_objects (list): List of object dictionaries.
    
    Returns:
        bool: True if any object has movement, False otherwise.
    """
    return any('movement' in obj for obj in all_objects)


def prompt_based_rendering():
    """
    Perform prompt-based rendering using the Gemini API and ChromaDB.
    """

    import random
    from chromadb.utils import embedding_functions
    from sentence_transformers import SentenceTransformer

    chroma_client = initialize_chroma_db(reset_hdri=False, reset_textures=False)

    # Create or get collections for each data type
    object_collection = chroma_client.get_or_create_collection(name="object_captions")
    hdri_collection = chroma_client.get_or_create_collection(name="hdri_backgrounds")
    texture_collection = chroma_client.get_or_create_collection(name="textures")

    prompt = input("Enter your prompt (or 'quit' to exit): ")

    # Generate Gemini
    model = setup_gemini()
    objects_background_ground_prompt = generate_gemini(model, OBJECTS_PROMPT, prompt)
    objects_background_ground_list = json.loads(objects_background_ground_prompt)

    # split array, background and ground are last two elements
    objects_prompt = objects_background_ground_list[:-2]
    background_prompt = objects_background_ground_list[-2]
    ground_prompt = objects_background_ground_list[-1]
    
    object_ids = []
    for i, obj in enumerate(objects_prompt):
        object_options =  query_collection(obj, object_collection, n_results=2)
        object_ids.append({object_options["ids"][0][0]: obj})
    
    background_query = query_collection(background_prompt, hdri_collection, n_results=2)
    background_id = background_query["ids"][0][0]
    background_data = background_query["metadatas"][0][0]

    formatted_background = {
        "background": {
            "name": background_data['name'],
            "url": background_data['url'],
            "id": background_id,
            "from": "hdri_data"
        }
    }

    ground_texture_query = query_collection(ground_prompt, texture_collection, n_results=2)
    ground_data = ground_texture_query["metadatas"][0][0]

    formatted_stage = {
        "stage": {
            "material": {
                "name": ground_data['name'],
                "maps": ground_data['maps']
            },
            "uv_scale": [random.uniform(0.8, 1.2), random.uniform(0.8, 1.2)],
            "uv_rotation": random.uniform(0, 360)
        }
    }

    prompt += str(object_ids)

    objects_json_prompt = generate_gemini(model, OBJECTS_JSON_PROMPT, prompt + str(object_ids))
    objects_parse = parse_gemini_json(objects_json_prompt)

    camera_prompt = generate_gemini(model, CAMERA_PROMPT, prompt)
    camera_parse = parse_gemini_json(camera_prompt)

    # Combine all pieces into the final structure
    final_structure = {
        "index": 0,
        "objects": objects_parse.get("objects"),
        "background": formatted_background["background"],
        "orientation": camera_parse.get("orientation", {}),
        "framing": camera_parse.get("framing", {}),
        "animation": camera_parse.get("animation", {}),
        "stage": formatted_stage["stage"],
        "postprocessing": camera_parse.get("postprocessing", {})
    }
    
    if not should_apply_movement(final_structure["objects"]):
        final_structure["no_movement"] = True

    updated_combination = calculate_transformed_positions(final_structure)
    write_combinations_json(updated_combination)
    
    # Get the directory of the current script
    current_dir = os.path.dirname(os.path.realpath(__file__))
    
    # Set default rendering parameters
    width = 1024
    height = 576
    start_frame = 1
    end_frame = 65

    render_objects(
        processes=None,
        render_timeout=3000,
        width=width,
        height=height,
        start_index=0, 
        end_index=1,
        start_frame=start_frame,
        end_frame=end_frame,
        images=False,  
        blend_file=None,
        animation_length=100 
    )

    return updated_combination


def write_combinations_json(combination: Dict[str, Any], output_file: str = "combinations.json"):
    """
    Write the combination data to a JSON file.

    Args:
        combination (dict): The combination data to write.
        output_file (str): The name of the output JSON file. Defaults to "combinations.json".

    Returns:
        None
    """
    # Prepare the data structure
    data = {
        "seed": 0,  # You might want to make this dynamic
        "count": 1,  # Since we're only writing one combination
        "combinations": [combination]
    }

    # Get the directory of the current script
    current_dir = os.path.dirname(os.path.realpath(__file__))
    
    # Construct the full path for the output file
    output_path = os.path.join(current_dir, "..", output_file)

    # Write the data to the JSON file
    with open(output_path, 'w') as f:
        json.dump(data, f, indent=4)


def main():
    simverse_ascii = """

 ░▒▓███████▓▒░▒▓█▓▒░▒▓██████████████▓▒░░▒▓█▓▒░░▒▓█▓▒░▒▓████████▓▒░▒▓███████▓▒░ ░▒▓███████▓▒░▒▓████████▓▒░ 
░▒▓█▓▒░      ░▒▓█▓▒░▒▓█▓▒░░▒▓█▓▒░░▒▓█▓▒░▒▓█▓▒░░▒▓█▓▒░▒▓█▓▒░      ░▒▓█▓▒░░▒▓█▓▒░▒▓█▓▒░      ░▒▓█▓▒░        
░▒▓█▓▒░      ░▒▓█▓▒░▒▓█▓▒░░▒▓█▓▒░░▒▓█▓▒░░▒▓█▓▒▒▓█▓▒░░▒▓█▓▒░      ░▒▓█▓▒░░▒▓█▓▒░▒▓█▓▒░      ░▒▓█▓▒░        
 ░▒▓██████▓▒░░▒▓█▓▒░▒▓█▓▒░░▒▓█▓▒░░▒▓█▓▒░░▒▓█▓▒▒▓█▓▒░░▒▓██████▓▒░ ░▒▓███████▓▒░ ░▒▓██████▓▒░░▒▓██████▓▒░   
       ░▒▓█▓▒░▒▓█▓▒░▒▓█▓▒░░▒▓█▓▒░░▒▓█▓▒░ ░▒▓█▓▓█▓▒░ ░▒▓█▓▒░      ░▒▓█▓▒░░▒▓█▓▒░      ░▒▓█▓▒░▒▓█▓▒░        
       ░▒▓█▓▒░▒▓█▓▒░▒▓█▓▒░░▒▓█▓▒░░▒▓█▓▒░ ░▒▓█▓▓█▓▒░ ░▒▓█▓▒░      ░▒▓█▓▒░░▒▓█▓▒░      ░▒▓█▓▒░▒▓█▓▒░        
░▒▓███████▓▒░░▒▓█▓▒░▒▓█▓▒░░▒▓█▓▒░░▒▓█▓▒░  ░▒▓██▓▒░  ░▒▓████████▓▒░▒▓█▓▒░░▒▓█▓▒░▒▓███████▓▒░░▒▓████████▓▒░ 
                                                                                                          
    """

    console.print(simverse_ascii)
    selected_mode = select_mode()

    if selected_mode == "Prompt Mode":
        prompt_based_rendering()
    else:
        while True:
            console.print("Command arguments (e.g., --start_index 0 --end_index 1000 --width 1024 --height 576 --start_frame 1 --end_frame 2). Type 'quit' to exit.")
            command = input("Enter command: ")
            if command.lower() == 'quit':
                break
            try:
                args = parse_args(command.split())
                render_objects(
                    processes=args.processes,
                    render_timeout=args.render_timeout,
                    width=args.width,
                    height=args.height,
                    start_index=args.start_index,
                    end_index=args.end_index,
                    start_frame=args.start_frame,
                    end_frame=args.end_frame,
                    images=args.images,
                    blend_file=args.blend,
                    animation_length=args.animation_length
                )
            except SystemExit:
                console.print("Invalid command. Please try again.", style="bold red")


if __name__ == "__main__":
    main()

```````

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/tests/__run__.py`:

```````py
import os
import subprocess
import sys
import argparse

# Parse command-line arguments
parser = argparse.ArgumentParser()
parser.add_argument(
    "--debug", action="store_true", help="Print the command to run each subtest"
)
args = parser.parse_args()

files = os.listdir("simian/tests")
test_files = [file for file in files if file.endswith("_test.py")]

# Flag to track if any test has failed
test_failed = False

for test_file in test_files:
    print(f"Running tests in {test_file}...")
    # get the module name from the file by removing .py
    module_name = test_file[:-3]
    cmd = [sys.executable, "-m", f"simian.tests.{module_name}"]

    if args.debug:
        print(f"Command: {' '.join(cmd)}")

    # Run the test file and capture the output
    result = subprocess.run(cmd, capture_output=True, text=True)

    # Check if there are any errors in the output
    if (
        "FAILED" in result.stderr
        or "ERROR" in result.stderr
        or "ValueError" in result.stderr
        or "Traceback" in result.stderr
    ):
        print(f"Tests in {test_file} failed!")
        print(result.stderr)
        test_failed = True
    else:
        print(f"Tests in {test_file} passed.")

# Exit with appropriate exit code based on test results
if test_failed:
    print("Some tests failed. Exiting with exit code 1.")
    sys.exit(1)
else:
    print("All tests passed. Exiting with exit code 0.")
    sys.exit(0)

```````

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/tests/postprocessing_test.py`:

```````py
from unittest.mock import MagicMock

from ..postprocessing import (
    setup_compositor_for_black_and_white,
    setup_compositor_for_cel_shading,
    setup_compositor_for_depth,
)


def mock_new_node(*args, **kwargs):
    """
    Mock for the nodes.new function in Blender.
    """
    node_type = kwargs.get("type", "UnknownType")
    node = MagicMock()
    node.type = node_type
    node.inputs = MagicMock()
    node.outputs = MagicMock()
    return node


def test_setup_compositor_for_black_and_white():
    """
    Test the setup_compositor_for_black_and_white function.
    """
    context = MagicMock()
    scene = MagicMock()
    nodes = MagicMock()
    links = MagicMock()

    context.scene = scene
    scene.node_tree.nodes = nodes
    scene.node_tree.links = links
    nodes.new.side_effect = mock_new_node  # This now handles keyword arguments

    setup_compositor_for_black_and_white(context)

    # Assertions to ensure nodes are created and links are made
    nodes.new.assert_any_call(type="CompositorNodeRLayers")
    nodes.new.assert_any_call(type="CompositorNodeHueSat")
    nodes.new.assert_any_call(type="CompositorNodeComposite")
    assert links.new.called, "Links between nodes should be created"


def test_setup_compositor_for_cel_shading():
    """
    Test the setup_compositor_for_cel_shading function.
    """
    context = MagicMock()
    scene = MagicMock()
    nodes = MagicMock()
    links = MagicMock()

    context.scene = scene
    scene.node_tree.nodes = nodes
    scene.node_tree.links = links
    nodes.new.side_effect = mock_new_node

    setup_compositor_for_cel_shading(context)

    nodes.new.assert_any_call(type="CompositorNodeRLayers")
    nodes.new.assert_any_call(type="CompositorNodeNormal")
    nodes.new.assert_any_call(type="CompositorNodeValToRGB")
    nodes.new.assert_any_call(type="CompositorNodeMixRGB")
    nodes.new.assert_any_call(type="CompositorNodeAlphaOver")
    nodes.new.assert_any_call(type="CompositorNodeComposite")
    assert links.new.called
    print(
        "============ Test Passed: test_setup_compositor_for_cel_shading ============"
    )


def test_setup_compositor_for_depth():
    """
    Test the setup_compositor_for_depth function.
    """
    context = MagicMock()
    scene = MagicMock()
    nodes = MagicMock()
    links = MagicMock()

    context.scene = scene
    scene.node_tree.nodes = nodes
    scene.node_tree.links = links
    nodes.new.side_effect = mock_new_node

    setup_compositor_for_depth(context)

    nodes.new.assert_any_call(type="CompositorNodeRLayers")
    nodes.new.assert_any_call(type="CompositorNodeNormalize")
    nodes.new.assert_any_call(type="CompositorNodeComposite")
    assert links.new.called
    print("============ Test Passed: test_setup_compositor_for_depth ============")


if __name__ == "__main__":
    test_setup_compositor_for_black_and_white()
    test_setup_compositor_for_cel_shading()
    test_setup_compositor_for_depth()
    print("============ ALL TESTS PASSED ============")

```````

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/tests/background_test.py`:

```````py
import os

from unittest.mock import patch, MagicMock
from ..background import (
    get_hdri_path,
    get_background,
    set_background,
    create_photosphere,
    create_photosphere_material,
)

import bpy
from ..background import set_background


def test_get_hdri_path():
    """
    Test the get_hdri_path function.
    """
    combination = {"background": {"id": "123", "from": "test_dataset"}}
    hdri_path = "/fake/path"
    expected_result = f"/fake/path/test_dataset/123.hdr"
    result = get_hdri_path(hdri_path, combination)

    print("test_get_hdri_path result: ", result)
    assert result == expected_result
    print("============ Test Passed: get_hdri_path ============")


def test_get_background():
    """
    Test the get_background function.
    """
    combination = {
        "background": {
            "url": "http://example.com/image.hdr",
            "id": "123",
            "from": "test_dataset",
        }
    }
    hdri_path = "/fake/path"

    with patch("os.makedirs"), patch("os.path.exists", return_value=False), patch(
        "requests.get"
    ) as mock_get, patch("builtins.open", new_callable=MagicMock):
        mock_response = MagicMock()
        mock_response.content = b"fake data"
        mock_get.return_value = mock_response

        get_background(hdri_path, combination)
        print("get_background called")

        mock_get.assert_called_with("http://example.com/image.hdr")
        print("============ Test Passed: test_get_background ============")


def test_set_background():
    """
    Test the set_background function.
    """
    combination = {
        "background": {
            "id": "123",
            "from": "test_dataset",
            "url": "http://example.com/image.hdr",
        }
    }
    # Use a path in the user's home directory or another writable location
    background_base_path = os.path.join(os.path.expanduser("~"), "test_backgrounds")

    # Mocking dependencies
    with patch(
        "..background.get_hdri_path",
        return_value=os.path.join(background_base_path, "test_dataset/123.hdr"),
    ) as mock_get_path:
        with patch("os.path.exists", return_value=False):
            with patch("requests.get") as mock_get:
                mock_response = MagicMock()
                mock_response.content = b"Fake HDR content"
                mock_get.return_value = mock_response

                # Ensure the directory exists before writing
                os.makedirs(
                    os.path.dirname(
                        os.path.join(background_base_path, "test_dataset/123.hdr")
                    ),
                    exist_ok=True,
                )

                # Function call
                set_background(background_base_path, combination)

                # Open the file to simulate writing to it
                with open(
                    os.path.join(background_base_path, "test_dataset/123.hdr"), "wb"
                ) as file:
                    file.write(mock_response.content)

                mock_get.assert_called_with("http://example.com/image.hdr")
                print("============ Test Passed: test_set_background ============")


def create_test_photosphere():
    """
    Create a UV sphere in Blender with specific parameters.
    """
    # Create a UV sphere in Blender with specific parameters
    bpy.ops.mesh.primitive_uv_sphere_add(
        segments=64, ring_count=32, radius=1.0, location=(0, 0, 3)
    )
    sphere = bpy.context.object
    bpy.ops.object.shade_smooth()

    # Enter edit mode, select all vertices, and flip the normals
    bpy.ops.object.mode_set(mode="EDIT")
    bpy.ops.mesh.select_all(action="SELECT")
    bpy.ops.mesh.flip_normals()
    bpy.ops.object.mode_set(mode="OBJECT")

    # Rename the sphere for identification
    sphere.name = "Photosphere"
    print("Sphere created successfully")
    return sphere


def test_create_photosphere():
    epsilon = 0.001  # Small threshold for floating-point comparisons
    sphere = create_test_photosphere()

    # Check each component of the sphere's location to see if it matches the expected values
    assert abs(sphere.location.x - 0.0) < epsilon, "X coordinate is incorrect"
    assert abs(sphere.location.y - 0.0) < epsilon, "Y coordinate is incorrect"
    assert abs(sphere.location.z - 3.0) < epsilon, "Z coordinate is incorrect"
    print("============ Test Passed: test_create_photosphere ============")


def test_create_photosphere_material():
    """
    Test the create_photosphere_material function.
    """
    # Create a UV sphere in Blender with specific parameters
    bpy.ops.mesh.primitive_uv_sphere_add(
        segments=64, ring_count=32, radius=1.0, location=(0, 0, 3)
    )
    sphere = bpy.context.object
    bpy.ops.object.shade_smooth()

    # Enter edit mode, select all vertices, and flip the normals
    bpy.ops.object.mode_set(mode="EDIT")
    bpy.ops.mesh.select_all(action="SELECT")
    bpy.ops.mesh.flip_normals()
    bpy.ops.object.mode_set(mode="OBJECT")

    # Rename the sphere for identification
    sphere.name = "Photosphere"

    # Create a combination dictionary with background information
    combination = {
        "background": {
            "id": "123",
            "from": "test_dataset",
            "url": "http://example.com/image.hdr",
        }
    }

    # Set the base path for background images
    background_base_path = os.path.join(os.path.expanduser("~"), "test_backgrounds")

    # Mock the get_hdri_path function to return a known path
    with patch(
        "..background.get_hdri_path",
        return_value=os.path.join(background_base_path, "test_dataset/123.hdr"),
    ) as mock_get_path:
        # Mock the existence of the background image file
        with patch("os.path.exists", return_value=True):
            # Mock the open function to simulate reading the background image
            with patch("builtins.open", MagicMock()) as mock_open:
                # Call the create_photosphere_material function
                create_photosphere_material(background_base_path, combination, sphere)

                # Verify that the material was created
                assert (
                    sphere.data.materials[0].name == "PhotosphereMaterial"
                ), "Material not created successfully"
                print(
                    "============ Test Passed: test_create_photosphere_material ============"
                )


# Run tests if this file is executed as a script
if __name__ == "__main__":
    test_get_hdri_path()
    test_get_background()
    # test_set_background()
    test_create_photosphere()
    # test_create_photosphere_material()
    print("============ ALL TESTS PASSED ============")

```````

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/tests/transform_test.py`:

```````py
import math

from ..transform import (
    degrees_to_radians,
    compute_rotation_matrix,
    apply_rotation,
    adjust_positions,
    determine_relationships,
)


def test_degrees_to_radians():
    epsilon = 1e-9
    assert abs(degrees_to_radians(180) - math.pi) < epsilon


def test_compute_rotation_matrix():
    epsilon = 1e-9
    theta = math.pi / 4  # 45 degrees in radians
    expected_matrix = [
        [math.sqrt(2) / 2, -math.sqrt(2) / 2],
        [math.sqrt(2) / 2, math.sqrt(2) / 2],
    ]
    result_matrix = compute_rotation_matrix(theta)
    for i in range(2):
        for j in range(2):
            assert abs(result_matrix[i][j] - expected_matrix[i][j]) < epsilon


def test_apply_rotation():
    epsilon = 1e-9
    point = [1, 0]
    rotation_matrix = compute_rotation_matrix(math.pi / 2)  # 90 degrees rotation
    expected_point = [0, 1]
    result_point = apply_rotation(point, rotation_matrix)
    for i in range(2):
        assert abs(result_point[i] - expected_point[i]) < epsilon


def test_adjust_positions():
    objects = [{"placement": 0}, {"placement": 1}]
    camera_yaw = 90
    adjusted_objects = adjust_positions(objects, camera_yaw)
    for obj in adjusted_objects:
        assert "transformed_position" in obj


def test_determine_relationships():
    # Define a set of objects with known transformed positions
    objects = [
        {"name": "obj1", "transformed_position": [0, 0]},
        {"name": "obj2", "transformed_position": [1, 1]},
        {"name": "obj3", "transformed_position": [-1, -1]},
    ]

    # Define the directional relationship phrases
    object_data = {
        "relationships": {
            "to_the_left": ["to the left of"],
            "to_the_right": ["to the right of"],
            "in_front_of": ["in front of"],
            "behind": ["behind"],
        }
    }

    # Define a known camera yaw
    camera_yaw = 45

    # Determine the relationships between the objects
    relationships = determine_relationships(objects, camera_yaw)

    # Expected relationships
    expected_relationships = [
        "obj1 is  and behind obj2.",
        "obj1 is  and in front of obj3.",
        "obj2 is  and in front of obj1.",
        "obj2 is  and in front of obj3.",
        "obj3 is  and behind obj1.",
        "obj3 is  and behind obj2.",
    ]

    # Check if the relationships are correctly formed
    assert len(relationships) == len(
        expected_relationships
    ), "The number of relationships is incorrect."

    for relationship in expected_relationships:
        assert (
            relationship in relationships
        ), f"Expected relationship '{relationship}' not found in results."


if __name__ == "__main__":
    test_degrees_to_radians()
    test_compute_rotation_matrix()
    test_apply_rotation()
    test_adjust_positions()
    test_determine_relationships()
    print("============ ALL TESTS PASSED ============")

```````

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/tests/combiner_test.py`:

```````py
import os
import sys
import json
import argparse
import math
from unittest.mock import patch, mock_open
import random

current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, "../../"))

# Mock arguments with absolute paths
mock_args = {
    "count": 2,
    "seed": 42,
    "max_number_of_objects": 5,
    "camera_file_path": os.path.join(project_root, "data/camera_data.json"),
    "object_data_path": os.path.join(project_root, "data/object_data.json"),
    "texture_data_path": os.path.join(project_root, "datasets/texture_data.json"),
    "datasets_path": os.path.join(project_root, "data/datasets.json"),
    "cap3d_captions_path": os.path.join(project_root, "datasets/cap3d_captions.json"),
    "simdata_path": os.path.join(project_root, "datasets"),
    "output_path": os.path.join(project_root, "combinations.json"),  # Corrected path
    "stage_data_path": os.path.join(project_root, "data/stage_data.json"),
}


def mock_parse_args(*args, **kwargs):
    return argparse.Namespace(**mock_args)


# Mock parse_args globally before importing ..combiner
with patch("argparse.ArgumentParser.parse_args", new=mock_parse_args):
    from ..combiner import (
        read_json_file,
        generate_combinations,
        generate_stage_captions,
        generate_orientation_caption,
        generate_object_name_description_captions,
        generate_relationship_captions,
        generate_fov_caption,
        generate_postprocessing_caption,
        generate_framing_caption,
        flatten_descriptions,
        generate_animation_captions,
        generate_postprocessing,
        generate_orientation,
        generate_framing,
        generate_animation,
        generate_objects,
        generate_background,
        generate_stage,
        flatten_descriptions,
        speed_factor_to_percentage,
    )


def test_generate_postprocessing_caption():
    combination = {
        "postprocessing": {
            "bloom": {"type": "medium"},
            "ssao": {"type": "high"},
            "ssrr": {"type": "none"},
            "motionblur": {"type": "medium"},
        }
    }

    camera_data = {
        "postprocessing": {
            "bloom": {
                "types": {
                    "medium": {"descriptions": ["medium bloom effect"]}
                }
            },
            "ssao": {
                "types": {
                    "high": {"descriptions": ["high ssao effect"]}
                }
            },
            "ssrr": {
                "types": {
                    "none": {"descriptions": ["no ssrr effect"]}
                }
            },
            "motionblur": {
                "types": {
                    "medium": {"descriptions": ["medium motion blur"]}
                }
            }
        }
    }

    # Set a fixed seed for reproducibility
    random.seed(42)

    actual_caption = generate_postprocessing_caption(combination, camera_data)

    # Define expected effects
    expected_effects = [
        "medium bloom effect",
        "high ssao effect",
        "no ssrr effect",
        "medium motion blur"
    ]

    # Check that the caption contains at least one of the expected effects
    assert any(effect in actual_caption for effect in expected_effects), \
        f"Caption doesn't contain any expected effects: {actual_caption}"

    # Check that the caption doesn't contain any unexpected words
    all_expected_words = set(" ".join(expected_effects).split())
    actual_words = set(actual_caption.split())
    unexpected_words = actual_words - all_expected_words
    assert not unexpected_words, f"Caption contains unexpected words: {unexpected_words}"

    print("============ Test Passed: test_generate_postprocessing_caption ============")


def test_read_json_file():
    """
    Test the read_json_file function.
    """
    mock_data = '{"name": "example", "type": "test"}'
    with patch("builtins.open", mock_open(read_data=mock_data)) as mocked_file:
        result = read_json_file("dummy_path.json")
        mocked_file.assert_called_once_with("dummy_path.json", "r")
        assert result == json.loads(mock_data)
        print("============ Test Passed: test_read_json_file ============")


data_dir = os.path.join(project_root, "data")
datasets_dir = os.path.join(project_root, "datasets")

object_data = read_json_file(os.path.join(data_dir, "object_data.json"))
stage_data = read_json_file(os.path.join(data_dir, "stage_data.json"))
texture_data = read_json_file(os.path.join(datasets_dir, "texture_data.json"))
camera_data = read_json_file(os.path.join(data_dir, "camera_data.json"))
datasets_data = read_json_file(os.path.join(data_dir, "datasets.json"))
cap3d_captions_data = read_json_file(os.path.join(datasets_dir, "cap3d_captions.json"))


def test_combination_caption():
    file_path = os.path.join(project_root, "combinations.json")

    with open(file_path, "r") as file:
        data = json.load(file)

    first_combination = data["combinations"][0]

    required_combination_fields = [
        "index",
        "objects",
        "objects_caption",
        "background",
        "background_caption",
        "orientation",
        "orientation_caption",
        "framing",
        "framing_caption",
        "animation",
        "animation_caption",
        "stage",
        "stage_caption",
        "postprocessing",
        "postprocessing_caption",
        "caption",
    ]

    required_object_fields = [
        "name",
        "uid",
        "description",
        "placement",
        "from",
        "scale",
        "transformed_position",
        "relationships",
    ]

    required_scale_fields = ["factor", "name", "name_synonym"]

    required_background_fields = ["name", "url", "id", "from"]
    required_orientation_fields = ["yaw", "pitch"]
    required_framing_fields = ["fov", "coverage_factor", "name"]
    required_animation_fields = ["name", "keyframes", "speed_factor"]
    required_stage_fields = ["material", "uv_scale", "uv_rotation"]
    required_material_fields = ["name", "maps"]
    required_postprocessing_fields = ["bloom", "ssao", "ssrr", "motionblur"]
    required_bloom_fields = ["threshold", "intensity", "radius", "type"]
    required_ssao_fields = ["distance", "factor", "type"]
    required_ssrr_fields = ["max_roughness", "thickness", "type"]
    required_motionblur_fields = ["shutter_speed", "type"]

    def assert_fields_exist(data, required_fields, parent_key=""):
        for field in required_fields:
            assert field in data, f"Missing field: {parent_key}{field}"

    # Check combination level fields
    assert_fields_exist(first_combination, required_combination_fields)

    # Check objects fields
    for obj in first_combination["objects"]:
        assert_fields_exist(obj, required_object_fields)
        assert_fields_exist(obj["scale"], required_scale_fields)
        assert isinstance(obj["relationships"], str), "relationships should be a string"
        if "movement" in obj:
            assert_fields_exist(obj["movement"], ["direction", "speed"])

    # Check background fields
    assert_fields_exist(first_combination["background"], required_background_fields)

    # Check orientation fields
    assert_fields_exist(first_combination["orientation"], required_orientation_fields)

    # Check framing fields
    assert_fields_exist(first_combination["framing"], required_framing_fields)

    # Check animation fields
    assert_fields_exist(first_combination["animation"], required_animation_fields)

    # Check stage fields
    assert_fields_exist(first_combination["stage"], required_stage_fields)
    assert_fields_exist(first_combination["stage"]["material"], required_material_fields)

    # Check postprocessing fields
    assert_fields_exist(first_combination["postprocessing"], required_postprocessing_fields)
    assert_fields_exist(first_combination["postprocessing"]["bloom"], required_bloom_fields)
    assert_fields_exist(first_combination["postprocessing"]["ssao"], required_ssao_fields)
    assert_fields_exist(first_combination["postprocessing"]["ssrr"], required_ssrr_fields)
    assert_fields_exist(first_combination["postprocessing"]["motionblur"], required_motionblur_fields)

    print("============ Test Passed: test_combination_structure ============")


def test_generate_combinations():
    """
    Test the generate_combinations function.
    """
    # Prepare mock data for camera_data and a small count
    camera_data = {
        "orientation": {
            "yaw_min": 0,
            "yaw_max": 180,
            "pitch_min": -90,
            "pitch_max": 90,
            "labels": {
                "pitch": {"0": ["neutral", "level"], "45": ["upward"]},
                "yaw": {"0": ["front"], "180": ["back"]}
            },
            "descriptions": [
                "The camera is oriented at a <pitch> angle and facing <yaw>."
            ],
        },
        "framings": [
            {
                "name": "wide",
                "fov_min": 30,
                "fov_max": 60,
                "coverage_factor_min": 1,
                "coverage_factor_max": 2,
                "descriptions": [
                    "Wide framing with FOV <fov> and coverage factor <coverage_factor>."
                ],
            }
        ],
        "animations": [
            {
                "name": "zoom_in",
                "descriptions": [
                    "The camera zooms in.",
                    "Zoom in.",
                    "Move closer to the scene.",
                    "The camera moves closer to the subject.",
                    "Zoom in to focus on details."
                ],
                "keyframes": [
                    {
                        "Camera": {
                            "angle_offset": 10
                        }
                    },
                    {
                        "Camera": {
                            "angle_offset": 0
                        }
                    }
                ]
            }
        ],
        "postprocessing": {
            "bloom": {
                "threshold_min": 0,
                "threshold_max": 1,
                "intensity_min": 0,
                "intensity_max": 1,
                "radius_min": 0,
                "radius_max": 10,
                "types": {
                    "low": {
                        "intensity_min": 0,
                        "intensity_max": 1,
                        "descriptions": ["Low bloom effect."]
                    }
                },
            },
            "ssao": {
                "distance_min": 0,
                "distance_max": 1,
                "factor_min": 0,
                "factor_max": 1,
                "types": {
                    "low": {
                        "factor_min": 0,
                        "factor_max": 1,
                        "descriptions": ["Low SSAO effect."]
                    }
                },
            },
            "ssrr": {
                "min_max_roughness": 0,
                "max_max_roughness": 1,
                "min_thickness": 0,
                "max_thickness": 1,
                "types": {
                    "low": {
                        "max_roughness_min": 0,
                        "max_roughness_max": 1,
                        "descriptions": ["Low SSRR effect."]
                    }
                },
            },
            "motionblur": {
                "shutter_speed_min": 0,
                "shutter_speed_max": 1,
                "types": {
                    "low": {
                        "shutter_speed_min": 0,
                        "shutter_speed_max": 1,
                        "descriptions": ["Low motion blur effect."]
                    }
                },
            },
        },
        "animation_speed": {
            "types": {
                "slow": {"min": 0.5, "max": 1.0, "descriptions": ["Slow animation."]},
                "fast": {"min": 1.0, "max": 2.0, "descriptions": ["Fast animation."]},
            }
        }
    }
    count = 2
    seed = 42  # Add seed argument

    # Prepare mock data for dataset_names and dataset_weights
    dataset_names = ["cap3d"]
    dataset_weights = [1]

    # Mock dataset_dict for generate_objects
    global dataset_dict, object_data, captions_data, background_names, background_weights, background_dict
    dataset_dict = {
        "cap3d": ["1", "2", "3"],
    }
    
    # Mock object_data for generate_objects
    object_data = {
        "scales": {
            "small": {"factor": 0.5, "names": ["tiny", "mini"]},
            "medium": {"factor": 1.0, "names": ["regular", "normal"]},
            "large": {"factor": 1.5, "names": ["big", "huge"]}
        },
        "name_description_relationship": [
            "<name> is a <size> object. <description>"
        ],
        "object_list_intro": [
            "The scene features <object_list>",
            "Present in this scene are <object_list>",
            "Objects visible in the environment include <object_list>"
        ]
    }

    # Mock captions_data for generate_objects
    captions_data = {
        "1": "This is a caption for Object1.",
        "2": "This is a caption for Object2.",
        "3": "This is a caption for Object3.",
    }

    # Mock background names and weights
    background_names = ["background1", "background2"]
    background_weights = [1, 1]

    # Mock background_dict
    background_dict = {
        "background1": {
            "1": {"name": "Sky", "url": "http://example.com/sky"},
            "2": {"name": "Mountains", "url": "http://example.com/mountains"},
        },
        "background2": {
            "1": {"name": "Forest", "url": "http://example.com/forest"},
            "2": {"name": "Beach", "url": "http://example.com/beach"},
        }
    }

    # Mock texture_data
    texture_data = {
        "texture1": {"name": "Wood", "maps": ["diffuse", "normal"]},
        "texture2": {"name": "Metal", "maps": ["diffuse", "normal", "specular"]}
    }

    combinations = generate_combinations(camera_data, 
                                         count, 
                                         seed, 
                                         dataset_names, 
                                         dataset_weights, 
                                         object_data, 
                                         dataset_dict, 
                                         captions_data, 
                                         background_dict,
                                         background_names, 
                                         background_weights,
                                         texture_data,
                                        )
    
    assert (
        len(combinations["combinations"]) == count
    ), "generate_combinations did not create the correct number of combinations."
    for combination in combinations["combinations"]:
        assert (
            combination["orientation"]["yaw"] <= 180
        ), "Yaw is out of the specified range."
        assert (
            combination["orientation"]["pitch"] <= 90
        ), "Pitch is out of the specified range."
        print("============ Test Passed: test_generate_combinations ============")



def test_generate_stage_captions():
    combination = {
        "background": {"name": "Forest"},
        "stage": {"material": {"name": "Grass"}},
    }

    with patch("simian.combiner.read_json_file", return_value=stage_data):
        captions = generate_stage_captions(combination)

        # Extract background and material prefixes from captions
        background_prefixes = stage_data["background_names"]
        material_prefixes = stage_data["material_names"]

        background_caption_found = any(
            f"The {prefix} is Forest." in captions for prefix in background_prefixes
        )
        material_caption_found = any(
            f"The {prefix} is Grass." in captions for prefix in material_prefixes
        )

        assert (
            background_caption_found
        ), "Stage caption did not include correct background prefix."
        assert (
            material_caption_found
        ), "Stage caption did not include correct material prefix."

        print("============ Test Passed: test_generate_stage_captions ============")


def test_generate_orientation_caption():
    combination = {"orientation": {"pitch": 15, "yaw": 90}}

    with patch("simian.combiner.read_json_file", return_value=camera_data):
        caption = generate_orientation_caption(camera_data, combination)

        # Extract pitch and yaw labels from the camera_data
        pitch_labels = camera_data["orientation"]["labels"]["pitch"]
        yaw_labels = camera_data["orientation"]["labels"]["yaw"]

        # Ensure the correct pitch and yaw values exist in the labels
        correct_pitch_label = pitch_labels.get("15", [])
        correct_yaw_label = yaw_labels.get("90", [])

        # Replace <degrees> placeholder with the actual yaw value
        correct_pitch_label = [
            label.replace("<degrees>", str(combination["orientation"]["pitch"]))
            for label in correct_pitch_label
        ]
        correct_yaw_label = [
            label.replace("<degrees>", str(combination["orientation"]["yaw"]))
            for label in correct_yaw_label
        ]

        # Print statements for debugging
        print("Correct pitch label:", correct_pitch_label)
        print("Correct yaw label:", correct_yaw_label)
        print("Generated caption:", caption)

        # Check if any of the correct pitch and yaw labels are in the caption
        pitch_caption_found = any(label in caption for label in correct_pitch_label)
        yaw_caption_found = any(label in caption for label in correct_yaw_label)

        assert (
            pitch_caption_found
        ), "Orientation caption does not include correct pitch label."
        assert (
            yaw_caption_found
        ), "Orientation caption does not include correct yaw label."

        print(
            "============ Test Passed: test_generate_orientation_caption ============"
        )


def test_generate_object_name_description_captions():
    combination = {
        "objects": [
            {
                "name": "A simple box",
                "description": "A simple box",
                "scale": {
                    "factor": 0.75,
                    "name": "small-medium",
                    "name_synonym": "medium-small",
                },
            }
        ]
    }

    object_data = {
        "scales": {
            "small": {"factor": 0.5, "names": ["tiny", "mini"]},
            "medium": {"factor": 1.0, "names": ["regular", "normal"]},
            "large": {"factor": 1.5, "names": ["big", "huge"]}
        },
        "name_description_relationship": [
            "<name> is a <size> object."
        ]
    }

    captions = generate_object_name_description_captions(combination, object_data)

    # Extract the expected relationship template from the object_data
    expected_relationship_templates = object_data["name_description_relationship"]

    # Construct possible expected captions
    expected_captions = [
        template.replace("<name>", "A simple box").replace("<description>", "A simple box").replace("<size>", "medium-small")
        for template in expected_relationship_templates
    ]

    # Check if any of the expected captions are in the generated caption
    caption_found = any(
        expected_caption in captions for expected_caption in expected_captions
    )

    assert caption_found, "Object name description caption is incorrect."

    print(
        "============ Test Passed: test_generate_object_name_description_captions ============"
    )


def test_generate_relationship_captions():
    combination = {
        "objects": [{"name": "Box"}, {"name": "Ball"}],
        "orientation": {"yaw": 0},
    }

    with patch(
        "simian.combiner.adjust_positions",
        return_value=[
            {"transformed_position": [0, 0]},
            {"transformed_position": [1, 0]},
        ],
    ):
        with patch(
            "simian.combiner.determine_relationships", return_value=["Box is near Ball"]
        ):
            captions = generate_relationship_captions(combination)
            print("Generated captions:", captions)  # Add this line for debugging
            assert "Box is near Ball" in captions, "Relationship caption is incorrect."

    print("============ Test Passed: test_generate_relationship_captions ============")


def test_generate_fov_caption():
    combination = {"framing": {"fov": 45}}

    with patch("random.choice", side_effect=["degrees", "The camera has a <fov> degree field of view."]):
        caption = generate_fov_caption(combination)

        # Extract the FOV templates from the function (hardcoded here for simplicity)
        fov_templates_degrees = [
            "The camera has a <fov> degree field of view.",
            "The camera has a <fov> degree FOV.",
            "The field of view is <fov> degrees.",
            "Set the fov of the camera to <fov> degrees.",
            "Set the FOV of the camera to <fov>°",
        ]
        fov_templates_mm = [
            "The camera has a <mm> mm focal length.",
            "The focal length is <mm> mm.",
            "Set the focal length of the camera to <mm> mm.",
        ]

        # Construct possible expected captions
        expected_captions_degrees = [
            template.replace("<fov>", "45") for template in fov_templates_degrees
        ]
        focal_length = 35 / (2 * math.tan(math.radians(45) / 2))  # Calculate the focal length
        expected_captions_mm = [
            template.replace("<mm>", str(focal_length)) for template in fov_templates_mm
        ]

        # Check if any of the expected captions are in the generated caption
        caption_found_degrees = any(
            expected_caption in caption
            for expected_caption in expected_captions_degrees
        )
        caption_found_mm = any(
            expected_caption in caption for expected_caption in expected_captions_mm
        )

        assert caption_found_degrees or caption_found_mm, "FOV caption is incorrect."
    print("============ Test Passed: test_generate_fov_caption ============")


def test_generate_postprocessing():
    camera_data = {
        "postprocessing": {
            "bloom": {
                "threshold_min": 0.1,
                "threshold_max": 0.9,
                "intensity_min": 0.1,
                "intensity_max": 0.9,
                "radius_min": 0.1,
                "radius_max": 0.9,
                "types": {
                    "type1": {"intensity_min": 0.1, "intensity_max": 0.5},
                    "type2": {"intensity_min": 0.6, "intensity_max": 0.9},
                },
            },
            # Add similar data for ssao, ssrr, and motionblur
        }
    }

    with patch("random.uniform", side_effect=[0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4]):
        postprocessing = generate_postprocessing(camera_data)

        assert postprocessing["bloom"]["threshold"] == 0.3
        assert postprocessing["bloom"]["intensity"] == 0.4
        assert postprocessing["bloom"]["radius"] == 0.5
        assert postprocessing["bloom"]["type"] == "type1"

        # Add similar assertions for ssao, ssrr, and motionblur
    print("============ Test Passed: test_generate_postprocessing ============")


def test_generate_framing_caption():
    combination = {"framing": {"name": "wide", "fov": 45, "coverage_factor": 1}}

    with patch("simian.combiner.read_json_file", return_value=camera_data):
        caption = generate_framing_caption(camera_data, combination)

        # Extract the framing descriptions from the camera_data
        framing_templates = [
            f["descriptions"] for f in camera_data["framings"] if f["name"] == "wide"
        ][0]

        # Construct possible expected captions
        expected_captions = [
            template.replace("<fov>", "45").replace("<coverage_factor>", "1")
            for template in framing_templates
        ]

        # Check if any of the expected captions are in the generated caption
        caption_found = any(
            expected_caption in caption for expected_caption in expected_captions
        )

        assert caption_found, "Framing caption is incorrect."

    print("============ Test Passed: test_generate_framing_caption ============")


def test_flatten_descriptions():
    descriptions = [["part1", "part2"], ["part3", "part4"], "part5"]
    flat_descriptions = flatten_descriptions(descriptions)
    assert flat_descriptions == [
        "part1",
        "part2",
        "part3",
        "part4",
        "part5",
    ], "Flatten descriptions is incorrect."
    print("============ Test Passed: test_flatten_descriptions ============")


def test_generate_animation_captions():
    combination = {
        "animation": {
            "name": "zoom_in",
            "speed_factor": 1.5
        }
    }

    camera_data = {
        "animations": [
            {
                "name": "zoom_in",
                "descriptions": [
                    "The camera zooms in.",
                    "Move closer to the scene.",
                    "Zoom in to the scene."
                ]
            }
        ],
        "animation_speed": {
            "types": {
                "slow": {"min": 0.5, "max": 1.0, "descriptions": ["Slow animation speed <animation_speed_value> applied."]},
                "fast": {"min": 1.0, "max": 2.0, "descriptions": ["Fast animation speed <animation_speed_value> applied."]}
            }
        }
    }

    captions = generate_animation_captions(combination, camera_data)

    assert len(captions) == 1, f"Expected 1 caption, but got {len(captions)}"
    generated_caption = captions[0]

    # Check if the generated caption contains a valid animation description
    animation_descriptions = next(anim['descriptions'] for anim in camera_data['animations'] if anim['name'] == combination['animation']['name'])
    animation_part_valid = any(desc.lower() in generated_caption.lower() for desc in animation_descriptions)
    assert animation_part_valid, f"Generated caption doesn't contain a valid animation description for {combination['animation']['name']}"

    # Check if the generated caption contains a valid speed description
    speed_factor = combination['animation']['speed_factor']
    speed_type = next((t for t, v in camera_data['animation_speed']['types'].items() if v['min'] <= speed_factor <= v['max']), None)
    assert speed_type, f"No matching speed type found for speed factor: {speed_factor}"

    speed_descriptions = camera_data['animation_speed']['types'][speed_type]['descriptions']
    speed_part_valid = any(desc.lower().replace('<animation_speed_value>', f"{speed_factor:.2f}") in generated_caption.lower() for desc in speed_descriptions)
    assert speed_part_valid, f"Generated caption doesn't contain a valid speed description for speed type: {speed_type}"

    print("============ Test Passed: test_generate_animation_captions ============")

    
def test_generate_postprocessing():
    camera_data = {
        "postprocessing": {
            "bloom": {
                "threshold_min": 0,
                "threshold_max": 1,
                "intensity_min": 0,
                "intensity_max": 1,
                "radius_min": 0,
                "radius_max": 10,
                "types": {"low": {"intensity_min": 0, "intensity_max": 1}},
            },
            "ssao": {
                "distance_min": 0,
                "distance_max": 1,
                "factor_min": 0,
                "factor_max": 1,
                "types": {"low": {"factor_min": 0, "factor_max": 1}},
            },
            "ssrr": {
                "min_max_roughness": 0,
                "max_max_roughness": 1,
                "min_thickness": 0,
                "max_thickness": 1,
                "types": {"low": {"max_roughness_min": 0, "max_roughness_max": 1}},
            },
            "motionblur": {
                "shutter_speed_min": 0,
                "shutter_speed_max": 1,
                "types": {"low": {"shutter_speed_min": 0, "shutter_speed_max": 1}},
            },
        }
    }
    postprocessing = generate_postprocessing(camera_data)
    assert "bloom" in postprocessing, "Postprocessing generation is incorrect."
    assert "ssao" in postprocessing, "Postprocessing generation is incorrect."
    assert "ssrr" in postprocessing, "Postprocessing generation is incorrect."
    assert "motionblur" in postprocessing, "Postprocessing generation is incorrect."
    print("============ Test Passed: test_generate_postprocessing ============")


def test_generate_orientation():
    camera_data = {
        "orientation": {"yaw_min": 0, "yaw_max": 180, "pitch_min": -90, "pitch_max": 90}
    }
    objects = [{"transformed_position": [0, 0]}, {"transformed_position": [1, 0]}]
    background = {"name": "Sky"}
    with patch("simian.combiner.adjust_positions", return_value=objects):
        orientation = generate_orientation(camera_data, objects, background)
        assert orientation["yaw"] <= 180, "Orientation yaw is out of range."
        assert orientation["pitch"] <= 90, "Orientation pitch is out of range."
    print("============ Test Passed: test_generate_orientation ============")


def test_generate_framing():
    camera_data = {
        "framings": [
            {
                "name": "wide",
                "fov_min": 30,
                "fov_max": 60,
                "coverage_factor_min": 1,
                "coverage_factor_max": 2,
            }
        ]
    }
    framing = generate_framing(camera_data)
    assert framing["fov"] >= 30, "FOV is out of range."
    assert framing["fov"] <= 60, "FOV is out of range."
    assert framing["name"] == "wide", "Framing name is incorrect."
    print("============ Test Passed: test_generate_framing ============")


def test_generate_animation():
    camera_data = {
        "animations": [{"name": "zoom_in", "keyframes": [{"start": 0, "end": 1}]}]
    }
    animation = generate_animation(camera_data)
    assert animation["name"] == "zoom_in", "Animation name is incorrect."
    assert "speed_factor" in animation, "Animation speed_factor is missing."
    print("============ Test Passed: test_generate_animation ============")


def test_generate_objects():
    """
    Test the generate_objects function.
    """
    # Mock data for object_data, dataset_dict, and captions_data
    object_data = {
        "scales": {
            "small": {"factor": 0.5, "names": ["tiny", "mini"]},
            "medium": {"factor": 1.0, "names": ["regular", "normal"]},
            "large": {"factor": 1.5, "names": ["big", "huge"]}
        },
        "name_description_relationship": [
            "<name> is a <size> object. <description>"
        ]
    }

    dataset_dict = {
        "cap3d": ["1", "2", "3"],
    }

    captions_data = {
        "1": "This is a caption for Object1.",
        "2": "This is a caption for Object2.",
        "3": "This is a caption for Object3.",
    }

    # Mock dataset_names and dataset_weights
    dataset_names = ["cap3d"]
    dataset_weights = [1]

    ontop_data = "none"

    # Generate objects
    objects = generate_objects(object_data, dataset_names, dataset_weights, dataset_dict, captions_data, ontop_data)

    # Ensure the function generates the correct number of objects
    assert len(objects) > 0, "Objects generation should create more than 1 object."

    # Ensure at least one object has placement 4
    assert any(obj["placement"] == 4 for obj in objects), "There should be at least one object with placement 4."

    # Ensure all objects have the required fields
    required_fields = ["name", "uid", "description", "placement", "from", "scale"]
    for obj in objects:
        for field in required_fields:
            assert field in obj, f"Missing field: {field}"

    print("============ Test Passed: test_generate_objects ============")


def test_generate_background():
    background_dict = {
        "background1": {
            "id1": {"name": "Sky", "url": "sky_url"}
        }
    }
    background_names = ["background1"]
    background_weights = [1]

    with patch("simian.combiner.random.choices", return_value=["background1"]):
        with patch("simian.combiner.random.choice", return_value="id1"):
            background = generate_background(background_dict, background_names, background_weights)
            assert background["name"] == "Sky", "Background name is incorrect."
            assert background["url"] == "sky_url", "Background url is incorrect."
    
    print("============ Test Passed: test_generate_background ============")


def test_generate_stage():
    texture_data = {
        "texture1": {"name": "Brick", "maps": {"diffuse": "brick_diffuse"}}
    }
    
    with patch("simian.combiner.random.choices", return_value=["texture1"]):
        with patch("simian.combiner.random.choice", return_value="diffuse"):
            stage = generate_stage(texture_data)
            assert stage["material"]["name"] == "Brick", "Stage material name is incorrect."
            assert "uv_scale" in stage, "Stage uv_scale is missing."
            assert "uv_rotation" in stage, "Stage uv_rotation is missing."
    
    print("============ Test Passed: test_generate_stage ============")


if __name__ == "__main__":
    test_read_json_file()
    test_combination_caption()
    test_generate_combinations()
    test_generate_stage_captions()
    test_generate_orientation_caption()
    test_generate_object_name_description_captions()
    test_generate_relationship_captions()
    test_generate_fov_caption()
    test_generate_framing_caption()
    test_flatten_descriptions()
    test_generate_animation_captions()
    test_generate_postprocessing()
    test_generate_postprocessing_caption()
    test_generate_orientation()
    test_generate_framing()
    test_generate_animation()
    test_generate_objects()
    test_generate_background()
    test_generate_stage()
    print("============ ALL TESTS PASSED ============")

```````

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/tests/camera_test.py`:

```````py
import math

import bpy
from ..camera import (
    create_camera_rig,
    set_camera_settings,
    position_camera,
)
from ..scene import initialize_scene


def test_create_camera_rig():
    """
    Test the create_camera_rig function.
    """

    EPSILON = 1e-6  # Small value to account for floating-point inaccuracies
    rig = create_camera_rig()  # call the function

    # Check if all the expected objects are created
    assert isinstance(rig["camera_animation_root"], bpy.types.Object)
    assert isinstance(rig["camera_orientation_pivot_yaw"], bpy.types.Object)
    assert isinstance(rig["camera_orientation_pivot_pitch"], bpy.types.Object)
    assert isinstance(rig["camera_framing_pivot"], bpy.types.Object)
    assert isinstance(rig["camera_animation_pivot"], bpy.types.Object)
    assert isinstance(rig["camera_object"], bpy.types.Object)
    assert isinstance(rig["camera"], bpy.types.Camera)

    # Check if the camera is set correctly
    assert bpy.context.scene.camera == rig["camera_object"]

    # Check if the camera rotation is set correctly
    camera_rotation = rig["camera_object"].delta_rotation_euler

    # Expected rotation values
    expected_x_rotation = 1.5708  # 90 degrees
    expected_y_rotation = 0.0
    expected_z_rotation = 1.5708  # 90 degrees

    # Check rotation values with epsilon error
    assert math.isclose(camera_rotation[0], expected_x_rotation, abs_tol=EPSILON)
    assert math.isclose(camera_rotation[1], expected_y_rotation, abs_tol=EPSILON)
    assert math.isclose(camera_rotation[2], expected_z_rotation, abs_tol=EPSILON)
    print("============ Test Passed: test_create_camera_rig ============")


def test_set_camera_settings():
    """
    Test the set_camera_settings function.
    """

    initialize_scene()
    create_camera_rig()

    combination = {
        "orientation": {"yaw": 327, "pitch": 14},
        "framing": {"fov": 20, "coverage_factor": 1.0},
        "animation": {
            "name": "tilt_left",
            "keyframes": [
                {
                    "CameraAnimationRoot": {"rotation": [0, 0, 45]},
                    "Camera": {"angle_offset": 5},
                },
                {"CameraAnimationRoot": {"rotation": [0, 0, 0]}},
            ],
        },
    }

    # Retrieve the camera object from the Blender scene
    camera = bpy.data.objects["Camera"].data

    # set the camera lens_unit type to FOV
    camera.lens_unit = "FOV"

    # Call the function with full data
    set_camera_settings(combination)

    # Retrieve the orientation pivot objects
    camera_orientation_pivot_yaw = bpy.data.objects["CameraOrientationPivotYaw"]
    camera_orientation_pivot_pitch = bpy.data.objects["CameraOrientationPivotPitch"]
    print("camera.angle")
    print(camera.angle)
    print('combination["framing"]["fov"]')
    print(math.radians(combination["framing"]["fov"]))
    print(
        'combination["framing"]["fov"] + combination["animation"]["keyframes"][0]["Camera"]["angle_offset"]'
    )
    print(
        math.radians(
            combination["framing"]["fov"]
            + combination["animation"]["keyframes"][0]["Camera"]["angle_offset"]
        )
    )
    fov = math.radians(
        combination["framing"]["fov"]
        + combination["animation"]["keyframes"][0]["Camera"]["angle_offset"]
    )
    # Assert the field of view is set correctly
    epsilon = 0.0001
    assert (
        camera.angle <= fov + epsilon and camera.angle >= fov - epsilon
    ), "FOV is not set correctly"

    # Convert degrees to radians for comparison
    expected_yaw_radians = math.radians(combination["orientation"]["yaw"])
    expected_pitch_radians = -math.radians(
        combination["orientation"]["pitch"]
    )  # Negative for Blender's coordinate system

    # Assert the orientation is set correctly
    assert math.isclose(
        camera_orientation_pivot_yaw.rotation_euler[2],
        expected_yaw_radians,
        abs_tol=0.001,
    ), "Yaw is not set correctly"
    assert math.isclose(
        camera_orientation_pivot_pitch.rotation_euler[1],
        expected_pitch_radians,
        abs_tol=0.001,
    ), "Pitch is not set correctly"
    print("============ Test Passed: test_set_camera_settings ============")


def test_position_camera():
    combination = {
        "framing": {"fov": 20, "coverage_factor": 1.0},
        "animation": {
            "keyframes": [
                {
                    "Camera": {
                        "position": (0, 0, 5),
                        "rotation": (0, 0, 0),
                        "angle_offset": 5,
                    }
                },
                {
                    "Camera": {
                        "position": (5, 0, 0),
                        "rotation": (0, 0, 90),
                        "angle_offset": 10,
                    }
                },
            ]
        },
    }

    # Create a dummy object to represent the focus object
    bpy.ops.mesh.primitive_cube_add(size=2)
    focus_object = bpy.context.active_object

    position_camera(combination, focus_object)

    camera = bpy.data.objects.get("Camera")
    assert camera is not None, "Camera object not found"

    # TODO: add more asserts here

    print("============ Test Passed: test_position_camera ============")


if __name__ == "__main__":
    test_create_camera_rig()
    test_set_camera_settings()
    # test_set_camera_animation()
    test_position_camera()
    print("============ ALL TESTS PASSED ============")

```````

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/tests/new_camera_test.py`:

```````py
import bpy
import math
from mathutils import Vector

from ..camera import create_camera_rig


def calculate_optimal_distance(camera, obj):
    """
    Calculate the optimal distance between the camera and an object
    so that the entire object is within the camera's frame, considering the object's depth.

    Args:
        camera (bpy.types.Object): The camera object.
        obj (bpy.types.Object): The object to be framed.

    Returns:
        float: The optimal distance between the camera and the object.
    """
    # Get the camera's field of view
    fov_h = camera.data.angle_x
    fov_v = camera.data.angle_y

    # Calculate the object's bounding box dimensions in world space
    bbox = [obj.matrix_world @ Vector(corner) for corner in obj.bound_box]
    bbox_dimensions = [max(coord) - min(coord) for coord in zip(*bbox)]
    obj_width, obj_height, obj_depth = bbox_dimensions

    # Find the diagonal of the bounding box which will require the farthest distance
    diagonal = math.sqrt(obj_width**2 + obj_height**2 + obj_depth**2)

    # Calculate the required distance to fit the diagonal in view
    fov_average = (fov_h + fov_v) / 2
    distance = (diagonal / 2) / math.tan(fov_average / 2)

    return distance


def position_camera_for_object(camera, obj):
    """
    Position the camera so that all vertices of the object are in frame or at the frame border.

    Args:
        camera (bpy.types.Object): The camera object.
        obj (bpy.types.Object): The object to frame.
    """
    optimal_distance = calculate_optimal_distance(camera, obj)
    camera.location = obj.location + Vector((optimal_distance, 0, 0))
    camera.rotation_euler = (0, 0, 0)  # Reset rotation or set as needed


def is_vertex_in_frame(camera, vertex):
    """
    Check if a vertex is within the camera's frame.

    Args:
        camera (bpy.types.Object): The camera object.
        vertex (mathutils.Vector): The vertex to check.

    Returns:
        bool: True if the vertex is within the camera's frame, False otherwise.
    """
    # Transform the vertex to camera space
    camera_matrix = camera.matrix_world.inverted()
    vertex_camera = camera_matrix @ vertex

    # Check if the vertex is within the camera's view frustum
    fov_h = camera.data.angle_x
    fov_v = camera.data.angle_y
    aspect_ratio = camera.data.sensor_width / camera.data.sensor_height

    # Calculate tangent values
    tan_fov_h_half = math.tan(fov_h / 2)
    tan_fov_v_half = math.tan(fov_v / 2)

    # Calculate the near plane dimensions
    near_plane_width = 2 * tan_fov_h_half * vertex_camera.z
    near_plane_height = 2 * tan_fov_v_half * vertex_camera.z

    # Check if the vertex is within the near plane
    x_relative = vertex_camera.x / near_plane_width
    y_relative = vertex_camera.y / near_plane_height

    return abs(x_relative) <= 0.5 and abs(y_relative) <= 0.5


def test_optimal_distance():
    # Clear existing objects
    bpy.ops.object.select_all(action="SELECT")
    bpy.ops.object.delete()

    # Create the camera rig
    dict = create_camera_rig()
    camera = dict["camera_object"]

    # Set camera properties
    camera.data.sensor_width = 36
    camera.data.sensor_height = 24
    camera.data.lens_unit = "FOV"
    camera.data.angle = math.radians(35)

    # Define different cube dimensions
    cube_dimensions = [(1, 10, 1), (10, 1, 1), (1, 1, 10)]

    # Iterate over each set of dimensions
    for i, (width, depth, height) in enumerate(cube_dimensions, start=1):
        # Create a test object (cube) with specific dimensions
        bpy.ops.mesh.primitive_cube_add(size=1, location=(0, 0, 0))
        cube = bpy.context.active_object
        cube.scale = (
            width / 2,
            depth / 2,
            height / 2,
        )  # Blender uses half-dimensions for scale
        bpy.context.view_layer.update()

        # Apply scale to ensure the transformations are correct
        bpy.ops.object.transform_apply(location=False, rotation=False, scale=True)

        # Position the camera for the cube
        position_camera_for_object(camera, cube)

        all_vertices_in_frame = True
        for vertex in cube.data.vertices:
            world_vertex = cube.matrix_world @ vertex.co
            if not is_vertex_in_frame(camera, world_vertex):
                all_vertices_in_frame = False
                print(f"Cube {i}: Vertex {vertex.co} is outside the camera's frame.")

        if all_vertices_in_frame:
            print(f"All vertices of Cube {i} are within the camera's frame.")
        else:
            print(f"Some vertices of Cube {i} are outside the camera's frame.")

        # Remove the cube after rendering to prepare for the next one
        bpy.data.objects.remove(cube, do_unlink=True)

    print("Test completed.")


if __name__ == "__main__":
    test_optimal_distance()

```````

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/tests/worker_test.py`:

```````py
import json
import sys
from unittest.mock import patch

from ..worker import run_job


@patch("..worker.subprocess.run")
def test_run_job(mock_subprocess_run):
    combination_index = 0
    combination = {"objects": []}
    width = 1920
    height = 1080
    output_dir = "test_output"
    hdri_path = "test_hdri"
    upload_dest = "hf"
    start_frame = 0
    end_frame = 10

    run_job(
        combination_index,
        combination,
        width,
        height,
        output_dir,
        hdri_path,
        upload_dest,
        start_frame,
        end_frame
    )

    combination_str = json.dumps(combination)
    combination_str = '"' + combination_str.replace('"', '\\"') + '"'

    command = f"{sys.executable} -m simian.render -- --width {width} --height {height} --combination_index {combination_index}"
    command += f" --output_dir {output_dir}"
    command += f" --hdri_path {hdri_path}"
    command += f" --start_frame {start_frame} --end_frame {end_frame}"
    command += f" --combination {combination_str}"

    mock_subprocess_run.assert_called_once_with(["bash", "-c", command], check=False)


if __name__ == "__main__":
    # test_run_job()
    pass

```````

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/tests/object_test.py`:

```````py
import os

from ..camera import create_camera_rig
from ..scene import initialize_scene
from ..object import (
    delete_all_empties,
    get_hierarchy_bbox,
    join_objects_in_hierarchy,
    lock_all_objects,
    normalize_object_scale,
    optimize_meshes_in_hierarchy,
    get_meshes_in_hierarchy,
    set_pivot_to_bottom,
    unlock_objects,
    unparent_keep_transform,
)
import bpy


def test_hierarchy_bbox():
    """
    Test the get_hierarchy_bbox function.
    """
    # Load an empty scene
    initialize_scene()

    # Create two cubes 1 meter apart
    bpy.ops.mesh.primitive_cube_add(size=2, location=(0, 0, 0))
    cube1 = bpy.context.active_object
    bpy.ops.mesh.primitive_cube_add(size=2, location=(3, 0, 0))
    cube2 = bpy.context.active_object

    # Create an empty and parent cube1 to it
    bpy.ops.object.empty_add(location=(0, 0, 0))
    empty1 = bpy.context.active_object
    cube1.parent = empty1

    # Create a second empty, parent it to the first empty, and parent cube2 to this second empty
    bpy.ops.object.empty_add(location=(0, 0, 0))
    empty2 = bpy.context.active_object
    empty2.parent = empty1
    cube2.parent = empty2

    # Call the function on the root empty
    min_coord, max_coord = get_hierarchy_bbox(empty1)

    # The expected width of the bounding box should be the distance between the centers plus the size of one cube (since both cubes have a center at their geometric center)
    expected_width = 5  # 3 meters apart plus 1 meter half-width of each cube
    calculated_width = max_coord[0] - min_coord[0]

    # Assert to check if the calculated width matches the expected width
    assert (
        abs(calculated_width - expected_width) < 0.001
    ), "Bounding box width is incorrect"
    print("============ Test Passed: test_hierarchy_bbox ============")


def test_remove_small_geometry():
    """
    Test the remove_small_geometry function.
    """
    current_dir = os.path.dirname(__file__)

    # Load an empty scene
    initialize_scene()
    create_camera_rig()

    initial_objects = lock_all_objects()

    # Path to the GLB file
    glb_path = os.path.join(current_dir, "../", "../", "examples", "dangling_parts.glb")

    # Load the model
    bpy.ops.import_scene.gltf(filepath=glb_path)

    # Assume the last object added is the root object (this may need to be adjusted based on actual structure)
    root_obj = bpy.data.objects[0]

    optimize_meshes_in_hierarchy(root_obj)

    join_objects_in_hierarchy(root_obj)

    meshes = get_meshes_in_hierarchy(root_obj)
    obj = meshes[0]

    set_pivot_to_bottom(obj)
    unparent_keep_transform(obj)
    normalize_object_scale(obj)
    delete_all_empties()

    # set position to 0, 0, 0
    obj.location = (0, 0, 0)

    # go back to object mode
    bpy.ops.object.mode_set(mode="OBJECT")

    unlock_objects(initial_objects)

    # Assert that the resulting object has fewer vertices than the initial count
    assert len(obj.data.vertices) > 0
    assert len(meshes) == 1
    print("============ Test Passed: test_remove_small_geometry ============")


def test_normalize_object_scale():
    """
    Test the normalize_object_scale function.
    """
    # Load an empty scene
    initialize_scene()
    # Create a cube with known dimensions
    bpy.ops.mesh.primitive_cube_add(size=2)  # Creates a cube with edge length 2
    cube = bpy.context.active_object

    # Calculate expected scale factor to normalize the cube to unit length (scale_factor / max_dimension)
    scale_factor = 1.0
    expected_dimension = 1.0  # Since we want the largest dimension to be scaled to 1.0

    # Normalize the cube's scale
    normalize_object_scale(cube, scale_factor)

    # Force a scene update to ensure that all transforms are applied
    bpy.context.view_layer.update()

    # Recalculate actual dimensions after scaling
    actual_dimensions = [cube.dimensions[i] for i in range(3)]
    max_dimension_after_scaling = max(actual_dimensions)

    # Assert that the maximum dimension is within a small epsilon of expected_dimension
    epsilon = 0.001  # Small threshold to account for floating point arithmetic errors
    assert (
        abs(max_dimension_after_scaling - expected_dimension) < epsilon
    ), f"Expected max dimension to be close to {expected_dimension}, but got {max_dimension_after_scaling}"
    print("============ Test Passed: test_normalize_object_scale ============")


# Run tests if this file is executed as a script
if __name__ == "__main__":
    test_hierarchy_bbox()
    test_remove_small_geometry()
    test_normalize_object_scale()
    print("============ ALL TESTS PASSED ============")

```````

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/tests/batch_test.py`:

```````py
from ..batch import render_objects


def test_render_objects():
    """
    Test the render_objects function.
    """
    processes = 4
    render_timeout = 3000
    width = 1920
    height = 1080
    start_index = 0
    end_index = 1
    start_frame = 1
    end_frame = 2

    # Call the function
    try:
        render_objects(
            processes=processes,
            render_timeout=render_timeout,
            width=width,
            height=height,
            start_index=start_index,
            end_index=end_index,
            start_frame=start_frame,
            end_frame=end_frame,
        )
        print("============ Test Passed: render_objects ============")
    except Exception as e:
        print("============ Test Failed: render_objects ============")
        print(f"Error: {e}")
        raise e


if __name__ == "__main__":
    test_render_objects()
    print("============ ALL TESTS PASSED ============")

```````

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/background.py`:

```````py
import os
from typing import Dict
import requests
import bpy
import logging

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)


def get_hdri_path(hdri_path: str, combination: Dict) -> str:
    """
    Get the local file path for the background HDR image.

    Args:
        hdri_path (str): The base directory for storing background images.
        combination (Dict): The combination dictionary containing background
    Returns:
        str: The local file path for the background HDR image.
    """
    background = combination["background"]
    background_id = background["id"]
    background_from = background["from"]
    hdri_path = f"{hdri_path}/{background_from}/{background_id}.hdr"

    return hdri_path


def get_background(hdri_path: str, combination: Dict) -> None:
    """
    Download the background HDR image if it doesn't exist locally.

    This function checks if the background HDR image specified in the combination dictionary
    exists locally. If it doesn't exist, it downloads the image from the provided URL and
    saves it to the local file path.

    Args:
        hdri_path (str): The base directory for storing background images.
        combination (Dict): The combination dictionary containing background information.

    Returns:
        None
    """
    hdri_path = get_hdri_path(hdri_path, combination)

    background = combination["background"]
    background_url = background["url"]

    # make sure each folder in the path exists
    os.makedirs(os.path.dirname(hdri_path), exist_ok=True)

    if not os.path.exists(hdri_path):
        # logger.info(f"Downloading {background_url} to {hdri_path}")
        response = requests.get(background_url)
        with open(hdri_path, "wb") as file:
            file.write(response.content)
    # else:
    #     logger.info(f"Background {hdri_path} already exists")


def set_background(hdri_path: str, combination: Dict) -> None:
    """
    Set the background HDR image of the scene.

    This function sets the background HDR image of the scene using the provided combination
    dictionary. It ensures that the world nodes are used and creates the necessary nodes
    (Environment Texture, Background, and World Output) if they don't exist. It then loads
    the HDR image, connects the nodes, and enables the world background in the render settings.

    Args:
        hdri_path (str): The base directory for storing background images.
        combination (Dict): The combination dictionary containing background information.

    Returns:
        None
    """
    get_background(hdri_path, combination)
    hdri_path = get_hdri_path(hdri_path, combination)

    # Check if the scene has a world, and create one if it doesn't
    if bpy.context.scene.world is None:
        bpy.context.scene.world = bpy.data.worlds.new("World")

    # Ensure world nodes are used
    bpy.context.scene.world.use_nodes = True
    tree = bpy.context.scene.world.node_tree

    # Clear existing nodes
    tree.nodes.clear()

    # Create the Environment Texture node
    env_tex_node = tree.nodes.new(type="ShaderNodeTexEnvironment")
    env_tex_node.location = (-300, 0)

    # Load the HDR image
    env_tex_node.image = bpy.data.images.load(hdri_path)

    # Create the Background node
    background_node = tree.nodes.new(type="ShaderNodeBackground")
    background_node.location = (0, 0)

    # Connect the Environment Texture node to the Background node
    tree.links.new(env_tex_node.outputs["Color"], background_node.inputs["Color"])

    # Create the World Output node
    output_node = tree.nodes.new(type="ShaderNodeOutputWorld")
    output_node.location = (300, 0)

    # Connect the Background node to the World Output
    tree.links.new(background_node.outputs["Background"], output_node.inputs["Surface"])

    # Enable the world background in the render settings
    bpy.context.scene.render.film_transparent = False

    # logger.info(f"Set background to {hdri_path}")


def create_photosphere(
    hdri_path: str, combination: Dict, scale: float = 10
) -> bpy.types.Object:
    """
    Create a photosphere object in the scene.

    This function creates a UV sphere object in the scene and positions it at (0, 0, 3).
    It smooths the sphere, inverts its normals, and renames it to "Photosphere". It then
    calls the `create_photosphere_material` function to create a material for the photosphere
    using the environment texture as emission.

    Args:
        hdri_path (str): The base directory for storing background images.
        combination (Dict): The combination dictionary containing background information.

    Returns:
        bpy.types.Object: The created photosphere object.
    """
    bpy.ops.mesh.primitive_uv_sphere_add(
        segments=64, ring_count=32, radius=scale, location=(0, 0, 3)
    )

    bpy.ops.object.shade_smooth()

    # invert the UV sphere normals
    bpy.ops.object.mode_set(mode="EDIT")
    bpy.ops.mesh.select_all(action="SELECT")
    bpy.ops.mesh.flip_normals()
    bpy.ops.object.mode_set(mode="OBJECT")

    sphere = bpy.context.object
    sphere.name = "Photosphere"
    sphere.data.name = "PhotosphereMesh"
    create_photosphere_material(hdri_path, combination, sphere)
    return sphere


def create_photosphere_material(
    hdri_path: str, combination: Dict, sphere: bpy.types.Object
) -> None:
    """
    Create a material for the photosphere object using the environment texture as emission.

    This function creates a new material for the provided photosphere object. It sets up
    the material nodes to use the environment texture as emission and assigns the material
    to the photosphere object.

    Args:
        hdri_path (str): The base directory for storing background images.
        combination (Dict): The combination dictionary containing background information.
        sphere (bpy.types.Object): The photosphere object to assign the material to.

    Returns:
        None
    """
    # Create a new material
    mat = bpy.data.materials.new(name="PhotosphereMaterial")
    mat.use_nodes = True
    nodes = mat.node_tree.nodes
    nodes.clear()

    # Create and connect the nodes
    emission = nodes.new(type="ShaderNodeEmission")
    env_tex = nodes.new(type="ShaderNodeTexEnvironment")
    env_tex.image = bpy.data.images.load(get_hdri_path(hdri_path, combination))
    mat.node_tree.links.new(env_tex.outputs["Color"], emission.inputs["Color"])
    output = nodes.new(type="ShaderNodeOutputMaterial")
    mat.node_tree.links.new(emission.outputs["Emission"], output.inputs["Surface"])

    # Assign material to the sphere
    if sphere.data.materials:
        sphere.data.materials[0] = mat
    else:
        sphere.data.materials.append(mat)

    # logger.info("Material created and applied to Photosphere")

```````

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/__init__.py`:

```````py
from .background import *
from .render import *
from .camera import *
from .distributed import *
from .combiner import *
from .object import *
from .postprocessing import *
from .scene import *
from .transform import *
from .prompts import *
from .server import *
from .worker import *
from .batch import *

```````

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/scene.py`:

```````py
from math import cos, sin
import bpy
from typing import Tuple
import bmesh
import os
import requests


def initialize_scene() -> None:
    # start bpy from scratch
    bpy.ops.wm.read_factory_settings(use_empty=True)

    # delete all objects
    bpy.ops.object.select_all(action="SELECT")

    bpy.ops.object.delete()
    
    # set render mode of blend file to eevee
    bpy.context.scene.render.engine = 'BLENDER_EEVEE'


def download_texture(url: str, material_name: str, texture_name: str) -> str:
    """
    Downloads the texture from the given URL and saves it in the materials/<material_name> folder.
    Returns the local file path of the downloaded texture.

    Args:
        url (str): The URL of the texture to download.
        material_name (str): The name of the material.
        texture_name (str): The name of the texture.

    Returns:
        str: The local file path of the downloaded texture.
    """
    materials_dir = os.path.join("materials", material_name)
    os.makedirs(materials_dir, exist_ok=True)

    local_path = os.path.join(materials_dir, f"{texture_name}.jpg")

    if not os.path.exists(local_path):
        response = requests.get(url)
        with open(local_path, "wb") as file:
            file.write(response.content)

    return local_path


def create_stage(
    combination: dict,
    stage_size: Tuple[int, int] = (100, 100),
    stage_height: float = 0.002,
) -> bpy.types.Object:
    """
    Creates a simple stage object in the scene.

    Args:
        combination (dict): A dictionary containing the stage settings.
        stage_size (Tuple[int, int], optional): The size of the stage in Blender units (width, height). Defaults to (100, 100).
        stage_height (float, optional): The height of the stage above the ground plane. Defaults to 0.002.

    Returns:
        bpy.types.Object: The created stage object.
    """
    # Create a new plane object
    bpy.ops.mesh.primitive_plane_add(size=1)
    stage = bpy.context.active_object

    stage_data = combination.get("stage", {})
    stage_material = stage_data.get("material", {})
    uv_scale = stage_data.get("uv_scale", [1.0, 1.0])
    uv_rotation = stage_data.get("uv_rotation", 0.0)

    # Scale the stage to the desired size
    stage.scale = (stage_size[0], stage_size[1], 1)

    # Set the stage location to be at the bottom of the scene
    stage.location = (0, 0, stage_height)

    # Rename the stage object
    stage.name = "Stage"

    # Rescale the UVs based on the inverse of the scale multiplied by 2
    scale_x = stage_size[0] * uv_scale[0]
    scale_y = stage_size[1] * uv_scale[1]

    # Enter edit mode
    bpy.ops.object.mode_set(mode="EDIT")

    # Get the bmesh representation of the stage
    bm = bmesh.from_edit_mesh(stage.data)

    # Get the UV layer
    uv_layer = bm.loops.layers.uv.verify()

    # convert uv rotation to radians
    uv_rotation = uv_rotation * 3.14159 / 180.0

    # create a 2x2 rotation matrix for the UVs
    rotation_matrix = [
        [cos(uv_rotation), -sin(uv_rotation)],
        [sin(uv_rotation), cos(uv_rotation)],
    ]

    # Iterate over the faces and rescale the UVs
    for face in bm.faces:
        for loop in face.loops:
            uv = loop[uv_layer].uv

            # rotate the UVs by multiplying by the rotation matrix
            uv.x, uv.y = (
                (rotation_matrix[0][0] * uv.x + rotation_matrix[0][1] * uv.y) * scale_x,
                (rotation_matrix[1][0] * uv.x + rotation_matrix[1][1] * uv.y) * scale_y,
            )

    # Update the mesh and return to object mode
    bmesh.update_edit_mesh(stage.data)
    bpy.ops.object.mode_set(mode="OBJECT")
    return stage


def apply_stage_material(stage: bpy.types.Object, combination: dict) -> None:
    """
    Applies the stage material to the given stage object based on the combination settings.

    Args:
        stage (bpy.types.Object): The stage object to apply the material to.
        combination (dict): A dictionary containing the stage material settings.
    """
    # Get the stage material settings from the combination
    stage_data = combination.get("stage", {})
    stage_material = stage_data.get("material", {})
    material_name = stage_material.get("name", "DefaultMaterial")

    # Create a new material for the stage
    material = bpy.data.materials.new(name="StageMaterial")

    # Assign the material to the stage object
    stage.data.materials.append(material)

    # Set the material properties based on the combination settings
    material.use_nodes = True
    nodes = material.node_tree.nodes

    # Clear existing nodes
    for node in nodes:
        nodes.remove(node)

    # Create shader nodes
    output = nodes.new(type="ShaderNodeOutputMaterial")
    principled = nodes.new(type="ShaderNodeBsdfPrincipled")

    # Create texture coordinate and mapping nodes
    tex_coord = nodes.new(type="ShaderNodeTexCoord")
    mapping = nodes.new(type="ShaderNodeMapping")

    # Connect texture coordinate to mapping
    links = material.node_tree.links
    links.new(tex_coord.outputs["UV"], mapping.inputs["Vector"])

    # Load and connect diffuse texture
    if "Diffuse" in stage_material["maps"]:
        diffuse_url = stage_material["maps"]["Diffuse"]
        diffuse_path = download_texture(diffuse_url, material_name, "Diffuse")
        diffuse_tex = nodes.new(type="ShaderNodeTexImage")
        diffuse_tex.image = bpy.data.images.load(diffuse_path)
        links.new(mapping.outputs["Vector"], diffuse_tex.inputs["Vector"])
        links.new(diffuse_tex.outputs["Color"], principled.inputs["Base Color"])

    # Load and connect normal texture
    if "nor_gl" in stage_material["maps"]:
        normal_url = stage_material["maps"]["nor_gl"]
        normal_path = download_texture(normal_url, material_name, "Normal")
        normal_tex = nodes.new(type="ShaderNodeTexImage")
        normal_tex.image = bpy.data.images.load(normal_path)
        normal_map = nodes.new(type="ShaderNodeNormalMap")
        links.new(mapping.outputs["Vector"], normal_tex.inputs["Vector"])
        links.new(normal_tex.outputs["Color"], normal_map.inputs["Color"])
        links.new(normal_map.outputs["Normal"], principled.inputs["Normal"])

    if "AO" in stage_material["maps"]:
        ao_url = stage_material["maps"]["AO"]
        ao_path = download_texture(ao_url, material_name, "AO")
        ao_tex = nodes.new(type="ShaderNodeTexImage")
        ao_tex.image = bpy.data.images.load(ao_path)
        mixRGB = nodes.new(type="ShaderNodeMixRGB")
        mixRGB.blend_type = "MULTIPLY"
        links.new(ao_tex.outputs["Color"], mixRGB.inputs["Color2"])
        links.new(mapping.outputs["Vector"], ao_tex.inputs["Vector"])

        # Connect the MixRGB node to the base color input
        if "Diffuse" in stage_material["maps"]:
            links.new(diffuse_tex.outputs["Color"], mixRGB.inputs["Color1"])
            links.new(mixRGB.outputs["Color"], principled.inputs["Base Color"])
    else:
        # If no diffuse texture, use a default base color
        principled.inputs["Base Color"].default_value = (1.0, 1.0, 1.0, 1.0)
        links.new(ao_tex.outputs["Color"], mixRGB.inputs["Color1"])
        links.new(mixRGB.outputs["Color"], principled.inputs["Base Color"])

    if "Rough" in stage_material["maps"]:
        rough_url = stage_material["maps"]["Rough"]
        rough_path = download_texture(rough_url, material_name, "Rough")
        rough_tex = nodes.new(type="ShaderNodeTexImage")
        rough_tex.image = bpy.data.images.load(rough_path)
        links.new(mapping.outputs["Vector"], rough_tex.inputs["Vector"])
        links.new(rough_tex.outputs["Color"], principled.inputs["Roughness"])

    if "Roughness" in stage_material["maps"]:
        roughness_url = stage_material["maps"]["Roughness"]
        roughness_path = download_texture(roughness_url, material_name, "Roughness")
        roughness_tex = nodes.new(type="ShaderNodeTexImage")
        roughness_tex.image = bpy.data.images.load(roughness_path)
        links.new(mapping.outputs["Vector"], roughness_tex.inputs["Vector"])
        links.new(roughness_tex.outputs["Color"], principled.inputs["Roughness"])

    if "arm" in stage_material["maps"]:
        arm_url = stage_material["maps"]["arm"]
        arm_path = download_texture(arm_url, material_name, "Arm")
        arm_tex = nodes.new(type="ShaderNodeTexImage")
        arm_tex.image = bpy.data.images.load(arm_path)
        links.new(mapping.outputs["Vector"], arm_tex.inputs["Vector"])

        # Create separate RGB nodes for ambient occlusion, roughness, and metallic
        ao_rgb = nodes.new(type="ShaderNodeSeparateRGB")
        rough_rgb = nodes.new(type="ShaderNodeSeparateRGB")
        metal_rgb = nodes.new(type="ShaderNodeSeparateRGB")

        # Connect the "arm" texture to the separate RGB nodes
        links.new(arm_tex.outputs["Color"], ao_rgb.inputs["Image"])
        links.new(arm_tex.outputs["Color"], rough_rgb.inputs["Image"])
        links.new(arm_tex.outputs["Color"], metal_rgb.inputs["Image"])

        # Multiply the ambient occlusion with the base color
        mixRGB = nodes.new(type="ShaderNodeMixRGB")
        mixRGB.blend_type = "MULTIPLY"
        links.new(ao_rgb.outputs["R"], mixRGB.inputs["Color2"])

        if "Diffuse" in stage_material["maps"]:
            links.new(diffuse_tex.outputs["Color"], mixRGB.inputs["Color1"])
            links.new(mixRGB.outputs["Color"], principled.inputs["Base Color"])
        else:
            # If no diffuse texture, use a default base color
            principled.inputs["Base Color"].default_value = (0.8, 0.8, 0.8, 1.0)
            links.new(ao_rgb.outputs["R"], mixRGB.inputs["Color1"])
            links.new(mixRGB.outputs["Color"], principled.inputs["Base Color"])

        # Connect roughness and metallic to the principled BSDF node
        links.new(rough_rgb.outputs["G"], principled.inputs["Roughness"])
        links.new(metal_rgb.outputs["B"], principled.inputs["Metallic"])

    # Load and connect rough_ao texture
    if "rough_ao" in stage_material["maps"]:
        rough_ao_url = stage_material["maps"]["rough_ao"]
        rough_ao_path = download_texture(rough_ao_url, material_name, "RoughAO")
        rough_ao_tex = nodes.new(type="ShaderNodeTexImage")
        rough_ao_tex.image = bpy.data.images.load(rough_ao_path)
        links.new(mapping.outputs["Vector"], rough_ao_tex.inputs["Vector"])
        links.new(rough_ao_tex.outputs["Color"], principled.inputs["Roughness"])

    # Load and connect displacement texture
    if "Displacement" in stage_material["maps"]:
        disp_url = stage_material["maps"]["Displacement"]
        disp_path = download_texture(disp_url, material_name, "Displacement")
        disp_tex = nodes.new(type="ShaderNodeTexImage")
        disp_tex.image = bpy.data.images.load(disp_path)
        disp_node = nodes.new(type="ShaderNodeDisplacement")
        links.new(mapping.outputs["Vector"], disp_tex.inputs["Vector"])
        links.new(disp_tex.outputs["Color"], disp_node.inputs["Height"])
        links.new(disp_node.outputs["Displacement"], output.inputs["Displacement"])

    # Connect the nodes
    links.new(principled.outputs["BSDF"], output.inputs["Surface"])

```````

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/prompts.py`:

```````py
import os
import json
from typing import Optional
import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv()
MODEL = "gemini-1.5-pro"

OBJECTS_PROMPT = """
    Extract the objects, background, and ground from the scene prompt and return as an array (make the last two the background and ground.

    Examples:
    
    "caption": "The camera is behind and to the right, pointing down. The scene contains Star Wars lightsabers with red and blue blades, a small town, a white box that looks like a refrigerator, Samsung Galaxy earbuds in their case, and a white piece of paper. The earbuds are behind the town and the piece of paper, the town is behind the paper, the paper is to the left of the box. Make all objects move slowly in their own direction: the lightsabers to the right, the town forward, the box right, the earbuds forward and the paper right. Keep the camera on the town. The background is Kloppenheim Pure Sky and the floor is Painted Worn Brick.",
    ["Star Wars lightsabers", "a small town", "a box", "earbuds", "piece of paper", "Kloppenheim 01 (Pure Sky)", "Painted Worn Brick"]
    Kloppenheim is the background while the painted is the ground/stage. 

    "caption": "A huge shark tooth, some wood, and a rock with ice are right behind a blue and white helmet with a beetle design. Camera follows the helmet as it moves. Shark tooth, wood, and rock move left slowly. The helmet moves right slowly. Camera is tilted down a bit and facing right. Tight FOV. Scene is bright and blurry. Background is Greenwich Park with a concrete rock path. Animation is fast.",
    ["A shark tooth", "some wood", "a rock with ice", "blue and white helmet", "Greenwich Park", "concrete rock path"]

    "caption": "A hedgehog is in the scene with a campfire that has marshmallows and a cake behind it. A sword is behind the hedgehog and to the right of a clock, brass sculpture, hooks, and a lock. A wall with a painting, a pot, and a statue is in front of the sword.  Camera is pointing down and left. Full FOV. Scene has some brightness, low shadow detail, and very blurry.  Background is Hochsal Forest with a cobblestone floor. Animation is a bit fast.",
    ["hedgehog", "campfire that has marshmallows", "cake", "sword", "clock", "brass sculpture", "hooks", "lock", "painting", "pot", "statue", "Hochsal Forest", "cobblestone floor"]

    include descriptions of the objects like if it says "black and white arrow" don't just say arrow include the description of that object.
    Recheck your answer.
    Here is the caption:
"""

OBJECTS_JSON_PROMPT = """
Ok so I want to place objects in a 3D scene, imagine a 3x3 grid 

0 1 2
3 4 5
6 7 8

the 6,7,8 part being the "front" where we assume the camera sees first. 
We want to split the caption into its respective descriptive objects (ONLY OBJECTS NOT background or ground/stage). We then want to generate a JSON which includes the objects, their placement, movement, camera_follow and all those attributes. For example, objects that are ontop of each other have the same index placement as a result.

Here are the speed "guidlines":

Something moderate/average/normal speed is around = 0.25-0.5
Something fast, quick, rapid, swift is around = 0.5

Here are scale guidlines:
Something tiny/mini/extra small is around = 0.25
Something small/undersized/smaller is around 0.5
Something medium/normal/average is around 0.75
Something medium-large/considerable is around 1.5
Something large/big/tall is around 2.25
Something huge/towering/giant is around 3.0

Examples:


Example 1: 

Prompt/caption: There's a small house and a modern white house in the scene. The small house is to the left of the modern house. The small house moves to the right at a moderate speed and the modern house moves backward. The camera should smoothly follow the movement of the modern house. Make the camera 325 degrees to the right, slightly tilted. The backdrop is Driving School and the flooring is Rock Ground. Apply a medium ambient occlusion and high ray tracing. Add a slight blur effect.

"objects": [
        {
            "name": "a small house with a roof, chimney, and insulation",
            "uid": "42cd3cd73892469492aa8e4bddbeed84",
            "description": "a small house with a roof, chimney, and insulation",
            "placement": 4,
            "from": "cap3d",
            "scale": {
                "factor": 0.25,
                "name": "tiny",
                "name_synonym": "petite"
            },
            "movement": {
                "direction": "right",
                "speed": 0.12043596124471068
            },
        },
        {
            "name": "a modern, white house surrounded by palm trees and plants",
            "uid": "d9ec8eb389e54dbdbfc06a44dab0f1ff",
            "description": "a modern, white house surrounded by palm trees and plants",
            "placement": 1,
            "from": "cap3d",
            "scale": {
                "factor": 1.5,
                "name": "medium-large",
                "name_synonym": "medium-large"
            },
            "movement": {
                "direction": "backward",
                "speed": 0.13079011338247018
            },
            "camera_follow": {
                "follow": true
            },
        }
    ]

Example 2: 

Prompt/caption: A building with a clock tower, a ferris wheel, and a boat, a white ring, a red and pink square, a pink alien toy, and a black teddy bear wearing a hat are in the scene. The ring is to the right of the building. The alien is to the left of the square and to the right of the teddy bear.  The building is to the left of and behind the square and behind the alien. The building moves left, the ring moves backward, the square moves left, the alien moves forward, and the teddy bear moves right.  The camera follows the teddy bear. Camera is facing backward and angled forward.  The scene has a normal blur effect.

"objects": [
        {
            "name": "a building featuring a clock tower, staircase, ferris wheel, and a floating boat nearby",
            "uid": "efccbdf2299e4b3d8ab767e879e60863",
            "description": "a building featuring a clock tower, staircase, ferris wheel, and a floating boat nearby",
            "placement": 4,
            "from": "cap3d",
            "scale": {
                "factor": 3.0,
                "name": "huge",
                "name_synonym": "very big"
            },
            "movement": {
                "direction": "left",
                "speed": 0.08990124981699919
            },
            "relationships": "a building featuring a clock tower, staircase, ferris wheel, and a floating boat nearby is to the left of white ring with a curved design."
        },
        {
            "name": "white ring with a curved design",
            "uid": "39825e7b48454e74bae405ea7f8ab964",
            "description": "white ring with a curved design",
            "placement": 1,
            "from": "cap3d",
            "scale": {
                "factor": 2.25,
                "name": "large",
                "name_synonym": "massive"
            },
            "movement": {
                "direction": "backward",
                "speed": 0.10487729616518639
            },
            "relationships": "a building featuring a clock tower, staircase, ferris wheel, and a floating boat nearby is to the left of and behind a square object featuring red and pink lines, resembling a table, rug, or ceiling panel."
        },
        {
            "name": "a square object featuring red and pink lines, resembling a table, rug, or ceiling panel",
            "uid": "69dfa841d1444622883d9fc1fd726625",
            "description": "a square object featuring red and pink lines, resembling a table, rug, or ceiling panel",
            "placement": 2,
            "from": "cap3d",
            "scale": {
                "factor": 3.0,
                "name": "huge",
                "name_synonym": "extra large"
            },
            "movement": {
                "direction": "left",
                "speed": 0.13250349055797148
            },
            "relationships": "a building featuring a clock tower, staircase, ferris wheel, and a floating boat nearby is  and behind A pink alien-shaped toy with eyes, arms, legs, and a mouth, resembling a rabbit or bunny."
        },
        {
            "name": "A pink alien-shaped toy with eyes, arms, legs, and a mouth, resembling a rabbit or bunny",
            "uid": "23da888f8d034c9594930eba8279c6e9",
            "description": "A pink alien-shaped toy with eyes, arms, legs, and a mouth, resembling a rabbit or bunny",
            "placement": 5,
            "from": "cap3d",
            "scale": {
                "factor": 2.25,
                "name": "large",
                "name_synonym": "large"
            },
            "movement": {
                "direction": "forward",
                "speed": 0.10110217017409817
            },
            "relationships": "a building featuring a clock tower, staircase, ferris wheel, and a floating boat nearby is to the right of and behind a black teddy bear and a black bird, both wearing hats."
        },
        {
            "name": "a black teddy bear and a black bird, both wearing hats",
            "uid": "18969fffe8674a91a41b093c4f6bf612",
            "description": "a black teddy bear and a black bird, both wearing hats",
            "placement": 8,
            "from": "cap3d",
            "scale": {
                "factor": 1.0,
                "name": "medium",
                "name_synonym": "usual"
            },
            "movement": {
                "direction": "right",
                "speed": 0.10007398751033873
            },
            "camera_follow": {
                "follow": true
            },
            "relationships": "white ring with a curved design is to the right of a building featuring a clock tower, staircase, ferris wheel, and a floating boat nearby."
        }
    ],

Example 3:

Prompt/caption: A house with a detailed roof and a map on the ceiling moves slowly to the left.  Camera angle is forward and ahead to show the house moving. The scene is set on an evening road with white sandstone brick flooring, and uses normal lighting with motion blur.

"objects": [
    {
        "name": "a house featuring a detailed roof structure and a suspended ceiling with a map on it",
        "uid": "ee7e6031912b46bc8ca7205a959c5c16",
        "description": "a house featuring a detailed roof structure and a suspended ceiling with a map on it",
        "placement": 4,
        "from": "cap3d",
        "scale": {
            "factor": 1.5,
            "name": "medium-large",
            "name_synonym": "considerable"
        },
        "transformed_position": [
            0,
            0
        ],
        "movement": {
            "direction": "left",
            "speed": 0.06752940064263789
        },
        "relationships": []
    }
],

TIP: Only add movement to the object if it is moving. If it is not moving, do not include the movement attribute.

Now generate the correct object placements and structures based on the prompt:  
"""



OBJECTS_JSON_IMPROVEMENT_PROMPT = """
Ok so I want to place objects in a 3D scene, imagine a 3x3 grid 

0 1 2
3 4 5
6 7 8

the 6,7,8 part being the "front" where we assume the camera sees first. 
We want to split the caption into its respective descriptive objects (ONLY OBJECTS NOT background or ground/stage). We then want to generate a JSON which includes the objects, their placement, movement, camera_follow and all those attributes. For example, objects that are ontop of each other have the same index placement as a result.

Here are the speed "guidlines":

Something moderate/average/normal speed is around = 0.25-0.5
Something fast, quick, rapid, swift is around = 0.5

Here are scale guidlines:
Something tiny/mini/extra small is around = 0.25
Something small/undersized/smaller is around 0.5
Something medium/normal/average is around 0.75
Something medium-large/considerable is around 1.5
Something large/big/tall is around 2.25
Something huge/towering/giant is around 3.0

Examples:


Example 1: 

Prompt/caption: There's a small house and a modern white house in the scene. The small house is to the left of the modern house. The small house moves to the right at a moderate speed and the modern house moves backward. The camera should smoothly follow the movement of the modern house. Make the camera 325 degrees to the right, slightly tilted. The backdrop is Driving School and the flooring is Rock Ground. Apply a medium ambient occlusion and high ray tracing. Add a slight blur effect.

"objects": [
        {
            "name": "a small house with a roof, chimney, and insulation",
            "uid": "42cd3cd73892469492aa8e4bddbeed84",
            "description": "a small house with a roof, chimney, and insulation",
            "placement": 4,
            "from": "cap3d",
            "scale": {
                "factor": 0.25,
                "name": "tiny",
                "name_synonym": "petite"
            },
            "movement": {
                "direction": "right",
                "speed": 0.12043596124471068
            },
        },
        {
            "name": "a modern, white house surrounded by palm trees and plants",
            "uid": "d9ec8eb389e54dbdbfc06a44dab0f1ff",
            "description": "a modern, white house surrounded by palm trees and plants",
            "placement": 1,
            "from": "cap3d",
            "scale": {
                "factor": 1.5,
                "name": "medium-large",
                "name_synonym": "medium-large"
            },
            "movement": {
                "direction": "backward",
                "speed": 0.13079011338247018
            },
            "camera_follow": {
                "follow": true
            },
        }
    ]

Example 2: 

Prompt/caption: A building with a clock tower, a ferris wheel, and a boat, a white ring, a red and pink square, a pink alien toy, and a black teddy bear wearing a hat are in the scene. The ring is to the right of the building. The alien is to the left of the square and to the right of the teddy bear.  The building is to the left of and behind the square and behind the alien. The building moves left, the ring moves backward, the square moves left, the alien moves forward, and the teddy bear moves right.  The camera follows the teddy bear. Camera is facing backward and angled forward.  The scene has a normal blur effect.

"objects": [
        {
            "name": "a building featuring a clock tower, staircase, ferris wheel, and a floating boat nearby",
            "uid": "efccbdf2299e4b3d8ab767e879e60863",
            "description": "a building featuring a clock tower, staircase, ferris wheel, and a floating boat nearby",
            "placement": 4,
            "from": "cap3d",
            "scale": {
                "factor": 3.0,
                "name": "huge",
                "name_synonym": "very big"
            },
            "movement": {
                "direction": "left",
                "speed": 0.08990124981699919
            },
            "relationships": "a building featuring a clock tower, staircase, ferris wheel, and a floating boat nearby is to the left of white ring with a curved design."
        },
        {
            "name": "white ring with a curved design",
            "uid": "39825e7b48454e74bae405ea7f8ab964",
            "description": "white ring with a curved design",
            "placement": 1,
            "from": "cap3d",
            "scale": {
                "factor": 2.25,
                "name": "large",
                "name_synonym": "massive"
            },
            "movement": {
                "direction": "backward",
                "speed": 0.10487729616518639
            },
            "relationships": "a building featuring a clock tower, staircase, ferris wheel, and a floating boat nearby is to the left of and behind a square object featuring red and pink lines, resembling a table, rug, or ceiling panel."
        },
        {
            "name": "a square object featuring red and pink lines, resembling a table, rug, or ceiling panel",
            "uid": "69dfa841d1444622883d9fc1fd726625",
            "description": "a square object featuring red and pink lines, resembling a table, rug, or ceiling panel",
            "placement": 2,
            "from": "cap3d",
            "scale": {
                "factor": 3.0,
                "name": "huge",
                "name_synonym": "extra large"
            },
            "movement": {
                "direction": "left",
                "speed": 0.13250349055797148
            },
            "relationships": "a building featuring a clock tower, staircase, ferris wheel, and a floating boat nearby is  and behind A pink alien-shaped toy with eyes, arms, legs, and a mouth, resembling a rabbit or bunny."
        },
        {
            "name": "A pink alien-shaped toy with eyes, arms, legs, and a mouth, resembling a rabbit or bunny",
            "uid": "23da888f8d034c9594930eba8279c6e9",
            "description": "A pink alien-shaped toy with eyes, arms, legs, and a mouth, resembling a rabbit or bunny",
            "placement": 5,
            "from": "cap3d",
            "scale": {
                "factor": 2.25,
                "name": "large",
                "name_synonym": "large"
            },
            "movement": {
                "direction": "forward",
                "speed": 0.10110217017409817
            },
            "relationships": "a building featuring a clock tower, staircase, ferris wheel, and a floating boat nearby is to the right of and behind a black teddy bear and a black bird, both wearing hats."
        },
        {
            "name": "a black teddy bear and a black bird, both wearing hats",
            "uid": "18969fffe8674a91a41b093c4f6bf612",
            "description": "a black teddy bear and a black bird, both wearing hats",
            "placement": 8,
            "from": "cap3d",
            "scale": {
                "factor": 1.0,
                "name": "medium",
                "name_synonym": "usual"
            },
            "movement": {
                "direction": "right",
                "speed": 0.10007398751033873
            },
            "camera_follow": {
                "follow": true
            },
            "relationships": "white ring with a curved design is to the right of a building featuring a clock tower, staircase, ferris wheel, and a floating boat nearby."
        }
    ],

Example 3:

Prompt/caption: A house with a detailed roof and a map on the ceiling moves slowly to the left.  Camera angle is forward and ahead to show the house moving. The scene is set on an evening road with white sandstone brick flooring, and uses normal lighting with motion blur.

"objects": [
    {
        "name": "a house featuring a detailed roof structure and a suspended ceiling with a map on it",
        "uid": "ee7e6031912b46bc8ca7205a959c5c16",
        "description": "a house featuring a detailed roof structure and a suspended ceiling with a map on it",
        "placement": 4,
        "from": "cap3d",
        "scale": {
            "factor": 1.5,
            "name": "medium-large",
            "name_synonym": "considerable"
        },
        "transformed_position": [
            0,
            0
        ],
        "movement": {
            "direction": "left",
            "speed": 0.06752940064263789
        },
        "relationships": []
    }
],

TIP: Only add movement to the object if it is moving. If it is not moving, do not include the movement attribute.

All I want you to do is check if the following JSON below is about correct. Leave it as is if it is correct, otherwise, make the necessary changes to make it correct.
Here's the JSON:
"""


CAMERA_JSON_IMPROVEMENT_PROMPT = """
    For camera information:

{
    "orientation": {
        "yaw_max": 360,
        "yaw_min": 0,
        "pitch_max": 75,
        "pitch_min": 0,
        "labels": {
            "yaw": {
                "0": [
                    "facing forward",
                    "directly ahead",
                    "straight ahead",
                    "head on",
                    "<degrees> degrees forward",
                    "dead ahead",
                    "right in front",
                    "pointed forward",
                    "aligned forward",
                    "squarely ahead",
                    "<degrees>° forward"
                ],
                "15": [
                    "slightly left of front",
                    "front left quarter",
                    "slight left front",
                    "on the left side",
                    "<degrees> degrees to the left of front",
                    "<degrees> degrees left of front",
                    "<degrees> degrees to the left",
                    "<degrees>° to the left of front",
                    "<degrees>° left of front",
                    "<degrees>° to the left",
                    "just off front to the left",
                    "almost forward but left",
                    "slightly turned left"
                ],
                "30": [
                    "front left",
                    "left forward",
                    "leftward front",
                    "<degrees>° to the left",
                    "<degrees>° left",
                    "<degrees>° front left",
                    "<degrees> degrees to the left",
                    "<degrees> degrees left",
                    "<degrees> degrees front left",
                    "a bit to the left front",
                    "leaning left forward",
                    "slightly forward left",
                    "angled left forward"
                ],
                "45": [
                    "front left side",
                    "left forward side",
                    "oblique left",
                    "<degrees> degrees to the left",
                    "<degrees>° to the left",
                    "slightly to the front-left",
                    "a bit to the left of forward",
                    "not quite perpendicular to the left",
                    "just off the left forward",
                    "leaning towards the left front"
                ],
                "60": [
                    "left front quarter",
                    "left forward quarter",
                    "<degrees> degrees to the left",
                    "<degrees>° to the left",
                    "<degrees> degrees front left",
                    "<degrees>° front left",
                    "a sharp left forward",
                    "leaning strongly to the left",
                    "angled forward on the left",
                    "deep into the left quadrant",
                    "majorly to the left front"
                ],
                "75": [
                    "far left front",
                    "left side forward",
                    "<degrees> degrees to the left",
                    "<degrees>° to the left",
                    "<degrees> degrees front left",
                    "<degrees>° front left",
                    "nearing perpendicular left",
                    "heavily biased to the left front",
                    "sharply forward left",
                    "greatly to the left front",
                    "deep left but forward"
                ],
                "90": [
                    "left side",
                    "directly left",
                    "perpendicular left",
                    "<degrees> degrees to the left",
                    "<degrees>° to the left",
                    "<degrees> degrees left",
                    "<degrees>° left",
                    "direct left",
                    "straight to the left",
                    "clearly to the left",
                    "flat left",
                    "horizontal left"
                ],
                "105": [
                    "back left quarter",
                    "left rear quarter",
                    "<degrees> degrees to the left",
                    "<degrees>° to the left",
                    "<degrees> degrees rear left",
                    "<degrees>° rear left",
                    "slightly back on the left",
                    "a bit behind to the left",
                    "just off left but backwards",
                    "leaning back on the left",
                    "rearward to the left"
                ],
                "120": [
                    "rear left",
                    "left backward",
                    "<degrees> degrees to the left",
                    "<degrees>° to the left",
                    "<degrees> degrees rear left",
                    "<degrees>° rear left",
                    "back towards the left",
                    "left and slightly behind",
                    "left rearward",
                    "angled back on the left",
                    "deep into the rear-left quadrant"
                ],
                "135": [
                    "back left side",
                    "left rear side",
                    "oblique rear left",
                    "<degrees> degrees to the left",
                    "<degrees>° to the left",
                    "<degrees> degrees rear left",
                    "<degrees>° rear left",
                    "strongly to the rear-left",
                    "deep left rear",
                    "sharply back left",
                    "heavily to the back left",
                    "far back to the left"
                ],
                "150": [
                    "left rear quarter",
                    "left back quarter",
                    "<degrees> degrees to the left",
                    "<degrees>° to the left",
                    "<degrees> degrees rear left",
                    "<degrees>° rear left",
                    "sharp left back",
                    "deep into left rear quadrant",
                    "majorly to the left back",
                    "back and deep left",
                    "left and far behind"
                ],
                "165": [
                    "far back left",
                    "left side back",
                    "<degrees> degrees to the left",
                    "<degrees>° to the left",
                    "nearing directly behind on the left",
                    "<degrees> degrees rear left",
                    "<degrees>° rear left",
                    "almost back left",
                    "strong left back",
                    "heavy left rear",
                    "deeply back on the left"
                ],
                "180": [
                    "back",
                    "directly back",
                    "rear",
                    "<degrees> degrees",
                    "<degrees>°",
                    "<degrees> degrees rear",
                    "<degrees>° rear",
                    "directly behind",
                    "straight back",
                    "clear back",
                    "flat back",
                    "horizontal rear"
                ],
                "195": [
                    "far back right",
                    "right side back",
                    "<degrees> degrees to the right",
                    "<degrees>° to the right",
                    "<degrees> degrees rear right",
                    "<degrees>° rear right",
                    "nearing directly behind on the right",
                    "almost back right",
                    "strong right back",
                    "heavy right rear",
                    "deeply back on the right"
                ],
                "210": [
                    "right rear quarter",
                    "right back quarter",
                    "<degrees> degrees to the right",
                    "<degrees>° to the right",
                    "<degrees> degrees rear right",
                    "<degrees>° rear right",
                    "sharp right back",
                    "deep into right rear quadrant",
                    "majorly to the right back",
                    "back and deep right",
                    "right and far behind"
                ],
                "225": [
                    "back right side",
                    "right rear side",
                    "oblique rear right",
                    "<degrees> degrees to the right",
                    "<degrees>° to the right",
                    "<degrees> degrees rear right",
                    "<degrees>° rear right",
                    "strongly to the rear-right",
                    "deep right rear",
                    "sharply back right",
                    "heavily to the back right",
                    "far back to the right"
                ],
                "240": [
                    "rear right",
                    "right backward",
                    "<degrees> degrees to the right",
                    "<degrees>° to the right",
                    "<degrees> degrees rear right",
                    "<degrees>° rear right",
                    "back towards the right",
                    "right and slightly behind",
                    "right rearward",
                    "angled back on the right",
                    "deep into the rear-right quadrant"
                ],
                "255": [
                    "back right quarter",
                    "right rear quarter",
                    "<degrees> degrees to the right",
                    "<degrees>° to the right",
                    "<degrees> degrees rear right",
                    "<degrees>° rear right",
                    "slightly back on the right",
                    "a bit behind to the right",
                    "just off right but backwards",
                    "leaning back on the right",
                    "rearward to the right"
                ],
                "270": [
                    "right side",
                    "directly right",
                    "perpendicular right",
                    "<degrees> degrees to the right",
                    "<degrees>° to the right",
                    "<degrees> degrees right",
                    "<degrees>° right",
                    "direct right",
                    "straight to the right",
                    "clearly to the right",
                    "flat right",
                    "horizontal right"
                ],
                "285": [
                    "far right front",
                    "right side forward",
                    "<degrees> degrees to the right",
                    "<degrees>° to the right",
                    "<degrees> degrees front right",
                    "<degrees>° front right",
                    "nearing perpendicular right",
                    "heavily biased to the right front",
                    "sharply forward right",
                    "greatly to the right front",
                    "deep right but forward"
                ],
                "300": [
                    "right front quarter",
                    "right forward quarter",
                    "<degrees> degrees to the right",
                    "<degrees>° to the right",
                    "<degrees> degrees front right",
                    "<degrees>° front right",
                    "a sharp right forward",
                    "leaning strongly to the right",
                    "angled forward on the right",
                    "deep into the right quadrant",
                    "majorly to the right front"
                ],
                "315": [
                    "front right side",
                    "right forward side",
                    "oblique right",
                    "<degrees> degrees to the right",
                    "<degrees>° to the right",
                    "<degrees> degrees front right",
                    "<degrees>° front right",
                    "slightly to the front-right",
                    "a bit to the right of forward",
                    "not quite perpendicular to the right",
                    "just off the right forward",
                    "leaning towards the right front"
                ],
                "330": [
                    "front right",
                    "right forward",
                    "<degrees> degrees to the right",
                    "<degrees>° to the right",
                    "<degrees> degrees front right",
                    "<degrees>° front right",
                    "slightly to the right of forward",
                    "slight bias to the right",
                    "marginally front right"
                ],
                "345": [
                    "slightly right of front",
                    "front right quarter",
                    "slight right front",
                    "on the right side",
                    "<degrees> degrees to the right of front",
                    "<degrees> degrees right of front",
                    "<degrees> degrees to the right",
                    "just shy of directly ahead on the right",
                    "almost facing front-right",
                    "nearly front but to the right",
                    "front-right leaning",
                    "ahead but veered right"
                ]
            },
            "pitch": {
                "0": [
                    "level with the ground",
                    "horizontally level",
                    "flat",
                    "even",
                    "<degrees> degrees"
                ],
                "15": [
                    "tilted down slightly",
                    "slightly downward",
                    "mildly downward",
                    "<degrees> degrees downward",
                    "<degrees> forward"
                ],
                "30": [
                    "tilted down",
                    "downward tilt",
                    "angled down",
                    "<degrees> degrees downward",
                    "<degrees> forward",
                    "angled downward"
                ],
                "45": [
                    "tilted down sharply",
                    "sharply downward",
                    "steeply downward",
                    "<degrees> degrees downward",
                    "<degrees> forward"
                ],
                "60": [
                    "tilted down steeply",
                    "steeply angled down",
                    "sharply angled downward",
                    "<degrees> degrees downward",
                    "<degrees> forward",
                    "angled down sharply",
                    "<degrees> down steeply>",
                    "<degrees> steeply down",
                    "<degrees> sharply downward"
                ]
            }
        },
        "descriptions": [
            "The camera is <yaw>, <pitch>.",
            "The camera is <pitch>, <yaw>.",
            "<pitch>, <yaw>.",
            "Yaw: <yaw>, Pitch: <pitch>.",
            "Pitch: <pitch>, Yaw: <yaw>.",
            "Tilt: <pitch>, Rotation: <yaw>.",
            "The lens is oriented <yaw>, with a <pitch> tilt.",
            "Camera facing <yaw> and angled <pitch>.",
            "The view is set <pitch>, looking <yaw>.",
            "Orientation: <yaw>, Angle: <pitch>.",
            "The shot is angled <pitch> while pointing <yaw>.",
            "Direction <yaw>, elevation <pitch>.",
            "Adjust the camera <yaw> and <pitch>.",
            "Set the camera to <pitch> and <yaw>.",
            "Position the camera at <pitch> and <yaw>.",
            "Yaw: <yaw>, Pitch: <pitch>.",
            "Pitch: <pitch>, Yaw: <yaw>.",
            "Tilt: <pitch>, Rotation: <yaw>.",
            "Orient the camera <yaw> with a <pitch> tilt.",
            "Direct the camera <yaw>, set tilt to <pitch>.",
            "Angle the lens <pitch> and rotate it <yaw>.",
            "Configure the camera to face <yaw> with a <pitch> angle.",
            "Align the camera to <yaw>, and adjust the pitch to <pitch>.",
            "Rotate the camera to <yaw> and tilt it <pitch>."
        ]
    },
    "framings": [
        {
            "name": "extreme_closeup",
            "fov_min": 10,
            "fov_max": 15,
            "coverage_factor_min": 0.7,
            "coverage_factor_max": 1.1,
            "descriptions": [
                "Extreme close-up shot",
                "Extremely tight framing",
                "Intensely intimate view",
                "Minute details captured",
                "Right up close",
                "Heavily zoomed in",
                "Details fill the frame",
                "Extreme detail focus",
                "Extremely near view",
                "As close as possible",
                "Extremely close framing",
                "Intense close-up perspective",
                "Extreme close-up of objects",
                "Objects dominate the frame",
                "Deeply intimate close-up",
                "Extremely tight shot",
                "Incredibly close view",
                "Extreme zoom on details",
                "Macro-level perspective",
                "Microscopic framing",
                "Up-close and personal",
                "Extreme close focus",
                "Highly detailed close-up",
                "Extremely zoomed in",
                "Intensely close shot",
                "Extreme proximity",
                "Ultra-tight framing",
                "Magnified view",
                "Ultra-close perspective",
                "Extremely detailed shot"
            ]
        },
        {
            "name": "closeup",
            "fov_min": 15,
            "fov_max": 24,
            "coverage_factor_min": 1.0,
            "coverage_factor_max": 1.2,
            "descriptions": [
                "Close-up shot of the objects.",
                "Close-up framing.",
                "Close view of the objects.",
                "Objects occupy the frame closely.",
                "Intimate close-up.",
                "Tight framing on the objects.",
                "Close-up perspective.",
                "Objects captured up close.",
                "Detailed close-up shot.",
                "Tight close-up.",
                "Close-up view.",
                "Intimate close-up.",
                "Detailed close-up.",
                "Close framing.",
                "Zoomed in close on the details.",
                "Intimately close view of the objects.",
                "Fills the frame for an up-close perspective.",
                "Tight shot that gets in close to the objects.",
                "Close-up shot",
                "Tight framing",
                "Intimate view",
                "Detailed perspective",
                "Up close",
                "Zoomed in",
                "Fill the frame",
                "Details highlighted",
                "Close focus",
                "Near view",
                "In tight"
            ]
        },
        {
            "name": "medium_closeup",
            "fov_min": 24,
            "fov_max": 30,
            "coverage_factor_min": 0.8,
            "coverage_factor_max": 1.2,
            "descriptions": [
                "Medium close-up shot",
                "Slightly tight framing",
                "Fairly intimate view",
                "Moderate detail focus",
                "Closer than medium",
                "Slightly zoomed in",
                "Objects fill more frame",
                "Emphasized details",
                "Medium-close perspective",
                "Moderately near view",
                "Tighter than standard",
                "Mildly close framing",
                "Somewhat close shot",
                "Moderate object focus",
                "Slightly intimate view",
                "Medium-tight shot",
                "Moderately close view",
                "Mild zoom on details",
                "Semi-close perspective",
                "In between medium and close",
                "Somewhat zoomed in",
                "Moderately detailed shot"
            ]
        },
        {
            "name": "medium",
            "fov_min": 30,
            "fov_max": 40,
            "coverage_factor_min": 1.0,
            "coverage_factor_max": 1.5,
            "descriptions": [
                "Medium shot",
                "Medium framing",
                "Medium shot framing the objects.",
                "Medium shot.",
                "Balanced medium.",
                "Standard medium.",
                "Standard medium framing on the objects.",
                "Not too tight, not too loose - just right in the middle.",
                "A moderate, neutral distance from the subjects.",
                "Medium distance, balanced composition.",
                "Standard view",
                "Typical framing",
                "Normal distance",
                "Mid-range shot",
                "Balanced perspective",
                "Average framing",
                "Neutral distance",
                "Middle ground",
                "Moderate zoom",
                "Halfway in"
            ]
        },
        {
            "name": "medium_wide",
            "fov_min": 40,
            "fov_max": 55,
            "coverage_factor_min": 1.5,
            "coverage_factor_max": 2.0,
            "descriptions": [
                "Medium-wide shot",
                "Medium wide shot",
                "Medium wide angle",
                "Medium wide framing",
                "Medium wide shot of the objects.",
                "Medium wide perspective.",
                "Medium-wide framing on the objects.",
                "Medium-wide shot of the scene.",
                "Medium-wide.",
                "Medium wide",
                "Medium-wide angle",
                "Not too close, not too far.",
                "A bit wider than a typical medium shot.",
                "Halfway between a medium and a wide.",
                "Pulls back a little from medium framing.",
                "Slightly expanded view compared to medium.",
                "Slightly wide",
                "Moderately broad",
                "Mild widening",
                "Somewhat expansive",
                "Gently zoomed out"
            ]
        },
        {
            "name": "wide",
            "fov_min": 55,
            "fov_max": 70,
            "coverage_factor_min": 2.0,
            "coverage_factor_max": 3.0,
            "descriptions": [
                "Wide shot.",
                "Wide angle",
                "Wide framing.",
                "Wide shot of the objects.",
                "Expansive view of the objects.",
                "Expansive view.",
                "Taking in the whole scene.",
                "Expansive, broad view of the full setting and objects.",
                "Wide perspective showing everything.",
                "Zoomed out to capture the big picture.",
                "All-encompassing wide angle on the full space.",
                "Zoomed out",
                "Broad view",
                "Expansive framing",
                "Full perspective",
                "Sweeping vista",
                "All-encompassing",
                "Big picture",
                "Comprehensive view",
                "Wide-angle",
                "Broad scope",
                "Fully zoomed out"
            ]
        },
        {
            "name": "extreme_wide",
            "fov_min": 70,
            "fov_max": 90,
            "coverage_factor_min": 3.0,
            "coverage_factor_max": 4.0,
            "descriptions": [
                "Extreme wide shot",
                "Extreme wide angle",
                "Extreme wide framing",
                "Extremely broad framing",
                "Extremely zoomed out",
                "Captures entire scene",
                "Panoramic view",
                "Massively wide shot",
                "Extremely expansive framing",
                "Ultra-wide perspective",
                "Extremely wide landscape",
                "Extremely broad view",
                "Ultra-zoomed out",
                "Extremely wide angle",
                "Captures entire setting",
                "Extremely wide coverage",
                "Ultra-expansive shot",
                "Extremely wide perspective"
            ]
        }
    ],
    "animation_speed": { 
        "min": 0.5,
        "max": 2.0,
        "types": {
            "slow": {
                "min": 0.5,
                "max": 0.75,
                "descriptions": [
                    "Slow animation speed <animation_speed_value> applied.",
                    "The scene has a slow animation speed of <animation_speed_value>.",
                    "Animation speed of <animation_speed_value> is set to slow.",
                    "The scene is rendered with a slow animation speed of <animation_speed_value>.",
                    "Slow animation is used in the scene.",
                    "The scene has a leisurely animation speed of <animation_speed_value>.",
                    "Animation is set to slow speed of <animation_speed_value> in the scene.",
                    "The scene is enhanced with a slow animation speed of <animation_speed_value>.",
                    "Slow animation is present in the scene.",
                    "The scene is rendered with a slow animation speed of <animation_speed_value>.",
                    "The animation runs at a slow pace.",
                    "The scene features a gentle animation speed.",
                    "Animations are intentionally slowed down.",
                    "A calm and slow animation speed is applied.",
                    "The scene moves with a relaxed animation speed.",
                    "Animations unfold slowly in the scene.",
                    "The pace of animation is deliberately slow.",
                    "The scene showcases a leisurely animation speed.",
                    "Slow-motion animation effects are used.",
                    "The scene transitions smoothly with a slow animation speed."
                ]
            },
            "medium": {
                "min": 0.75,
                "max": 1.25,
                "descriptions": [
                    [
                        "Medium animation speed of <animation_speed_value> applied.",
                        "The scene has a moderate animation speed of <animation_speed_value>.",
                        "Animation speed is set to medium of <animation_speed_value>.",
                        "The scene is rendered with a moderate animation speed of <animation_speed_value>.",
                        "Medium animation of <animation_speed_value> is used in the scene.",
                        "The scene has a normal animation speed of <animation_speed_value>.",
                        "Animation is set to medium speed of <animation_speed_value> in the scene.",
                        "The scene is enhanced with a moderate animation speed of <animation_speed_value>.",
                        "Medium animation of <animation_speed_value> is present in the scene.",
                        "The scene is rendered with a moderate animation speed of <animation_speed_value>.",
                        "The animation speed is set to a balanced pace.",
                        "The scene features a moderate animation tempo.",
                        "Animations operate at a medium speed.",
                        "A standard animation speed is applied to the scene.",
                        "The scene progresses with a medium-paced animation.",
                        "Animations move at a steady speed in the scene.",
                        "The animation runs at a moderate rate.",
                        "The scene has a typical animation speed.",
                        "Animations flow at a medium speed in the scene.",
                        "The scene transitions at a moderate animation speed."
                    ]
                ]
            },
            "fast": {
                "min": 1.25,
                "max": 1.5,
                "descriptions": [
                    [
                        "Fast animation speed of <animation_speed_value> applied.",
                        "The scene has a fast animation speed of <animation_speed_value>.",
                        "Animation speed is set to fast of <animation_speed_value>.",
                        "The scene is rendered with a fast animation speed of <animation_speed_value>.",
                        "Fast animation of <animation_speed_value> is used in the scene.",
                        "The scene has a quick animation speed of <animation_speed_value>.",
                        "Animation is set to fast speed of <animation_speed_value> in the scene.",
                        "The scene is enhanced with a fast animation speed of <animation_speed_value>.",
                        "Fast animation of <animation_speed_value> is present in the scene.",
                        "The scene is rendered with a fast animation speed of <animation_speed_value>.",
                        "The animation runs at a rapid pace.",
                        "The scene features a swift animation speed.",
                        "Animations are set to a fast tempo.",
                        "A quick animation speed is applied.",
                        "The scene moves with high-speed animation.",
                        "Animations unfold rapidly in the scene.",
                        "The pace of animation is brisk.",
                        "The scene showcases a fast animation rate.",
                        "High-speed animation effects are used.",
                        "The scene transitions quickly with fast animation."
                    ]
                ]
            },
            "faster": {
                "min": 1.5,
                "max": 2.0,
                "descriptions": [
                    [
                        "Faster animation speed of <animation_speed_value> applied.",
                        "The scene has a faster animation speed of <animation_speed_value>.",
                        "Animation speed is set to faster of <animation_speed_value>.",
                        "The scene is rendered with a faster animation speed of <animation_speed_value>.",
                        "Faster animation of <animation_speed_value> is used in the scene.",
                        "The scene has an accelerated animation speed of <animation_speed_value>.",
                        "Animation is set to faster speed of <animation_speed_value> in the scene.",
                        "The scene is enhanced with a faster animation speed of <animation_speed_value>.",
                        "Faster animation of <animation_speed_value> is present in the scene.",
                        "The scene is rendered with a faster animation speed of <animation_speed_value>.",
                        "The animation speed is increased significantly.",
                        "The scene features an enhanced animation tempo.",
                        "Animations are set to a much faster pace.",
                        "A highly accelerated animation speed is applied.",
                        "The scene progresses with a rapid animation rate.",
                        "Animations move at an expedited speed in the scene.",
                        "The animation runs at a notably faster rate.",
                        "The scene showcases a significantly increased animation speed.",
                        "Animations unfold at an accelerated pace.",
                        "The scene transitions swiftly with enhanced animation speed."
                    ]
                ]
            }
        }
    },
    "animations": [        
        {
            "name": "static",
            "descriptions": [
                "The camera remains stationary.",
                "No camera movement.",
                "The camera holds still.",
                "Static camera shot.",
                "The camera does not move.",
                "Steady, unmoving camera.",
                "Fixed camera position.",
                "Motionless camera.",
                "The camera is locked in place.",
                "Stable, static camera view.",
                "The scene is captured from a fixed perspective.",
                "Unmoving camera angle.",
                "The camera is set in a fixed position.",
                "No motion in the camera view.",
                "The objects are captured with a static camera."
            ],
            "keyframes": []
        },
        {
            "name": "pan_left",
            "descriptions": [
                "The camera moves horizontally to the left.",
                "The view shifts to the left.",
                "Sweeping leftward movement of the camera.",
                "Camera glides left.",
                "The scene moves smoothly to the left as the camera pans.",
                "Horizontal pan to the left.",
                "The camera shifts left.",
                "Leftward panning of the camera.",
                "The camera moves left.",
                "Pan the camera left.",
                "Shift the camera smoothly to the left to alter the view.",
                "Pan the camera to the left to capture a wider perspective.",
                "Glide the camera left to change the scene laterally.",
                "Sweep the camera to the left.",
                "Move the camera horizontally to the left.",
                "Adjust the camera to pan left.",
                "Slide the camera leftward.",
                "The camera swings to the left.",
                "Lateral movement of the camera to the left.",
                "The view transitions to the left as the camera moves.",
                "Horizontal shift of the camera to the left.",
                "The camera tracks left."
            ],
            "keyframes": [
                {
                    "CameraAnimationPivot": {
                        "position": [
                            0,
                            -1.5,
                            0
                        ]
                    }
                },
                {
                    "CameraAnimationPivot": {
                        "position": [
                            0,
                            0.5,
                            0
                        ]
                    }
                }
            ]
        },
        {
            "name": "pan_right",
            "descriptions": [
                "The camera moves horizontally to the right.",
                "Pan the camera right.",
                "The view shifts to the right.",
                "Sweeping rightward movement of the camera.",
                "Camera glides right.",
                "The scene moves smoothly to the right as the camera pans.",
                "Horizontal pan to the right.",
                "The camera shifts right.",
                "Rightward panning of the camera.",
                "The camera moves right.",
                "Shift the camera smoothly to the right to alter the view.",
                "Pan the camera to the right to capture a wider perspective.",
                "Glide the camera right to change the scene laterally.",
                "Sweep the camera to the right.",
                "Move the camera horizontally to the right to extend the visible area.",
                "Rotate the camera right.",
                "Adjust the camera to pan right.",
                "Swivel the camera to the right.",
                "Turn the camera to the right.",
                "Slide the camera rightward to enhance the panoramic perspective.",
                "The camera swings to the right.",
                "Lateral movement of the camera to the right.",
                "The view transitions to the right as the camera moves.",
                "Horizontal shift of the camera to the right.",
                "The camera tracks right."
            ],
            "keyframes": [
                {
                    "CameraAnimationPivot": {
                        "position": [
                            0,
                            1.5,
                            0
                        ]
                    }
                },
                {
                    "CameraAnimationPivot": {
                        "position": [
                            0,
                            -0.5,
                            0
                        ]
                    }
                }
            ]
        },
        {
            "name": "orbit_left",
            "descriptions": [
                "The camera orbits counter-clockwise around the scene.",
                "Move the camera in a circular path to the left around the stage.",
                "The camera circles the objects to the left.",
                "Counter-clockwise orbit around the objects.",
                "Camera makes a complete rotation to the left around the objects.",
                "Orbiting left around the objects.",
                "The camera encircles the scene to the left.",
                "Circular motion of the camera around the objects to the left.",
                "Camera travels in a leftward circle around the subjects.",
                "Full orbit to the left around the objects.",
                "Orbit the camera counter-clockwise around the subject.",
                "Circle the camera to the left around the subject.",
                "Rotate the camera in a counter-clockwise path around the objects.",
                "Move the camera in a circular leftward path around the objects.",
                "Guide the camera to orbit left around the objects.",
                "Swing the camera around the objects to the left.",
                "Rotate the camera left in a circular motion around the objects.",
                "Spiral the camera to the left around the objects.",
                "Encircle the scene with the camera moving left.",
                "Turn the camera in a counter-clockwise orbit.",
                "The camera revolves around the objects counter-clockwise.",
                "Circular trajectory of the camera moving to the left around the scene.",
                "The camera arcs around the objects to the left.",
                "Rotate the camera around the objects in a counter-clockwise direction.",
                "The camera traces a circular path to the left around the subjects."
            ],
            "keyframes": [
                {
                    "CameraAnimationRoot": {
                        "rotation": [
                            0,
                            0,
                            90
                        ]
                    }
                },
                {
                    "CameraAnimationRoot": {
                        "rotation": [
                            0,
                            0,
                            0
                        ]
                    }
                }
            ]
        },
        {
            "name": "orbit_right",
            "descriptions": [
                "The camera orbits clockwise around the objects.",
                "Move the camera in a circular path to the right around the subject.",
                "The camera circles the scene to the right.",
                "Clockwise orbit around the subject.",
                "Camera makes a complete rotation to the right.",
                "Orbiting right around the objects.",
                "The camera encircles the objects to the right.",
                "Spiral motion of the camera around the objects.",
                "Camera travels in a rightward circle around the objects.",
                "Full orbit to the right around the objects.",
                "Orbit the camera clockwise around the scene.",
                "Circle the camera to the right around the stage.",
                "Rotate the camera in a clockwise path around the objects.",
                "Move the camera in a circular rightward path around the objects.",
                "Guide the camera to orbit right around the objects.",
                "Swing the camera around the objects to the right.",
                "Rotate the camera right in a circular motion around the objects.",
                "Spiral the camera to the right around the objects.",
                "Encircle the objects with the camera moving right.",
                "Spin the camera in a clockwise orbit around the objects.",
                "The camera revolves around the objects clockwise.",
                "Circular trajectory of the camera moving to the right around the scene.",
                "The camera arcs around the objects to the right.",
                "Rotate the camera around the objects in a clockwise direction.",
                "The camera traces a circular path to the right around the subjects."
            ],
            "keyframes": [
                {
                    "CameraAnimationRoot": {
                        "rotation": [
                            0,
                            0,
                            0
                        ]
                    }
                },
                {
                    "CameraAnimationRoot": {
                        "rotation": [
                            0,
                            0,
                            90
                        ]
                    }
                }
            ]
        },
        {
            "name": "orbit_up",
            "descriptions": [
                "The camera orbits upward from 0 to 90 degrees over the scene.",
                "Elevate the camera in an orbital path above the objects.",
                "Raise the camera perspective while orbiting over the scene.",
                "Orbit the camera up to a 90-degree angle.",
                "The view lifts as the camera orbits upward.",
                "Upward orbital movement of the camera.",
                "The camera arcs up and over the scene.",
                "Rotate the camera upward in an orbital trajectory.",
                "Orbit up to capture a bird's eye view of the scene.",
                "The camera ascends in an orbital path over the objects.",
                "Sweep the camera up and over the scene.",
                "Adjust the camera to orbit upward.",
                "Trace an upward orbital path with the camera.",
                "Elevate the camera perspective with an upward orbit.",
                "Orbit the camera up to look down on the scene from above."
            ],
            "keyframes": [
                {
                    "CameraFramingPivot": {
                        "rotation": [
                            0,
                            0,
                            0
                        ]
                    }
                },
                {
                    "CameraFramingPivot": {
                        "rotation": [
                            0,
                            -30,
                            0
                        ]
                    }
                }
            ]
        },
        {
            "name": "orbit_down",
            "descriptions": [
                "The camera orbits downward from 90 to 0 degrees over the scene.",
                "Lower the camera in an orbital path back to the starting position.",
                "Descend the camera perspective while orbiting over the scene.",
                "Orbit the camera down from a 90-degree angle.",
                "The view lowers as the camera orbits downward.",
                "Downward orbital movement of the camera.",
                "The camera arcs down and returns to the initial view.",
                "Rotate the camera downward in an orbital trajectory.",
                "Orbit down to restore the original camera perspective.",
                "The camera descends in an orbital path back to the objects.",
                "Sweep the camera down from the elevated position.",
                "Adjust the camera to orbit downward.",
                "Trace a downward orbital path with the camera.",
                "Lower the camera perspective with a downward orbit.",
                "Orbit the camera down to look at the scene from the starting angle."
            ],
            "keyframes": [
                {
                    "CameraFramingPivot": {
                        "rotation": [
                            0,
                            -30,
                            0
                        ]
                    }
                },
                {
                    "CameraFramingPivot": {
                        "rotation": [
                            0,
                            0,
                            0
                        ]
                    }
                }
            ]
        },
        {
            "name": "dolly_in",
            "descriptions": [
                "The camera moves closer to the subject.",
                "Dolly the camera toward the scene.",
                "The camera advances toward the objects.",
                "The camera pushes toward the subject.",
                "Dolly in to focus more closely.",
                "The camera travels in to capture details.",
                "Dolly in to highlight the objects.",
                "The camera moves in to concentrate on the objects.",
                "The camera progresses closer to emphasize details.",
                "The camera narrows the scene by moving in.",
                "The camera approaches the objects.",
                "Dolly in to bring the objects nearer.",
                "The camera advances to fill the frame with objects."
            ],
            "keyframes": [
                {
                    "CameraAnimationPivot": {
                        "position": [
                            4,
                            0,
                            0
                        ]
                    }
                },
                {
                    "CameraAnimationPivot": {
                        "position": [
                            0,
                            0,
                            0
                        ]
                    }
                }
            ]
        },
        {
            "name": "dolly_out",
            "descriptions": [
                "The camera pulls back from the scene.",
                "Dolly out to widen the view.",
                "The camera retreats from the objects.",
                "The camera withdraws from the subject.",
                "Pull the camera back.",
                "Dolly out to de-emphasize the objects.",
                "The camera moves out to show a wider scene.",
                "The camera pulls back to reveal more context.",
                "The camera pulls away, expanding the view.",
                "Dolly out to reduce focus on the objects.",
                "The camera retreats to show a broader perspective.",
                "Dolly out to place the objects into a larger context."
            ],
            "keyframes": [
                {
                    "CameraAnimationPivot": {
                        "position": [
                            0,
                            0,
                            0
                        ]
                    }
                },
                {
                    "CameraAnimationPivot": {
                        "position": [
                            4,
                            0,
                            0
                        ]
                    }
                }
            ]
        },
        {
            "name": "zoom_in",
            "descriptions": [
                "The camera zooms in.",
                "Move closer to the scene.",
                "Move toward the objects.",
                "Move toward the subject.",
                "Zoom in.",
                "Zoom in to the scene.",
                "Zoom in on the objects",
                "Camera narrows the view to concentrate on the objects.",
                "Zooming in to highlight details of the objects.",
                "The camera zooms in.",
                "The camera closes in on the objects.",
                "Zoom in to make the objects appear closer.",
                "Adjust the zoom to concentrate on the objects.",
                "Move the camera closer.",
                "Zoom in to focus on the objects.",
                "Close in on the objects with the zoom.",
                "The view narrows as the camera zooms in.",
                "The camera focuses to emphasize the objects.",
                "Zooming in brings the objects closer.",
                "The camera magnifies the view of the objects.",
                "Adjust the zoom to fill the frame with the objects."
            ],
            "keyframes": [
                {
                    "Camera": {
                        "angle_offset": 10
                    }
                },
                {
                    "Camera": {
                        "angle_offset": 0
                    }
                }
            ]
        },
        {
            "name": "zoom_out",
            "descriptions": [
                "The camera zooms out.",
                "Zoom out.",
                "Zooming out.",
                "The objects become smaller as the camera zooms out.",
                "Increasing the view width.",
                "Zooming out de-emphasizes the objects.",
                "The camera zooms in.",
                "Move away from the scene.",
                "Move away the objects.",
                "Move away the subject.",
                "Pull back.",
                "Pull away from the objects.",
                "Pull back with the camera zoom.",
                "Zoom in.",
                "Zoom in to the scene.",
                "Zoom in on the objects",
                "Zooming in to highlight details of the objects.",
                "The camera zooms in.",
                "The camera closes in on the objects.",
                "Zoom in to make the objects appear closer.",
                "Adjust the zoom to concentrate on the objects.",
                "Move the camera closer.",
                "Zoom in to focus on the objects.",
                "Close in on the objects with the zoom.",
                "Pull back with the camera zoom.",
                "The view widens as the camera zooms out.",
                "The camera pulls back to de-emphasize the objects.",
                "Zooming out pushes the objects further away.",
                "The camera reduces the view of the objects.",
                "Adjust the zoom to expand the frame beyond the objects."
            ],
            "keyframes": [
                {
                    "Camera": {
                        "angle_offset": 0
                    }
                },
                {
                    "Camera": {
                        "angle_offset": 10
                    }
                }
            ]
        },
        {
            "name": "tilt_up",
            "descriptions": [
                "The camera tilts upward.",
                "Tilt the view up.",
                "Camera points up.",
                "Angle the camera upward.",
                "The scene tilts up as the camera moves.",
                "Vertical tilt upward.",
                "The camera tilts up.",
                "Upward tilting of the camera.",
                "The camera rotates up.",
                "Tilt the camera up.",
                "Angle the camera up to shift the view vertically.",
                "Tilt the camera upward to capture more of the upper scene.",
                "Rotate the camera up to change the vertical perspective.",
                "Incline the camera upward.",
                "Move the camera to tilt up.",
                "Adjust the camera for an upward tilt.",
                "Pivot the camera up.",
                "The camera inclines vertically.",
                "Vertical rotation of the camera upward.",
                "The camera pans up.",
                "Upward pivot of the camera.",
                "The view shifts up as the camera tilts."
            ],
            "keyframes": [
                {
                    "Camera": {
                        "rotation": [
                            -25,
                            0,
                            0
                        ]
                    }
                },
                {
                    "Camera": {
                        "rotation": [
                            5,
                            0,
                            0
                        ]
                    }
                }
            ]
        },
        {
            "name": "tilt_down",
            "descriptions": [
                "The camera tilts downward.",
                "Tilt the view down.",
                "Camera points down.",
                "Angle the camera downward.",
                "The scene tilts down as the camera moves.",
                "Vertical tilt downward.",
                "The camera tilts down.",
                "Downward tilting of the camera.",
                "The camera rotates down.",
                "Tilt the camera down.",
                "Angle the camera down to shift the view vertically.",
                "Tilt the camera downward to capture more of the lower scene.",
                "Rotate the camera down to change the vertical perspective.",
                "Decline the camera downward.",
                "Move the camera to tilt down.",
                "Adjust the camera for a downward tilt.",
                "Pivot the camera down.",
                "The camera declines vertically.",
                "Vertical rotation of the camera downward.",
                "The camera pans down.",
                "Downward pivot of the camera.",
                "The view shifts down as the camera tilts."
            ],
            "keyframes": [
                {
                    "Camera": {
                        "rotation": [
                            25,
                            0,
                            0
                        ]
                    }
                },
                {
                    "Camera": {
                        "rotation": [
                            -5,
                            0,
                            0
                        ]
                    }
                }
            ]
        },
        {
            "name": "tilt_left",
            "descriptions": [
                "The camera tilts to the left.",
                "Tilt the view left.",
                "Camera banks left.",
                "Angle the camera to the left.",
                "The scene tilts left as the camera moves.",
                "Horizontal tilt to the left.",
                "The camera tilts left.",
                "Leftward tilting of the camera.",
                "The camera rotates left.",
                "Tilt the camera left.",
                "Angle the camera left to shift the view horizontally.",
                "Tilt the camera to the left.",
                "Rotate the camera left to change the horizontal perspective.",
                "Bank the camera to the left.",
                "Move the camera to tilt left.",
                "Adjust the camera for a leftward tilt.",
                "Pivot the camera left.",
                "The camera swivels to the left.",
                "Horizontal rotation of the camera to the left.",
                "The camera swivels left.",
                "Leftward banking of the camera.",
                "The view shifts to the left as the camera tilts."
            ],
            "keyframes": [
                {
                    "Camera": {
                        "rotation": [
                            0,
                            -25,
                            0
                        ]
                    }
                },
                {
                    "Camera": {
                        "rotation": [
                            0,
                            5,
                            0
                        ]
                    }
                }
            ]
        },
        {
            "name": "tilt_right",
            "descriptions": [
                "The camera tilts to the right.",
                "Tilt the view right.",
                "Camera banks right.",
                "Angle the camera to the right.",
                "The scene tilts right as the camera moves.",
                "Horizontal tilt to the right.",
                "The camera tilts right.",
                "Rightward tilting of the camera.",
                "The camera rotates right.",
                "Tilt the camera right.",
                "Angle the camera right to shift the view horizontally.",
                "Tilt the camera to the right.",
                "Rotate the camera right to change the horizontal perspective.",
                "Bank the camera to the right.",
                "Move the camera to tilt right.",
                "Adjust the camera for a rightward tilt.",
                "Pivot the camera right.",
                "The camera swivels to the right.",
                "Horizontal rotation of the camera to the right.",
                "The camera swivels right.",
                "Rightward banking of the camera.",
                "The view shifts to the right as the camera tilts."
            ],
            "keyframes": [
                {
                    "Camera": {
                        "rotation": [
                            0,
                            25,
                            0
                        ]
                    }
                },
                {
                    "Camera": {
                        "rotation": [
                            0,
                            -5,
                            0
                        ]
                    }
                }
            ]
        },
        {
            "name": "crane_up",
            "descriptions": [
                "The camera cranes upward.",
                "Crane the camera up.",
                "The view lifts as the camera cranes up.",
                "Camera rises in a crane shot.",
                "The scene moves up with the camera crane.",
                "Upward crane of the camera.",
                "The camera moves up on a crane.",
                "Craning shot lifting the camera.",
                "The camera elevates in a crane movement.",
                "Crane up with the camera.",
                "Lift the camera view using a crane shot.",
                "Crane the camera upward to gain height.",
                "Raise the camera perspective with a crane.",
                "Elevate the camera angle using a crane.",
                "Move the camera up in a craning motion.",
                "Adjust the camera to crane up.",
                "Hoist the camera view with a crane shot.",
                "The camera boom lifts.",
                "Elevating crane movement of the camera.",
                "The camera rises in a crane shot.",
                "Vertical tracking of the camera upward on a crane.",
                "The view ascends with the camera crane."
            ],
            "keyframes": [
                {
                    "CameraAnimationPivot": {
                        "position": [
                            0,
                            0,
                            0
                        ]
                    }
                },
                {
                    "CameraAnimationPivot": {
                        "position": [
                            0,
                            0,
                            2
                        ]
                    }
                }
            ]
        },
        {
            "name": "crane_down",
            "descriptions": [
                "The camera cranes downward.",
                "Crane the camera down.",
                "The view lowers as the camera cranes down.",
                "Camera descends in a crane shot.",
                "The scene moves down with the camera crane.",
                "Downward crane of the camera.",
                "The camera moves down on a crane.",
                "Craning shot lowering the camera.",
                "The camera lowers in a crane movement.",
                "Crane down with the camera.",
                "Lower the camera view using a crane shot.",
                "Crane the camera downward to reduce height.",
                "Descend the camera perspective with a crane.",
                "Lower the camera angle using a crane.",
                "Move the camera down in a craning motion.",
                "Adjust the camera to crane down.",
                "Bring down the camera view with a crane shot.",
                "The camera boom lowers.",
                "Descending crane movement of the camera.",
                "The camera falls in a crane shot.",
                "Vertical tracking of the camera downward on a crane.",
                "The view descends with the camera crane."
            ],
            "keyframes": [
                {
                    "CameraAnimationPivot": {
                        "position": [
                            0,
                            0,
                            2
                        ]
                    }
                },
                {
                    "CameraAnimationPivot": {
                        "position": [
                            0,
                            0,
                            0
                        ]
                    }
                }
            ]
        }
    ],
    "postprocessing": {
        "bloom": {
            "threshold_min": 0.7,
            "threshold_max": 1.0,
            "intensity_min": 0.0,
            "intensity_max": 1.0,
            "radius_min": 1.0,
            "radius_max": 10.0,
            "types": {
                "none": {
                    "intensity_min": 0.0,
                    "intensity_max": 0.02,
                    "descriptions": [
                        "No bloom effect applied.",
                        "The scene has no bloom effect.",
                        "Bloom effect is turned off.",
                        "The scene is rendered without bloom.",
                        "No bloom is used in the scene.",
                        "The scene does not have a bloom effect.",
                        "Bloom is disabled in the scene.",
                        "The scene is not enhanced with bloom.",
                        "No bloom is present in the scene.",
                        "The scene is not rendered with bloom."
                    ]
                },
                "low": {
                    "intensity_min": 0.02,
                    "intensity_max": 0.2,
                    "descriptions": [
                        "Low bloom effect applied.",
                        "The scene has a subtle bloom effect.",
                        "Bloom effect is set to low intensity.",
                        "The scene is rendered with a mild bloom effect.",
                        "Low bloom is used in the scene.",
                        "The scene has a slight bloom effect.",
                        "Bloom is set to low intensity in the scene.",
                        "The scene is enhanced with a gentle bloom effect.",
                        "Low bloom is present in the scene.",
                        "The scene is rendered with a mild bloom effect."
                    ]
                },
                "medium": {
                    "intensity_min": 0.2,
                    "intensity_max": 0.5,
                    "descriptions": [
                        "Medium bloom effect applied.",
                        "The scene has a moderate bloom effect.",
                        "Bloom effect is set to medium intensity.",
                        "The scene is rendered with a moderate bloom effect.",
                        "Medium bloom is used in the scene.",
                        "The scene has a noticeable bloom effect.",
                        "Bloom is set to medium intensity in the scene.",
                        "The scene is enhanced with a moderate bloom effect.",
                        "Medium bloom is present in the scene.",
                        "The scene is rendered with a moderate bloom effect."
                    ]
                },
                "high": {
                    "intensity_min": 0.5,
                    "intensity_max": 0.8,
                    "descriptions": [
                        "High bloom effect applied.",
                        "The scene has a strong bloom effect.",
                        "Bloom effect is set to high intensity.",
                        "The scene is rendered with a strong bloom effect.",
                        "High bloom is used in the scene.",
                        "The scene has a powerful bloom effect.",
                        "Bloom is set to high intensity in the scene.",
                        "The scene is enhanced with a strong bloom effect.",
                        "High bloom is present in the scene.",
                        "The scene is rendered with a strong bloom effect."
                    ]
                },
                "extreme": {
                    "intensity_min": 0.8,
                    "intensity_max": 1.1,
                    "descriptions": [
                        "Extreme bloom effect applied.",
                        "The scene has an intense bloom effect.",
                        "Bloom effect is set to extreme intensity.",
                        "The scene is rendered with an extreme bloom effect.",
                        "Extreme bloom is used in the scene.",
                        "The scene has an overwhelming bloom effect.",
                        "Bloom is set to extreme intensity in the scene.",
                        "The scene is enhanced with an extreme bloom effect.",
                        "Extreme bloom is present in the scene.",
                        "The scene is rendered with an extreme bloom effect."
                    ]
                }
            }
        },
        "ssao": {
            "distance_min": 0.02,
            "distance_max": 1.2,
            "factor_min": 0.0,
            "factor_max": 1.0,
            "types": {
                "none": {
                    "factor_min": 0.0,
                    "factor_max": 0.02,
                    "descriptions": [
                        "No ambient occlusion effect applied.",
                        "The scene has no ambient occlusion effect.",
                        "Ambient occlusion is turned off.",
                        "The scene is rendered without ambient occlusion.",
                        "No ambient occlusion is used in the scene.",
                        "The scene does not have an ambient occlusion effect.",
                        "Ambient occlusion is disabled in the scene.",
                        "The scene is not enhanced with ambient occlusion.",
                        "No ambient occlusion is present in the scene.",
                        "The scene is not rendered with ambient occlusion."
                    ]
                },
                "low": {
                    "factor_min": 0.02,
                    "factor_max": 0.2,
                    "descriptions": [
                        "Low ambient occlusion effect applied.",
                        "The scene has a subtle ambient occlusion effect.",
                        "Ambient occlusion is set to low intensity.",
                        "The scene is rendered with a mild ambient occlusion effect.",
                        "Low ambient occlusion is used in the scene.",
                        "The scene has a slight ambient occlusion effect.",
                        "Ambient occlusion is set to low intensity in the scene.",
                        "The scene is enhanced with a gentle ambient occlusion effect.",
                        "Low ambient occlusion is present in the scene.",
                        "The scene is rendered with a mild ambient occlusion effect."
                    ]
                },
                "medium": {
                    "factor_min": 0.2,
                    "factor_max": 0.6,
                    "descriptions": [
                        "Medium ambient occlusion effect applied.",
                        "The scene has a moderate ambient occlusion effect.",
                        "Ambient occlusion is set to medium intensity.",
                        "The scene is rendered with a moderate ambient occlusion effect.",
                        "Medium ambient occlusion is used in the scene.",
                        "The scene has a noticeable ambient occlusion effect.",
                        "Ambient occlusion is set to medium intensity in the scene.",
                        "The scene is enhanced with a moderate ambient occlusion effect.",
                        "Medium ambient occlusion is present in the scene.",
                        "The scene is rendered with a moderate ambient occlusion effect."
                    ]
                },
                "high": {
                    "factor_min": 0.6,
                    "factor_max": 1.0,
                    "descriptions": [
                        "High ambient occlusion effect applied.",
                        "The scene has a strong ambient occlusion effect.",
                        "Ambient occlusion is set to high intensity.",
                        "The scene is rendered with a strong ambient occlusion effect.",
                        "High ambient occlusion is used in the scene.",
                        "The scene has a powerful ambient occlusion effect.",
                        "Ambient occlusion is set to high intensity in the scene.",
                        "The scene is enhanced with a strong ambient occlusion effect.",
                        "High ambient occlusion is present in the scene.",
                        "The scene is rendered with a strong ambient occlusion effect."
                    ]
                },
                "extreme": {
                    "factor_min": 1.0,
                    "factor_max": 1.2,
                    "descriptions": [
                        "Extreme ambient occlusion effect applied.",
                        "The scene has an intense ambient occlusion effect.",
                        "Ambient occlusion is set to extreme intensity.",
                        "The scene is rendered with an extreme ambient occlusion effect.",
                        "Extreme ambient occlusion is used in the scene.",
                        "The scene has an overwhelming ambient occlusion effect.",
                        "Ambient occlusion is set to extreme intensity in the scene.",
                        "The scene is enhanced with an extreme ambient occlusion effect.",
                        "Extreme ambient occlusion is present in the scene.",
                        "The scene is rendered with an extreme ambient occlusion effect."
                    ]
                }
            }
        },
        "ssrr": {
            "min_max_roughness": 0.0,
            "max_max_roughness": 1.0,
            "min_thickness": 0.05,
            "max_thickness": 5.0,
            "types": {
                "none": {
                    "max_roughness_min": 0.0,
                    "max_roughness_max": 0.02,
                    "descriptions": [
                        "No screen-space ray tracing effect applied.",
                        "The scene has no screen-space ray tracing effect.",
                        "Screen-space ray tracing is turned off.",
                        "The scene is rendered without screen-space ray tracing.",
                        "No screen-space ray tracing is used in the scene.",
                        "The scene does not have a screen-space ray tracing effect.",
                        "Screen-space ray tracing is disabled in the scene.",
                        "The scene is not enhanced with screen-space ray tracing.",
                        "No screen-space ray tracing is present in the scene.",
                        "The scene is not rendered with screen-space ray tracing."
                    ]
                },
                "low": {
                    "max_roughness_min": 0.02,
                    "max_roughness_max": 0.1,
                    "descriptions": [
                        "Low screen-space ray tracing effect applied.",
                        "The scene has a subtle screen-space ray tracing effect.",
                        "Screen-space ray tracing is set to low intensity.",
                        "The scene is rendered with a mild screen-space ray tracing effect.",
                        "Low screen-space ray tracing is used in the scene.",
                        "The scene has a slight screen-space ray tracing effect.",
                        "Screen-space ray tracing is set to low intensity in the scene.",
                        "The scene is enhanced with a gentle screen-space ray tracing effect.",
                        "Low screen-space ray tracing is present in the scene.",
                        "The scene is rendered with a mild screen-space ray tracing effect."
                    ]
                },
                "medium": {
                    "max_roughness_min": 0.1,
                    "max_roughness_max": 0.3,
                    "descriptions": [
                        "Medium screen-space ray tracing effect applied.",
                        "The scene has a moderate screen-space ray tracing effect.",
                        "Screen-space ray tracing is set to medium intensity.",
                        "The scene is rendered with a moderate screen-space ray tracing effect.",
                        "Medium screen-space ray tracing is used in the scene.",
                        "The scene has a noticeable screen-space ray tracing effect.",
                        "Screen-space ray tracing is set to medium intensity in the scene.",
                        "The scene is enhanced with a moderate screen-space ray tracing effect.",
                        "Medium screen-space ray tracing is present in the scene.",
                        "The scene is rendered with a moderate screen-space ray tracing effect."
                    ]
                },
                "high": {
                    "max_roughness_min": 0.3,
                    "max_roughness_max": 0.6,
                    "descriptions": [
                        "High screen-space ray tracing effect applied.",
                        "The scene has a strong screen-space ray tracing effect.",
                        "Screen-space ray tracing is set to high intensity.",
                        "The scene is rendered with a strong screen-space ray tracing effect.",
                        "High screen-space ray tracing is used in the scene.",
                        "The scene has a powerful screen-space ray tracing effect.",
                        "Screen-space ray tracing is set to high intensity in the scene.",
                        "The scene is enhanced with a strong screen-space ray tracing effect.",
                        "High screen-space ray tracing is present in the scene.",
                        "The scene is rendered with a strong screen-space ray tracing effect."
                    ],
                    "extreme": {
                        "max_roughness_min": 0.6,
                        "max_roughness_max": 1.0,
                        "descriptions": [
                            "Extreme screen-space ray tracing effect applied.",
                            "The scene has an intense screen-space ray tracing effect.",
                            "Screen-space ray tracing is set to extreme intensity.",
                            "The scene is rendered with an extreme screen-space ray tracing effect.",
                            "Extreme screen-space ray tracing is used in the scene.",
                            "The scene has an overwhelming screen-space ray tracing effect.",
                            "Screen-space ray tracing is set to extreme intensity in the scene.",
                            "The scene is enhanced with an extreme screen-space ray tracing effect.",
                            "Extreme screen-space ray tracing is present in the scene.",
                            "The scene is rendered with an extreme screen-space ray tracing effect."
                        ]
                    }
                }
            }
        },
        "motionblur": {
            "shutter_speed_min": 0.0,
            "shutter_speed_max": 1.0,
            "types": {
                "none": {
                    "shutter_speed_min": 0.0,
                    "shutter_speed_max": 0.02,
                    "descriptions": [
                        "No motion blur effect applied.",
                        "The scene has no motion blur effect.",
                        "Motion blur is turned off.",
                        "The scene is rendered without motion blur.",
                        "No motion blur is used in the scene.",
                        "The scene does not have a motion blur effect.",
                        "Motion blur is disabled in the scene.",
                        "The scene is not enhanced with motion blur.",
                        "No motion blur is present in the scene.",
                        "The scene is not rendered with motion blur."
                    ]
                },
                "low": {
                    "shutter_speed_min": 0.02,
                    "shutter_speed_max": 0.2,
                    "descriptions": [
                        "Low motion blur effect applied.",
                        "The scene has a subtle motion blur effect.",
                        "Motion blur is set to low intensity.",
                        "The scene is rendered with a mild motion blur effect.",
                        "Low motion blur is used in the scene.",
                        "The scene has a slight motion blur effect.",
                        "Motion blur is set to low intensity in the scene.",
                        "The scene is enhanced with a gentle motion blur effect.",
                        "Low motion blur is present in the scene.",
                        "The scene is rendered with a mild motion blur effect."
                    ]
                },
                "medium": {
                    "shutter_speed_min": 0.2,
                    "shutter_speed_max": 0.5,
                    "descriptions": [
                        "Medium motion blur effect applied.",
                        "The scene has a moderate motion blur effect.",
                        "Motion blur is set to medium intensity.",
                        "The scene is rendered with a moderate motion blur effect.",
                        "Medium motion blur is used in the scene.",
                        "The scene has a noticeable motion blur effect.",
                        "Motion blur is set to medium intensity in the scene.",
                        "The scene is enhanced with a moderate motion blur effect.",
                        "Medium motion blur is present in the scene.",
                        "The scene is rendered with a moderate motion blur effect."
                    ]
                },
                "high": {
                    "shutter_speed_min": 0.5,
                    "shutter_speed_max": 0.8,
                    "descriptions": [
                        "High motion blur effect applied.",
                        "The scene has a strong motion blur effect.",
                        "Motion blur is set to high intensity.",
                        "The scene is rendered with a strong motion blur effect.",
                        "High motion blur is used in the scene.",
                        "The scene has a powerful motion blur effect.",
                        "Motion blur is set to high intensity in the scene.",
                        "The scene is enhanced with a strong motion blur effect.",
                        "High motion blur is present in the scene.",
                        "The scene is rendered with a strong motion blur effect."
                    ]
                },
                "extreme": {
                    "shutter_speed_min": 0.8,
                    "shutter_speed_max": 1.0,
                    "descriptions": [
                        "Extreme motion blur effect applied.",
                        "The scene has an intense motion blur effect.",
                        "Motion blur is set to extreme intensity.",
                        "The scene is rendered with an extreme motion blur effect.",
                        "Extreme motion blur is used in the scene.",
                        "The scene has an overwhelming motion blur effect.",
                        "Motion blur is set to extreme intensity in the scene.",
                        "The scene is enhanced with an extreme motion blur effect.",
                        "Extreme motion blur is present in the scene.",
                        "The scene is rendered with an extreme motion blur effect."
                    ]
                }
            }
        }
    }
}

Examples:

Example 1:
Prompt/caption: 

caption: A small wooden book with a lock, a teapot, text that says 'Geo Winter', and an axe are all in the scene. The text is behind the teapot, the axe is to the right of the teapot, and the book is behind both the axe and the teapot. The book moves backward slowly, the teapot moves forward slowly, the text slides left, and the axe moves forward. The camera follows the axe. The camera is angled right and level with the ground.  The scene has a slight blur effect. The scene is animated at a fast pace.

{
    "orientation_caption": "Camera orientation: Yaw: oblique right, Pitch: level with the ground.",
    "orientation": {
        "yaw": 317,
        "pitch": 1
    },
    "framing_caption": "Camera framing: Captures entire scene Set the fov of the camera to 74 degrees. (23.00 mm focal length)",
    "framing": {
        "fov": 74,
        "coverage_factor": 3.3787342191874163,
        "name": "extreme_wide"
    },
    "animation_caption": "Camera animation: Throughout the sequence, the camera follows a red, yellow, and black axe seamlessly. The scene transitions quickly with fast animation.",
    "animation": {
        "name": "crane_down",
        "keyframes": [
            {
                "CameraAnimationPivot": {
                    "position": [
                        0,
                        0,
                        2
                    ]
                }
            },
            {
                "CameraAnimationPivot": {
                    "position": [
                        0,
                        0,
                        0
                    ]
                }
            }
        ],
        "speed_factor": 1.3754731748375375
    },
    "postprocessing_caption": "Post-processing effects: The scene is rendered with a mild ambient occlusion effect. The scene has a slight motion blur effect.",
    "postprocessing": {
        "bloom": {
            "threshold": 0.9049127669422115,
            "intensity": 0.8086834007424443,
            "radius": 4.345852681550845,
            "type": "extreme"
        },
        "ssao": {
            "distance": 0.2675165509421571,
            "factor": 0.03757264803813665,
            "type": "low"
        },
        "ssrr": {
            "max_roughness": 0.674072407335585,
            "thickness": 0.7906668082985685,
            "type": "none"
        },
        "motionblur": {
            "shutter_speed": 0.1915879393539549,
            "type": "low"
        }
    },
}


Example 2:

Prompt/caption: A house with a roof is on the left and a skateboard ramp with a building is on the right. The house moves left and the skateboard ramp moves backward. The camera is positioned behind and to the right, tilted steeply downward, showing a zoomed-in view. The scene has subtle shadows and a slight ray tracing effect. The background is a Belfast Sunset with a grassy and rocky ground.

{
    "orientation_caption": "Camera orientation: Orientation: right back quarter, Angle: tilted down steeply.",
    "orientation": {
        "yaw": 214,
        "pitch": 69
    },
    "framing_caption": "Camera framing: Magnified view The camera has a 181 mm focal length.",
    "framing": {
        "fov": 11,
        "coverage_factor": 0.8407846352674209,
        "name": "extreme_closeup"
    },
    "animation_caption": "Camera animation:  Fast animation speed of 1.4x applied.",
    "animation": {
        "name": "zoom_in",
        "keyframes": [
            {
                "Camera": {
                    "angle_offset": 10
                }
            },
            {
                "Camera": {
                    "angle_offset": 0
                }
            }
        ],
        "speed_factor": 1.404395487155547
    },
    "postprocessing_caption": "Post-processing effects: The scene is enhanced with a gentle ambient occlusion effect. Low screen-space ray tracing effect applied.",
    "postprocessing": {
        "bloom": {
            "threshold": 0.8496130440000619,
            "intensity": 0.2220181839537102,
            "radius": 1.3457534686773542,
            "type": "medium"
        },
        "ssao": {
            "distance": 0.4896150467674612,
            "factor": 0.04480824053315857,
            "type": "low"
        },
        "ssrr": {
            "max_roughness": 0.09441042633095198,
            "thickness": 0.7098830677702582,
            "type": "low"
        },
        "motionblur": {
            "shutter_speed": 0.9803040919463857,
            "type": "extreme"
        }
    },
}

Example 3:
Prompt/caption: Show a stick with a blue and white handle, a blue crystal, and a blue and white bird, a light blue hat with white polka dots, a turquoise window with a blue shutter, a chicken in a yellow dress, and a house with trees, a pool, and cars. The chicken is to the right and behind the window. The window is in front of the hat. The stick is to the left of the hat. The hat is behind the window. The house is to the left of and behind the window. Camera follows the chicken as it moves. Camera is positioned low and far right. Use a very wide camera angle. The scene is set at night with cobblestone flooring, no bloom, and some motion blur.

{
    "orientation_caption": "Camera orientation: The camera is steeply downward, far back to the right.",
    "orientation": {
        "yaw": 225,
        "pitch": 40
    },
    "framing_caption": "Camera framing: Extremely wide coverage The camera has a 76 degree field of view. (22.00 mm focal length)",
    "framing": {
        "fov": 76,
        "coverage_factor": 3.8613973293159507,
        "name": "extreme_wide"
    },
    "animation_caption": "Camera animation: Capturing A low poly a chicken in a yellow dress in motion, the camera stays locked on target. The scene has a typical animation speed.",
    "animation": {
        "name": "orbit_left",
        "keyframes": [
            {
                "CameraAnimationRoot": {
                    "rotation": [
                        0,
                        0,
                        90
                    ]
                }
            },
            {
                "CameraAnimationRoot": {
                    "rotation": [
                        0,
                        0,
                        0
                    ]
                }
            }
        ],
        "speed_factor": 1.228949164388366
    },
    "postprocessing_caption": "Post-processing effects: No bloom is present in the scene. Medium motion blur is used in the scene.",
    "postprocessing": {
        "bloom": {
            "threshold": 0.95650677196721,
            "intensity": 0.017154654617546394,
            "radius": 6.005265759454469,
            "type": "none"
        },
        "ssao": {
            "distance": 0.7460410976901686,
            "factor": 0.7479792653337443,
            "type": "high"
        },
        "ssrr": {
            "max_roughness": 0.3143241035761323,
            "thickness": 4.2697366020954,
            "type": "high"
        },
        "motionblur": {
            "shutter_speed": 0.40866738731531493,
            "type": "medium"
        }
    },
}

You must include all attributes. Be sure to include keyframes and CameraAnimationRoot stuff. Go back and make sure you're not missing anything.

Generate the correct values for the orientation, framing, and postprocessing based on the caption:
"""


CAMERA_PROMPT = """
    For camera information:

{
    "orientation": {
        "yaw_max": 360,
        "yaw_min": 0,
        "pitch_max": 75,
        "pitch_min": 0,
        "labels": {
            "yaw": {
                "0": [
                    "facing forward",
                    "directly ahead",
                    "straight ahead",
                    "head on",
                    "<degrees> degrees forward",
                    "dead ahead",
                    "right in front",
                    "pointed forward",
                    "aligned forward",
                    "squarely ahead",
                    "<degrees>° forward"
                ],
                "15": [
                    "slightly left of front",
                    "front left quarter",
                    "slight left front",
                    "on the left side",
                    "<degrees> degrees to the left of front",
                    "<degrees> degrees left of front",
                    "<degrees> degrees to the left",
                    "<degrees>° to the left of front",
                    "<degrees>° left of front",
                    "<degrees>° to the left",
                    "just off front to the left",
                    "almost forward but left",
                    "slightly turned left"
                ],
                "30": [
                    "front left",
                    "left forward",
                    "leftward front",
                    "<degrees>° to the left",
                    "<degrees>° left",
                    "<degrees>° front left",
                    "<degrees> degrees to the left",
                    "<degrees> degrees left",
                    "<degrees> degrees front left",
                    "a bit to the left front",
                    "leaning left forward",
                    "slightly forward left",
                    "angled left forward"
                ],
                "45": [
                    "front left side",
                    "left forward side",
                    "oblique left",
                    "<degrees> degrees to the left",
                    "<degrees>° to the left",
                    "slightly to the front-left",
                    "a bit to the left of forward",
                    "not quite perpendicular to the left",
                    "just off the left forward",
                    "leaning towards the left front"
                ],
                "60": [
                    "left front quarter",
                    "left forward quarter",
                    "<degrees> degrees to the left",
                    "<degrees>° to the left",
                    "<degrees> degrees front left",
                    "<degrees>° front left",
                    "a sharp left forward",
                    "leaning strongly to the left",
                    "angled forward on the left",
                    "deep into the left quadrant",
                    "majorly to the left front"
                ],
                "75": [
                    "far left front",
                    "left side forward",
                    "<degrees> degrees to the left",
                    "<degrees>° to the left",
                    "<degrees> degrees front left",
                    "<degrees>° front left",
                    "nearing perpendicular left",
                    "heavily biased to the left front",
                    "sharply forward left",
                    "greatly to the left front",
                    "deep left but forward"
                ],
                "90": [
                    "left side",
                    "directly left",
                    "perpendicular left",
                    "<degrees> degrees to the left",
                    "<degrees>° to the left",
                    "<degrees> degrees left",
                    "<degrees>° left",
                    "direct left",
                    "straight to the left",
                    "clearly to the left",
                    "flat left",
                    "horizontal left"
                ],
                "105": [
                    "back left quarter",
                    "left rear quarter",
                    "<degrees> degrees to the left",
                    "<degrees>° to the left",
                    "<degrees> degrees rear left",
                    "<degrees>° rear left",
                    "slightly back on the left",
                    "a bit behind to the left",
                    "just off left but backwards",
                    "leaning back on the left",
                    "rearward to the left"
                ],
                "120": [
                    "rear left",
                    "left backward",
                    "<degrees> degrees to the left",
                    "<degrees>° to the left",
                    "<degrees> degrees rear left",
                    "<degrees>° rear left",
                    "back towards the left",
                    "left and slightly behind",
                    "left rearward",
                    "angled back on the left",
                    "deep into the rear-left quadrant"
                ],
                "135": [
                    "back left side",
                    "left rear side",
                    "oblique rear left",
                    "<degrees> degrees to the left",
                    "<degrees>° to the left",
                    "<degrees> degrees rear left",
                    "<degrees>° rear left",
                    "strongly to the rear-left",
                    "deep left rear",
                    "sharply back left",
                    "heavily to the back left",
                    "far back to the left"
                ],
                "150": [
                    "left rear quarter",
                    "left back quarter",
                    "<degrees> degrees to the left",
                    "<degrees>° to the left",
                    "<degrees> degrees rear left",
                    "<degrees>° rear left",
                    "sharp left back",
                    "deep into left rear quadrant",
                    "majorly to the left back",
                    "back and deep left",
                    "left and far behind"
                ],
                "165": [
                    "far back left",
                    "left side back",
                    "<degrees> degrees to the left",
                    "<degrees>° to the left",
                    "nearing directly behind on the left",
                    "<degrees> degrees rear left",
                    "<degrees>° rear left",
                    "almost back left",
                    "strong left back",
                    "heavy left rear",
                    "deeply back on the left"
                ],
                "180": [
                    "back",
                    "directly back",
                    "rear",
                    "<degrees> degrees",
                    "<degrees>°",
                    "<degrees> degrees rear",
                    "<degrees>° rear",
                    "directly behind",
                    "straight back",
                    "clear back",
                    "flat back",
                    "horizontal rear"
                ],
                "195": [
                    "far back right",
                    "right side back",
                    "<degrees> degrees to the right",
                    "<degrees>° to the right",
                    "<degrees> degrees rear right",
                    "<degrees>° rear right",
                    "nearing directly behind on the right",
                    "almost back right",
                    "strong right back",
                    "heavy right rear",
                    "deeply back on the right"
                ],
                "210": [
                    "right rear quarter",
                    "right back quarter",
                    "<degrees> degrees to the right",
                    "<degrees>° to the right",
                    "<degrees> degrees rear right",
                    "<degrees>° rear right",
                    "sharp right back",
                    "deep into right rear quadrant",
                    "majorly to the right back",
                    "back and deep right",
                    "right and far behind"
                ],
                "225": [
                    "back right side",
                    "right rear side",
                    "oblique rear right",
                    "<degrees> degrees to the right",
                    "<degrees>° to the right",
                    "<degrees> degrees rear right",
                    "<degrees>° rear right",
                    "strongly to the rear-right",
                    "deep right rear",
                    "sharply back right",
                    "heavily to the back right",
                    "far back to the right"
                ],
                "240": [
                    "rear right",
                    "right backward",
                    "<degrees> degrees to the right",
                    "<degrees>° to the right",
                    "<degrees> degrees rear right",
                    "<degrees>° rear right",
                    "back towards the right",
                    "right and slightly behind",
                    "right rearward",
                    "angled back on the right",
                    "deep into the rear-right quadrant"
                ],
                "255": [
                    "back right quarter",
                    "right rear quarter",
                    "<degrees> degrees to the right",
                    "<degrees>° to the right",
                    "<degrees> degrees rear right",
                    "<degrees>° rear right",
                    "slightly back on the right",
                    "a bit behind to the right",
                    "just off right but backwards",
                    "leaning back on the right",
                    "rearward to the right"
                ],
                "270": [
                    "right side",
                    "directly right",
                    "perpendicular right",
                    "<degrees> degrees to the right",
                    "<degrees>° to the right",
                    "<degrees> degrees right",
                    "<degrees>° right",
                    "direct right",
                    "straight to the right",
                    "clearly to the right",
                    "flat right",
                    "horizontal right"
                ],
                "285": [
                    "far right front",
                    "right side forward",
                    "<degrees> degrees to the right",
                    "<degrees>° to the right",
                    "<degrees> degrees front right",
                    "<degrees>° front right",
                    "nearing perpendicular right",
                    "heavily biased to the right front",
                    "sharply forward right",
                    "greatly to the right front",
                    "deep right but forward"
                ],
                "300": [
                    "right front quarter",
                    "right forward quarter",
                    "<degrees> degrees to the right",
                    "<degrees>° to the right",
                    "<degrees> degrees front right",
                    "<degrees>° front right",
                    "a sharp right forward",
                    "leaning strongly to the right",
                    "angled forward on the right",
                    "deep into the right quadrant",
                    "majorly to the right front"
                ],
                "315": [
                    "front right side",
                    "right forward side",
                    "oblique right",
                    "<degrees> degrees to the right",
                    "<degrees>° to the right",
                    "<degrees> degrees front right",
                    "<degrees>° front right",
                    "slightly to the front-right",
                    "a bit to the right of forward",
                    "not quite perpendicular to the right",
                    "just off the right forward",
                    "leaning towards the right front"
                ],
                "330": [
                    "front right",
                    "right forward",
                    "<degrees> degrees to the right",
                    "<degrees>° to the right",
                    "<degrees> degrees front right",
                    "<degrees>° front right",
                    "slightly to the right of forward",
                    "slight bias to the right",
                    "marginally front right"
                ],
                "345": [
                    "slightly right of front",
                    "front right quarter",
                    "slight right front",
                    "on the right side",
                    "<degrees> degrees to the right of front",
                    "<degrees> degrees right of front",
                    "<degrees> degrees to the right",
                    "just shy of directly ahead on the right",
                    "almost facing front-right",
                    "nearly front but to the right",
                    "front-right leaning",
                    "ahead but veered right"
                ]
            },
            "pitch": {
                "0": [
                    "level with the ground",
                    "horizontally level",
                    "flat",
                    "even",
                    "<degrees> degrees"
                ],
                "15": [
                    "tilted down slightly",
                    "slightly downward",
                    "mildly downward",
                    "<degrees> degrees downward",
                    "<degrees> forward"
                ],
                "30": [
                    "tilted down",
                    "downward tilt",
                    "angled down",
                    "<degrees> degrees downward",
                    "<degrees> forward",
                    "angled downward"
                ],
                "45": [
                    "tilted down sharply",
                    "sharply downward",
                    "steeply downward",
                    "<degrees> degrees downward",
                    "<degrees> forward"
                ],
                "60": [
                    "tilted down steeply",
                    "steeply angled down",
                    "sharply angled downward",
                    "<degrees> degrees downward",
                    "<degrees> forward",
                    "angled down sharply",
                    "<degrees> down steeply>",
                    "<degrees> steeply down",
                    "<degrees> sharply downward"
                ]
            }
        },
        "descriptions": [
            "The camera is <yaw>, <pitch>.",
            "The camera is <pitch>, <yaw>.",
            "<pitch>, <yaw>.",
            "Yaw: <yaw>, Pitch: <pitch>.",
            "Pitch: <pitch>, Yaw: <yaw>.",
            "Tilt: <pitch>, Rotation: <yaw>.",
            "The lens is oriented <yaw>, with a <pitch> tilt.",
            "Camera facing <yaw> and angled <pitch>.",
            "The view is set <pitch>, looking <yaw>.",
            "Orientation: <yaw>, Angle: <pitch>.",
            "The shot is angled <pitch> while pointing <yaw>.",
            "Direction <yaw>, elevation <pitch>.",
            "Adjust the camera <yaw> and <pitch>.",
            "Set the camera to <pitch> and <yaw>.",
            "Position the camera at <pitch> and <yaw>.",
            "Yaw: <yaw>, Pitch: <pitch>.",
            "Pitch: <pitch>, Yaw: <yaw>.",
            "Tilt: <pitch>, Rotation: <yaw>.",
            "Orient the camera <yaw> with a <pitch> tilt.",
            "Direct the camera <yaw>, set tilt to <pitch>.",
            "Angle the lens <pitch> and rotate it <yaw>.",
            "Configure the camera to face <yaw> with a <pitch> angle.",
            "Align the camera to <yaw>, and adjust the pitch to <pitch>.",
            "Rotate the camera to <yaw> and tilt it <pitch>."
        ]
    },
    "framings": [
        {
            "name": "extreme_closeup",
            "fov_min": 10,
            "fov_max": 15,
            "coverage_factor_min": 0.7,
            "coverage_factor_max": 1.1,
            "descriptions": [
                "Extreme close-up shot",
                "Extremely tight framing",
                "Intensely intimate view",
                "Minute details captured",
                "Right up close",
                "Heavily zoomed in",
                "Details fill the frame",
                "Extreme detail focus",
                "Extremely near view",
                "As close as possible",
                "Extremely close framing",
                "Intense close-up perspective",
                "Extreme close-up of objects",
                "Objects dominate the frame",
                "Deeply intimate close-up",
                "Extremely tight shot",
                "Incredibly close view",
                "Extreme zoom on details",
                "Macro-level perspective",
                "Microscopic framing",
                "Up-close and personal",
                "Extreme close focus",
                "Highly detailed close-up",
                "Extremely zoomed in",
                "Intensely close shot",
                "Extreme proximity",
                "Ultra-tight framing",
                "Magnified view",
                "Ultra-close perspective",
                "Extremely detailed shot"
            ]
        },
        {
            "name": "closeup",
            "fov_min": 15,
            "fov_max": 24,
            "coverage_factor_min": 1.0,
            "coverage_factor_max": 1.2,
            "descriptions": [
                "Close-up shot of the objects.",
                "Close-up framing.",
                "Close view of the objects.",
                "Objects occupy the frame closely.",
                "Intimate close-up.",
                "Tight framing on the objects.",
                "Close-up perspective.",
                "Objects captured up close.",
                "Detailed close-up shot.",
                "Tight close-up.",
                "Close-up view.",
                "Intimate close-up.",
                "Detailed close-up.",
                "Close framing.",
                "Zoomed in close on the details.",
                "Intimately close view of the objects.",
                "Fills the frame for an up-close perspective.",
                "Tight shot that gets in close to the objects.",
                "Close-up shot",
                "Tight framing",
                "Intimate view",
                "Detailed perspective",
                "Up close",
                "Zoomed in",
                "Fill the frame",
                "Details highlighted",
                "Close focus",
                "Near view",
                "In tight"
            ]
        },
        {
            "name": "medium_closeup",
            "fov_min": 24,
            "fov_max": 30,
            "coverage_factor_min": 0.8,
            "coverage_factor_max": 1.2,
            "descriptions": [
                "Medium close-up shot",
                "Slightly tight framing",
                "Fairly intimate view",
                "Moderate detail focus",
                "Closer than medium",
                "Slightly zoomed in",
                "Objects fill more frame",
                "Emphasized details",
                "Medium-close perspective",
                "Moderately near view",
                "Tighter than standard",
                "Mildly close framing",
                "Somewhat close shot",
                "Moderate object focus",
                "Slightly intimate view",
                "Medium-tight shot",
                "Moderately close view",
                "Mild zoom on details",
                "Semi-close perspective",
                "In between medium and close",
                "Somewhat zoomed in",
                "Moderately detailed shot"
            ]
        },
        {
            "name": "medium",
            "fov_min": 30,
            "fov_max": 40,
            "coverage_factor_min": 1.0,
            "coverage_factor_max": 1.5,
            "descriptions": [
                "Medium shot",
                "Medium framing",
                "Medium shot framing the objects.",
                "Medium shot.",
                "Balanced medium.",
                "Standard medium.",
                "Standard medium framing on the objects.",
                "Not too tight, not too loose - just right in the middle.",
                "A moderate, neutral distance from the subjects.",
                "Medium distance, balanced composition.",
                "Standard view",
                "Typical framing",
                "Normal distance",
                "Mid-range shot",
                "Balanced perspective",
                "Average framing",
                "Neutral distance",
                "Middle ground",
                "Moderate zoom",
                "Halfway in"
            ]
        },
        {
            "name": "medium_wide",
            "fov_min": 40,
            "fov_max": 55,
            "coverage_factor_min": 1.5,
            "coverage_factor_max": 2.0,
            "descriptions": [
                "Medium-wide shot",
                "Medium wide shot",
                "Medium wide angle",
                "Medium wide framing",
                "Medium wide shot of the objects.",
                "Medium wide perspective.",
                "Medium-wide framing on the objects.",
                "Medium-wide shot of the scene.",
                "Medium-wide.",
                "Medium wide",
                "Medium-wide angle",
                "Not too close, not too far.",
                "A bit wider than a typical medium shot.",
                "Halfway between a medium and a wide.",
                "Pulls back a little from medium framing.",
                "Slightly expanded view compared to medium.",
                "Slightly wide",
                "Moderately broad",
                "Mild widening",
                "Somewhat expansive",
                "Gently zoomed out"
            ]
        },
        {
            "name": "wide",
            "fov_min": 55,
            "fov_max": 70,
            "coverage_factor_min": 2.0,
            "coverage_factor_max": 3.0,
            "descriptions": [
                "Wide shot.",
                "Wide angle",
                "Wide framing.",
                "Wide shot of the objects.",
                "Expansive view of the objects.",
                "Expansive view.",
                "Taking in the whole scene.",
                "Expansive, broad view of the full setting and objects.",
                "Wide perspective showing everything.",
                "Zoomed out to capture the big picture.",
                "All-encompassing wide angle on the full space.",
                "Zoomed out",
                "Broad view",
                "Expansive framing",
                "Full perspective",
                "Sweeping vista",
                "All-encompassing",
                "Big picture",
                "Comprehensive view",
                "Wide-angle",
                "Broad scope",
                "Fully zoomed out"
            ]
        },
        {
            "name": "extreme_wide",
            "fov_min": 70,
            "fov_max": 90,
            "coverage_factor_min": 3.0,
            "coverage_factor_max": 4.0,
            "descriptions": [
                "Extreme wide shot",
                "Extreme wide angle",
                "Extreme wide framing",
                "Extremely broad framing",
                "Extremely zoomed out",
                "Captures entire scene",
                "Panoramic view",
                "Massively wide shot",
                "Extremely expansive framing",
                "Ultra-wide perspective",
                "Extremely wide landscape",
                "Extremely broad view",
                "Ultra-zoomed out",
                "Extremely wide angle",
                "Captures entire setting",
                "Extremely wide coverage",
                "Ultra-expansive shot",
                "Extremely wide perspective"
            ]
        }
    ],
    "animation_speed": { 
        "min": 0.5,
        "max": 2.0,
        "types": {
            "slow": {
                "min": 0.5,
                "max": 0.75,
                "descriptions": [
                    "Slow animation speed <animation_speed_value> applied.",
                    "The scene has a slow animation speed of <animation_speed_value>.",
                    "Animation speed of <animation_speed_value> is set to slow.",
                    "The scene is rendered with a slow animation speed of <animation_speed_value>.",
                    "Slow animation is used in the scene.",
                    "The scene has a leisurely animation speed of <animation_speed_value>.",
                    "Animation is set to slow speed of <animation_speed_value> in the scene.",
                    "The scene is enhanced with a slow animation speed of <animation_speed_value>.",
                    "Slow animation is present in the scene.",
                    "The scene is rendered with a slow animation speed of <animation_speed_value>.",
                    "The animation runs at a slow pace.",
                    "The scene features a gentle animation speed.",
                    "Animations are intentionally slowed down.",
                    "A calm and slow animation speed is applied.",
                    "The scene moves with a relaxed animation speed.",
                    "Animations unfold slowly in the scene.",
                    "The pace of animation is deliberately slow.",
                    "The scene showcases a leisurely animation speed.",
                    "Slow-motion animation effects are used.",
                    "The scene transitions smoothly with a slow animation speed."
                ]
            },
            "medium": {
                "min": 0.75,
                "max": 1.25,
                "descriptions": [
                    [
                        "Medium animation speed of <animation_speed_value> applied.",
                        "The scene has a moderate animation speed of <animation_speed_value>.",
                        "Animation speed is set to medium of <animation_speed_value>.",
                        "The scene is rendered with a moderate animation speed of <animation_speed_value>.",
                        "Medium animation of <animation_speed_value> is used in the scene.",
                        "The scene has a normal animation speed of <animation_speed_value>.",
                        "Animation is set to medium speed of <animation_speed_value> in the scene.",
                        "The scene is enhanced with a moderate animation speed of <animation_speed_value>.",
                        "Medium animation of <animation_speed_value> is present in the scene.",
                        "The scene is rendered with a moderate animation speed of <animation_speed_value>.",
                        "The animation speed is set to a balanced pace.",
                        "The scene features a moderate animation tempo.",
                        "Animations operate at a medium speed.",
                        "A standard animation speed is applied to the scene.",
                        "The scene progresses with a medium-paced animation.",
                        "Animations move at a steady speed in the scene.",
                        "The animation runs at a moderate rate.",
                        "The scene has a typical animation speed.",
                        "Animations flow at a medium speed in the scene.",
                        "The scene transitions at a moderate animation speed."
                    ]
                ]
            },
            "fast": {
                "min": 1.25,
                "max": 1.5,
                "descriptions": [
                    [
                        "Fast animation speed of <animation_speed_value> applied.",
                        "The scene has a fast animation speed of <animation_speed_value>.",
                        "Animation speed is set to fast of <animation_speed_value>.",
                        "The scene is rendered with a fast animation speed of <animation_speed_value>.",
                        "Fast animation of <animation_speed_value> is used in the scene.",
                        "The scene has a quick animation speed of <animation_speed_value>.",
                        "Animation is set to fast speed of <animation_speed_value> in the scene.",
                        "The scene is enhanced with a fast animation speed of <animation_speed_value>.",
                        "Fast animation of <animation_speed_value> is present in the scene.",
                        "The scene is rendered with a fast animation speed of <animation_speed_value>.",
                        "The animation runs at a rapid pace.",
                        "The scene features a swift animation speed.",
                        "Animations are set to a fast tempo.",
                        "A quick animation speed is applied.",
                        "The scene moves with high-speed animation.",
                        "Animations unfold rapidly in the scene.",
                        "The pace of animation is brisk.",
                        "The scene showcases a fast animation rate.",
                        "High-speed animation effects are used.",
                        "The scene transitions quickly with fast animation."
                    ]
                ]
            },
            "faster": {
                "min": 1.5,
                "max": 2.0,
                "descriptions": [
                    [
                        "Faster animation speed of <animation_speed_value> applied.",
                        "The scene has a faster animation speed of <animation_speed_value>.",
                        "Animation speed is set to faster of <animation_speed_value>.",
                        "The scene is rendered with a faster animation speed of <animation_speed_value>.",
                        "Faster animation of <animation_speed_value> is used in the scene.",
                        "The scene has an accelerated animation speed of <animation_speed_value>.",
                        "Animation is set to faster speed of <animation_speed_value> in the scene.",
                        "The scene is enhanced with a faster animation speed of <animation_speed_value>.",
                        "Faster animation of <animation_speed_value> is present in the scene.",
                        "The scene is rendered with a faster animation speed of <animation_speed_value>.",
                        "The animation speed is increased significantly.",
                        "The scene features an enhanced animation tempo.",
                        "Animations are set to a much faster pace.",
                        "A highly accelerated animation speed is applied.",
                        "The scene progresses with a rapid animation rate.",
                        "Animations move at an expedited speed in the scene.",
                        "The animation runs at a notably faster rate.",
                        "The scene showcases a significantly increased animation speed.",
                        "Animations unfold at an accelerated pace.",
                        "The scene transitions swiftly with enhanced animation speed."
                    ]
                ]
            }
        }
    },
    "animations": [        
        {
            "name": "static",
            "descriptions": [
                "The camera remains stationary.",
                "No camera movement.",
                "The camera holds still.",
                "Static camera shot.",
                "The camera does not move.",
                "Steady, unmoving camera.",
                "Fixed camera position.",
                "Motionless camera.",
                "The camera is locked in place.",
                "Stable, static camera view.",
                "The scene is captured from a fixed perspective.",
                "Unmoving camera angle.",
                "The camera is set in a fixed position.",
                "No motion in the camera view.",
                "The objects are captured with a static camera."
            ],
            "keyframes": []
        },
        {
            "name": "pan_left",
            "descriptions": [
                "The camera moves horizontally to the left.",
                "The view shifts to the left.",
                "Sweeping leftward movement of the camera.",
                "Camera glides left.",
                "The scene moves smoothly to the left as the camera pans.",
                "Horizontal pan to the left.",
                "The camera shifts left.",
                "Leftward panning of the camera.",
                "The camera moves left.",
                "Pan the camera left.",
                "Shift the camera smoothly to the left to alter the view.",
                "Pan the camera to the left to capture a wider perspective.",
                "Glide the camera left to change the scene laterally.",
                "Sweep the camera to the left.",
                "Move the camera horizontally to the left.",
                "Adjust the camera to pan left.",
                "Slide the camera leftward.",
                "The camera swings to the left.",
                "Lateral movement of the camera to the left.",
                "The view transitions to the left as the camera moves.",
                "Horizontal shift of the camera to the left.",
                "The camera tracks left."
            ],
            "keyframes": [
                {
                    "CameraAnimationPivot": {
                        "position": [
                            0,
                            -1.5,
                            0
                        ]
                    }
                },
                {
                    "CameraAnimationPivot": {
                        "position": [
                            0,
                            0.5,
                            0
                        ]
                    }
                }
            ]
        },
        {
            "name": "pan_right",
            "descriptions": [
                "The camera moves horizontally to the right.",
                "Pan the camera right.",
                "The view shifts to the right.",
                "Sweeping rightward movement of the camera.",
                "Camera glides right.",
                "The scene moves smoothly to the right as the camera pans.",
                "Horizontal pan to the right.",
                "The camera shifts right.",
                "Rightward panning of the camera.",
                "The camera moves right.",
                "Shift the camera smoothly to the right to alter the view.",
                "Pan the camera to the right to capture a wider perspective.",
                "Glide the camera right to change the scene laterally.",
                "Sweep the camera to the right.",
                "Move the camera horizontally to the right to extend the visible area.",
                "Rotate the camera right.",
                "Adjust the camera to pan right.",
                "Swivel the camera to the right.",
                "Turn the camera to the right.",
                "Slide the camera rightward to enhance the panoramic perspective.",
                "The camera swings to the right.",
                "Lateral movement of the camera to the right.",
                "The view transitions to the right as the camera moves.",
                "Horizontal shift of the camera to the right.",
                "The camera tracks right."
            ],
            "keyframes": [
                {
                    "CameraAnimationPivot": {
                        "position": [
                            0,
                            1.5,
                            0
                        ]
                    }
                },
                {
                    "CameraAnimationPivot": {
                        "position": [
                            0,
                            -0.5,
                            0
                        ]
                    }
                }
            ]
        },
        {
            "name": "orbit_left",
            "descriptions": [
                "The camera orbits counter-clockwise around the scene.",
                "Move the camera in a circular path to the left around the stage.",
                "The camera circles the objects to the left.",
                "Counter-clockwise orbit around the objects.",
                "Camera makes a complete rotation to the left around the objects.",
                "Orbiting left around the objects.",
                "The camera encircles the scene to the left.",
                "Circular motion of the camera around the objects to the left.",
                "Camera travels in a leftward circle around the subjects.",
                "Full orbit to the left around the objects.",
                "Orbit the camera counter-clockwise around the subject.",
                "Circle the camera to the left around the subject.",
                "Rotate the camera in a counter-clockwise path around the objects.",
                "Move the camera in a circular leftward path around the objects.",
                "Guide the camera to orbit left around the objects.",
                "Swing the camera around the objects to the left.",
                "Rotate the camera left in a circular motion around the objects.",
                "Spiral the camera to the left around the objects.",
                "Encircle the scene with the camera moving left.",
                "Turn the camera in a counter-clockwise orbit.",
                "The camera revolves around the objects counter-clockwise.",
                "Circular trajectory of the camera moving to the left around the scene.",
                "The camera arcs around the objects to the left.",
                "Rotate the camera around the objects in a counter-clockwise direction.",
                "The camera traces a circular path to the left around the subjects."
            ],
            "keyframes": [
                {
                    "CameraAnimationRoot": {
                        "rotation": [
                            0,
                            0,
                            90
                        ]
                    }
                },
                {
                    "CameraAnimationRoot": {
                        "rotation": [
                            0,
                            0,
                            0
                        ]
                    }
                }
            ]
        },
        {
            "name": "orbit_right",
            "descriptions": [
                "The camera orbits clockwise around the objects.",
                "Move the camera in a circular path to the right around the subject.",
                "The camera circles the scene to the right.",
                "Clockwise orbit around the subject.",
                "Camera makes a complete rotation to the right.",
                "Orbiting right around the objects.",
                "The camera encircles the objects to the right.",
                "Spiral motion of the camera around the objects.",
                "Camera travels in a rightward circle around the objects.",
                "Full orbit to the right around the objects.",
                "Orbit the camera clockwise around the scene.",
                "Circle the camera to the right around the stage.",
                "Rotate the camera in a clockwise path around the objects.",
                "Move the camera in a circular rightward path around the objects.",
                "Guide the camera to orbit right around the objects.",
                "Swing the camera around the objects to the right.",
                "Rotate the camera right in a circular motion around the objects.",
                "Spiral the camera to the right around the objects.",
                "Encircle the objects with the camera moving right.",
                "Spin the camera in a clockwise orbit around the objects.",
                "The camera revolves around the objects clockwise.",
                "Circular trajectory of the camera moving to the right around the scene.",
                "The camera arcs around the objects to the right.",
                "Rotate the camera around the objects in a clockwise direction.",
                "The camera traces a circular path to the right around the subjects."
            ],
            "keyframes": [
                {
                    "CameraAnimationRoot": {
                        "rotation": [
                            0,
                            0,
                            0
                        ]
                    }
                },
                {
                    "CameraAnimationRoot": {
                        "rotation": [
                            0,
                            0,
                            90
                        ]
                    }
                }
            ]
        },
        {
            "name": "orbit_up",
            "descriptions": [
                "The camera orbits upward from 0 to 90 degrees over the scene.",
                "Elevate the camera in an orbital path above the objects.",
                "Raise the camera perspective while orbiting over the scene.",
                "Orbit the camera up to a 90-degree angle.",
                "The view lifts as the camera orbits upward.",
                "Upward orbital movement of the camera.",
                "The camera arcs up and over the scene.",
                "Rotate the camera upward in an orbital trajectory.",
                "Orbit up to capture a bird's eye view of the scene.",
                "The camera ascends in an orbital path over the objects.",
                "Sweep the camera up and over the scene.",
                "Adjust the camera to orbit upward.",
                "Trace an upward orbital path with the camera.",
                "Elevate the camera perspective with an upward orbit.",
                "Orbit the camera up to look down on the scene from above."
            ],
            "keyframes": [
                {
                    "CameraFramingPivot": {
                        "rotation": [
                            0,
                            0,
                            0
                        ]
                    }
                },
                {
                    "CameraFramingPivot": {
                        "rotation": [
                            0,
                            -30,
                            0
                        ]
                    }
                }
            ]
        },
        {
            "name": "orbit_down",
            "descriptions": [
                "The camera orbits downward from 90 to 0 degrees over the scene.",
                "Lower the camera in an orbital path back to the starting position.",
                "Descend the camera perspective while orbiting over the scene.",
                "Orbit the camera down from a 90-degree angle.",
                "The view lowers as the camera orbits downward.",
                "Downward orbital movement of the camera.",
                "The camera arcs down and returns to the initial view.",
                "Rotate the camera downward in an orbital trajectory.",
                "Orbit down to restore the original camera perspective.",
                "The camera descends in an orbital path back to the objects.",
                "Sweep the camera down from the elevated position.",
                "Adjust the camera to orbit downward.",
                "Trace a downward orbital path with the camera.",
                "Lower the camera perspective with a downward orbit.",
                "Orbit the camera down to look at the scene from the starting angle."
            ],
            "keyframes": [
                {
                    "CameraFramingPivot": {
                        "rotation": [
                            0,
                            -30,
                            0
                        ]
                    }
                },
                {
                    "CameraFramingPivot": {
                        "rotation": [
                            0,
                            0,
                            0
                        ]
                    }
                }
            ]
        },
        {
            "name": "dolly_in",
            "descriptions": [
                "The camera moves closer to the subject.",
                "Dolly the camera toward the scene.",
                "The camera advances toward the objects.",
                "The camera pushes toward the subject.",
                "Dolly in to focus more closely.",
                "The camera travels in to capture details.",
                "Dolly in to highlight the objects.",
                "The camera moves in to concentrate on the objects.",
                "The camera progresses closer to emphasize details.",
                "The camera narrows the scene by moving in.",
                "The camera approaches the objects.",
                "Dolly in to bring the objects nearer.",
                "The camera advances to fill the frame with objects."
            ],
            "keyframes": [
                {
                    "CameraAnimationPivot": {
                        "position": [
                            4,
                            0,
                            0
                        ]
                    }
                },
                {
                    "CameraAnimationPivot": {
                        "position": [
                            0,
                            0,
                            0
                        ]
                    }
                }
            ]
        },
        {
            "name": "dolly_out",
            "descriptions": [
                "The camera pulls back from the scene.",
                "Dolly out to widen the view.",
                "The camera retreats from the objects.",
                "The camera withdraws from the subject.",
                "Pull the camera back.",
                "Dolly out to de-emphasize the objects.",
                "The camera moves out to show a wider scene.",
                "The camera pulls back to reveal more context.",
                "The camera pulls away, expanding the view.",
                "Dolly out to reduce focus on the objects.",
                "The camera retreats to show a broader perspective.",
                "Dolly out to place the objects into a larger context."
            ],
            "keyframes": [
                {
                    "CameraAnimationPivot": {
                        "position": [
                            0,
                            0,
                            0
                        ]
                    }
                },
                {
                    "CameraAnimationPivot": {
                        "position": [
                            4,
                            0,
                            0
                        ]
                    }
                }
            ]
        },
        {
            "name": "zoom_in",
            "descriptions": [
                "The camera zooms in.",
                "Move closer to the scene.",
                "Move toward the objects.",
                "Move toward the subject.",
                "Zoom in.",
                "Zoom in to the scene.",
                "Zoom in on the objects",
                "Camera narrows the view to concentrate on the objects.",
                "Zooming in to highlight details of the objects.",
                "The camera zooms in.",
                "The camera closes in on the objects.",
                "Zoom in to make the objects appear closer.",
                "Adjust the zoom to concentrate on the objects.",
                "Move the camera closer.",
                "Zoom in to focus on the objects.",
                "Close in on the objects with the zoom.",
                "The view narrows as the camera zooms in.",
                "The camera focuses to emphasize the objects.",
                "Zooming in brings the objects closer.",
                "The camera magnifies the view of the objects.",
                "Adjust the zoom to fill the frame with the objects."
            ],
            "keyframes": [
                {
                    "Camera": {
                        "angle_offset": 10
                    }
                },
                {
                    "Camera": {
                        "angle_offset": 0
                    }
                }
            ]
        },
        {
            "name": "zoom_out",
            "descriptions": [
                "The camera zooms out.",
                "Zoom out.",
                "Zooming out.",
                "The objects become smaller as the camera zooms out.",
                "Increasing the view width.",
                "Zooming out de-emphasizes the objects.",
                "The camera zooms in.",
                "Move away from the scene.",
                "Move away the objects.",
                "Move away the subject.",
                "Pull back.",
                "Pull away from the objects.",
                "Pull back with the camera zoom.",
                "Zoom in.",
                "Zoom in to the scene.",
                "Zoom in on the objects",
                "Zooming in to highlight details of the objects.",
                "The camera zooms in.",
                "The camera closes in on the objects.",
                "Zoom in to make the objects appear closer.",
                "Adjust the zoom to concentrate on the objects.",
                "Move the camera closer.",
                "Zoom in to focus on the objects.",
                "Close in on the objects with the zoom.",
                "Pull back with the camera zoom.",
                "The view widens as the camera zooms out.",
                "The camera pulls back to de-emphasize the objects.",
                "Zooming out pushes the objects further away.",
                "The camera reduces the view of the objects.",
                "Adjust the zoom to expand the frame beyond the objects."
            ],
            "keyframes": [
                {
                    "Camera": {
                        "angle_offset": 0
                    }
                },
                {
                    "Camera": {
                        "angle_offset": 10
                    }
                }
            ]
        },
        {
            "name": "tilt_up",
            "descriptions": [
                "The camera tilts upward.",
                "Tilt the view up.",
                "Camera points up.",
                "Angle the camera upward.",
                "The scene tilts up as the camera moves.",
                "Vertical tilt upward.",
                "The camera tilts up.",
                "Upward tilting of the camera.",
                "The camera rotates up.",
                "Tilt the camera up.",
                "Angle the camera up to shift the view vertically.",
                "Tilt the camera upward to capture more of the upper scene.",
                "Rotate the camera up to change the vertical perspective.",
                "Incline the camera upward.",
                "Move the camera to tilt up.",
                "Adjust the camera for an upward tilt.",
                "Pivot the camera up.",
                "The camera inclines vertically.",
                "Vertical rotation of the camera upward.",
                "The camera pans up.",
                "Upward pivot of the camera.",
                "The view shifts up as the camera tilts."
            ],
            "keyframes": [
                {
                    "Camera": {
                        "rotation": [
                            -25,
                            0,
                            0
                        ]
                    }
                },
                {
                    "Camera": {
                        "rotation": [
                            5,
                            0,
                            0
                        ]
                    }
                }
            ]
        },
        {
            "name": "tilt_down",
            "descriptions": [
                "The camera tilts downward.",
                "Tilt the view down.",
                "Camera points down.",
                "Angle the camera downward.",
                "The scene tilts down as the camera moves.",
                "Vertical tilt downward.",
                "The camera tilts down.",
                "Downward tilting of the camera.",
                "The camera rotates down.",
                "Tilt the camera down.",
                "Angle the camera down to shift the view vertically.",
                "Tilt the camera downward to capture more of the lower scene.",
                "Rotate the camera down to change the vertical perspective.",
                "Decline the camera downward.",
                "Move the camera to tilt down.",
                "Adjust the camera for a downward tilt.",
                "Pivot the camera down.",
                "The camera declines vertically.",
                "Vertical rotation of the camera downward.",
                "The camera pans down.",
                "Downward pivot of the camera.",
                "The view shifts down as the camera tilts."
            ],
            "keyframes": [
                {
                    "Camera": {
                        "rotation": [
                            25,
                            0,
                            0
                        ]
                    }
                },
                {
                    "Camera": {
                        "rotation": [
                            -5,
                            0,
                            0
                        ]
                    }
                }
            ]
        },
        {
            "name": "tilt_left",
            "descriptions": [
                "The camera tilts to the left.",
                "Tilt the view left.",
                "Camera banks left.",
                "Angle the camera to the left.",
                "The scene tilts left as the camera moves.",
                "Horizontal tilt to the left.",
                "The camera tilts left.",
                "Leftward tilting of the camera.",
                "The camera rotates left.",
                "Tilt the camera left.",
                "Angle the camera left to shift the view horizontally.",
                "Tilt the camera to the left.",
                "Rotate the camera left to change the horizontal perspective.",
                "Bank the camera to the left.",
                "Move the camera to tilt left.",
                "Adjust the camera for a leftward tilt.",
                "Pivot the camera left.",
                "The camera swivels to the left.",
                "Horizontal rotation of the camera to the left.",
                "The camera swivels left.",
                "Leftward banking of the camera.",
                "The view shifts to the left as the camera tilts."
            ],
            "keyframes": [
                {
                    "Camera": {
                        "rotation": [
                            0,
                            -25,
                            0
                        ]
                    }
                },
                {
                    "Camera": {
                        "rotation": [
                            0,
                            5,
                            0
                        ]
                    }
                }
            ]
        },
        {
            "name": "tilt_right",
            "descriptions": [
                "The camera tilts to the right.",
                "Tilt the view right.",
                "Camera banks right.",
                "Angle the camera to the right.",
                "The scene tilts right as the camera moves.",
                "Horizontal tilt to the right.",
                "The camera tilts right.",
                "Rightward tilting of the camera.",
                "The camera rotates right.",
                "Tilt the camera right.",
                "Angle the camera right to shift the view horizontally.",
                "Tilt the camera to the right.",
                "Rotate the camera right to change the horizontal perspective.",
                "Bank the camera to the right.",
                "Move the camera to tilt right.",
                "Adjust the camera for a rightward tilt.",
                "Pivot the camera right.",
                "The camera swivels to the right.",
                "Horizontal rotation of the camera to the right.",
                "The camera swivels right.",
                "Rightward banking of the camera.",
                "The view shifts to the right as the camera tilts."
            ],
            "keyframes": [
                {
                    "Camera": {
                        "rotation": [
                            0,
                            25,
                            0
                        ]
                    }
                },
                {
                    "Camera": {
                        "rotation": [
                            0,
                            -5,
                            0
                        ]
                    }
                }
            ]
        },
        {
            "name": "crane_up",
            "descriptions": [
                "The camera cranes upward.",
                "Crane the camera up.",
                "The view lifts as the camera cranes up.",
                "Camera rises in a crane shot.",
                "The scene moves up with the camera crane.",
                "Upward crane of the camera.",
                "The camera moves up on a crane.",
                "Craning shot lifting the camera.",
                "The camera elevates in a crane movement.",
                "Crane up with the camera.",
                "Lift the camera view using a crane shot.",
                "Crane the camera upward to gain height.",
                "Raise the camera perspective with a crane.",
                "Elevate the camera angle using a crane.",
                "Move the camera up in a craning motion.",
                "Adjust the camera to crane up.",
                "Hoist the camera view with a crane shot.",
                "The camera boom lifts.",
                "Elevating crane movement of the camera.",
                "The camera rises in a crane shot.",
                "Vertical tracking of the camera upward on a crane.",
                "The view ascends with the camera crane."
            ],
            "keyframes": [
                {
                    "CameraAnimationPivot": {
                        "position": [
                            0,
                            0,
                            0
                        ]
                    }
                },
                {
                    "CameraAnimationPivot": {
                        "position": [
                            0,
                            0,
                            2
                        ]
                    }
                }
            ]
        },
        {
            "name": "crane_down",
            "descriptions": [
                "The camera cranes downward.",
                "Crane the camera down.",
                "The view lowers as the camera cranes down.",
                "Camera descends in a crane shot.",
                "The scene moves down with the camera crane.",
                "Downward crane of the camera.",
                "The camera moves down on a crane.",
                "Craning shot lowering the camera.",
                "The camera lowers in a crane movement.",
                "Crane down with the camera.",
                "Lower the camera view using a crane shot.",
                "Crane the camera downward to reduce height.",
                "Descend the camera perspective with a crane.",
                "Lower the camera angle using a crane.",
                "Move the camera down in a craning motion.",
                "Adjust the camera to crane down.",
                "Bring down the camera view with a crane shot.",
                "The camera boom lowers.",
                "Descending crane movement of the camera.",
                "The camera falls in a crane shot.",
                "Vertical tracking of the camera downward on a crane.",
                "The view descends with the camera crane."
            ],
            "keyframes": [
                {
                    "CameraAnimationPivot": {
                        "position": [
                            0,
                            0,
                            2
                        ]
                    }
                },
                {
                    "CameraAnimationPivot": {
                        "position": [
                            0,
                            0,
                            0
                        ]
                    }
                }
            ]
        }
    ],
    "postprocessing": {
        "bloom": {
            "threshold_min": 0.7,
            "threshold_max": 1.0,
            "intensity_min": 0.0,
            "intensity_max": 1.0,
            "radius_min": 1.0,
            "radius_max": 10.0,
            "types": {
                "none": {
                    "intensity_min": 0.0,
                    "intensity_max": 0.02,
                    "descriptions": [
                        "No bloom effect applied.",
                        "The scene has no bloom effect.",
                        "Bloom effect is turned off.",
                        "The scene is rendered without bloom.",
                        "No bloom is used in the scene.",
                        "The scene does not have a bloom effect.",
                        "Bloom is disabled in the scene.",
                        "The scene is not enhanced with bloom.",
                        "No bloom is present in the scene.",
                        "The scene is not rendered with bloom."
                    ]
                },
                "low": {
                    "intensity_min": 0.02,
                    "intensity_max": 0.2,
                    "descriptions": [
                        "Low bloom effect applied.",
                        "The scene has a subtle bloom effect.",
                        "Bloom effect is set to low intensity.",
                        "The scene is rendered with a mild bloom effect.",
                        "Low bloom is used in the scene.",
                        "The scene has a slight bloom effect.",
                        "Bloom is set to low intensity in the scene.",
                        "The scene is enhanced with a gentle bloom effect.",
                        "Low bloom is present in the scene.",
                        "The scene is rendered with a mild bloom effect."
                    ]
                },
                "medium": {
                    "intensity_min": 0.2,
                    "intensity_max": 0.5,
                    "descriptions": [
                        "Medium bloom effect applied.",
                        "The scene has a moderate bloom effect.",
                        "Bloom effect is set to medium intensity.",
                        "The scene is rendered with a moderate bloom effect.",
                        "Medium bloom is used in the scene.",
                        "The scene has a noticeable bloom effect.",
                        "Bloom is set to medium intensity in the scene.",
                        "The scene is enhanced with a moderate bloom effect.",
                        "Medium bloom is present in the scene.",
                        "The scene is rendered with a moderate bloom effect."
                    ]
                },
                "high": {
                    "intensity_min": 0.5,
                    "intensity_max": 0.8,
                    "descriptions": [
                        "High bloom effect applied.",
                        "The scene has a strong bloom effect.",
                        "Bloom effect is set to high intensity.",
                        "The scene is rendered with a strong bloom effect.",
                        "High bloom is used in the scene.",
                        "The scene has a powerful bloom effect.",
                        "Bloom is set to high intensity in the scene.",
                        "The scene is enhanced with a strong bloom effect.",
                        "High bloom is present in the scene.",
                        "The scene is rendered with a strong bloom effect."
                    ]
                },
                "extreme": {
                    "intensity_min": 0.8,
                    "intensity_max": 1.1,
                    "descriptions": [
                        "Extreme bloom effect applied.",
                        "The scene has an intense bloom effect.",
                        "Bloom effect is set to extreme intensity.",
                        "The scene is rendered with an extreme bloom effect.",
                        "Extreme bloom is used in the scene.",
                        "The scene has an overwhelming bloom effect.",
                        "Bloom is set to extreme intensity in the scene.",
                        "The scene is enhanced with an extreme bloom effect.",
                        "Extreme bloom is present in the scene.",
                        "The scene is rendered with an extreme bloom effect."
                    ]
                }
            }
        },
        "ssao": {
            "distance_min": 0.02,
            "distance_max": 1.2,
            "factor_min": 0.0,
            "factor_max": 1.0,
            "types": {
                "none": {
                    "factor_min": 0.0,
                    "factor_max": 0.02,
                    "descriptions": [
                        "No ambient occlusion effect applied.",
                        "The scene has no ambient occlusion effect.",
                        "Ambient occlusion is turned off.",
                        "The scene is rendered without ambient occlusion.",
                        "No ambient occlusion is used in the scene.",
                        "The scene does not have an ambient occlusion effect.",
                        "Ambient occlusion is disabled in the scene.",
                        "The scene is not enhanced with ambient occlusion.",
                        "No ambient occlusion is present in the scene.",
                        "The scene is not rendered with ambient occlusion."
                    ]
                },
                "low": {
                    "factor_min": 0.02,
                    "factor_max": 0.2,
                    "descriptions": [
                        "Low ambient occlusion effect applied.",
                        "The scene has a subtle ambient occlusion effect.",
                        "Ambient occlusion is set to low intensity.",
                        "The scene is rendered with a mild ambient occlusion effect.",
                        "Low ambient occlusion is used in the scene.",
                        "The scene has a slight ambient occlusion effect.",
                        "Ambient occlusion is set to low intensity in the scene.",
                        "The scene is enhanced with a gentle ambient occlusion effect.",
                        "Low ambient occlusion is present in the scene.",
                        "The scene is rendered with a mild ambient occlusion effect."
                    ]
                },
                "medium": {
                    "factor_min": 0.2,
                    "factor_max": 0.6,
                    "descriptions": [
                        "Medium ambient occlusion effect applied.",
                        "The scene has a moderate ambient occlusion effect.",
                        "Ambient occlusion is set to medium intensity.",
                        "The scene is rendered with a moderate ambient occlusion effect.",
                        "Medium ambient occlusion is used in the scene.",
                        "The scene has a noticeable ambient occlusion effect.",
                        "Ambient occlusion is set to medium intensity in the scene.",
                        "The scene is enhanced with a moderate ambient occlusion effect.",
                        "Medium ambient occlusion is present in the scene.",
                        "The scene is rendered with a moderate ambient occlusion effect."
                    ]
                },
                "high": {
                    "factor_min": 0.6,
                    "factor_max": 1.0,
                    "descriptions": [
                        "High ambient occlusion effect applied.",
                        "The scene has a strong ambient occlusion effect.",
                        "Ambient occlusion is set to high intensity.",
                        "The scene is rendered with a strong ambient occlusion effect.",
                        "High ambient occlusion is used in the scene.",
                        "The scene has a powerful ambient occlusion effect.",
                        "Ambient occlusion is set to high intensity in the scene.",
                        "The scene is enhanced with a strong ambient occlusion effect.",
                        "High ambient occlusion is present in the scene.",
                        "The scene is rendered with a strong ambient occlusion effect."
                    ]
                },
                "extreme": {
                    "factor_min": 1.0,
                    "factor_max": 1.2,
                    "descriptions": [
                        "Extreme ambient occlusion effect applied.",
                        "The scene has an intense ambient occlusion effect.",
                        "Ambient occlusion is set to extreme intensity.",
                        "The scene is rendered with an extreme ambient occlusion effect.",
                        "Extreme ambient occlusion is used in the scene.",
                        "The scene has an overwhelming ambient occlusion effect.",
                        "Ambient occlusion is set to extreme intensity in the scene.",
                        "The scene is enhanced with an extreme ambient occlusion effect.",
                        "Extreme ambient occlusion is present in the scene.",
                        "The scene is rendered with an extreme ambient occlusion effect."
                    ]
                }
            }
        },
        "ssrr": {
            "min_max_roughness": 0.0,
            "max_max_roughness": 1.0,
            "min_thickness": 0.05,
            "max_thickness": 5.0,
            "types": {
                "none": {
                    "max_roughness_min": 0.0,
                    "max_roughness_max": 0.02,
                    "descriptions": [
                        "No screen-space ray tracing effect applied.",
                        "The scene has no screen-space ray tracing effect.",
                        "Screen-space ray tracing is turned off.",
                        "The scene is rendered without screen-space ray tracing.",
                        "No screen-space ray tracing is used in the scene.",
                        "The scene does not have a screen-space ray tracing effect.",
                        "Screen-space ray tracing is disabled in the scene.",
                        "The scene is not enhanced with screen-space ray tracing.",
                        "No screen-space ray tracing is present in the scene.",
                        "The scene is not rendered with screen-space ray tracing."
                    ]
                },
                "low": {
                    "max_roughness_min": 0.02,
                    "max_roughness_max": 0.1,
                    "descriptions": [
                        "Low screen-space ray tracing effect applied.",
                        "The scene has a subtle screen-space ray tracing effect.",
                        "Screen-space ray tracing is set to low intensity.",
                        "The scene is rendered with a mild screen-space ray tracing effect.",
                        "Low screen-space ray tracing is used in the scene.",
                        "The scene has a slight screen-space ray tracing effect.",
                        "Screen-space ray tracing is set to low intensity in the scene.",
                        "The scene is enhanced with a gentle screen-space ray tracing effect.",
                        "Low screen-space ray tracing is present in the scene.",
                        "The scene is rendered with a mild screen-space ray tracing effect."
                    ]
                },
                "medium": {
                    "max_roughness_min": 0.1,
                    "max_roughness_max": 0.3,
                    "descriptions": [
                        "Medium screen-space ray tracing effect applied.",
                        "The scene has a moderate screen-space ray tracing effect.",
                        "Screen-space ray tracing is set to medium intensity.",
                        "The scene is rendered with a moderate screen-space ray tracing effect.",
                        "Medium screen-space ray tracing is used in the scene.",
                        "The scene has a noticeable screen-space ray tracing effect.",
                        "Screen-space ray tracing is set to medium intensity in the scene.",
                        "The scene is enhanced with a moderate screen-space ray tracing effect.",
                        "Medium screen-space ray tracing is present in the scene.",
                        "The scene is rendered with a moderate screen-space ray tracing effect."
                    ]
                },
                "high": {
                    "max_roughness_min": 0.3,
                    "max_roughness_max": 0.6,
                    "descriptions": [
                        "High screen-space ray tracing effect applied.",
                        "The scene has a strong screen-space ray tracing effect.",
                        "Screen-space ray tracing is set to high intensity.",
                        "The scene is rendered with a strong screen-space ray tracing effect.",
                        "High screen-space ray tracing is used in the scene.",
                        "The scene has a powerful screen-space ray tracing effect.",
                        "Screen-space ray tracing is set to high intensity in the scene.",
                        "The scene is enhanced with a strong screen-space ray tracing effect.",
                        "High screen-space ray tracing is present in the scene.",
                        "The scene is rendered with a strong screen-space ray tracing effect."
                    ],
                    "extreme": {
                        "max_roughness_min": 0.6,
                        "max_roughness_max": 1.0,
                        "descriptions": [
                            "Extreme screen-space ray tracing effect applied.",
                            "The scene has an intense screen-space ray tracing effect.",
                            "Screen-space ray tracing is set to extreme intensity.",
                            "The scene is rendered with an extreme screen-space ray tracing effect.",
                            "Extreme screen-space ray tracing is used in the scene.",
                            "The scene has an overwhelming screen-space ray tracing effect.",
                            "Screen-space ray tracing is set to extreme intensity in the scene.",
                            "The scene is enhanced with an extreme screen-space ray tracing effect.",
                            "Extreme screen-space ray tracing is present in the scene.",
                            "The scene is rendered with an extreme screen-space ray tracing effect."
                        ]
                    }
                }
            }
        },
        "motionblur": {
            "shutter_speed_min": 0.0,
            "shutter_speed_max": 1.0,
            "types": {
                "none": {
                    "shutter_speed_min": 0.0,
                    "shutter_speed_max": 0.02,
                    "descriptions": [
                        "No motion blur effect applied.",
                        "The scene has no motion blur effect.",
                        "Motion blur is turned off.",
                        "The scene is rendered without motion blur.",
                        "No motion blur is used in the scene.",
                        "The scene does not have a motion blur effect.",
                        "Motion blur is disabled in the scene.",
                        "The scene is not enhanced with motion blur.",
                        "No motion blur is present in the scene.",
                        "The scene is not rendered with motion blur."
                    ]
                },
                "low": {
                    "shutter_speed_min": 0.02,
                    "shutter_speed_max": 0.2,
                    "descriptions": [
                        "Low motion blur effect applied.",
                        "The scene has a subtle motion blur effect.",
                        "Motion blur is set to low intensity.",
                        "The scene is rendered with a mild motion blur effect.",
                        "Low motion blur is used in the scene.",
                        "The scene has a slight motion blur effect.",
                        "Motion blur is set to low intensity in the scene.",
                        "The scene is enhanced with a gentle motion blur effect.",
                        "Low motion blur is present in the scene.",
                        "The scene is rendered with a mild motion blur effect."
                    ]
                },
                "medium": {
                    "shutter_speed_min": 0.2,
                    "shutter_speed_max": 0.5,
                    "descriptions": [
                        "Medium motion blur effect applied.",
                        "The scene has a moderate motion blur effect.",
                        "Motion blur is set to medium intensity.",
                        "The scene is rendered with a moderate motion blur effect.",
                        "Medium motion blur is used in the scene.",
                        "The scene has a noticeable motion blur effect.",
                        "Motion blur is set to medium intensity in the scene.",
                        "The scene is enhanced with a moderate motion blur effect.",
                        "Medium motion blur is present in the scene.",
                        "The scene is rendered with a moderate motion blur effect."
                    ]
                },
                "high": {
                    "shutter_speed_min": 0.5,
                    "shutter_speed_max": 0.8,
                    "descriptions": [
                        "High motion blur effect applied.",
                        "The scene has a strong motion blur effect.",
                        "Motion blur is set to high intensity.",
                        "The scene is rendered with a strong motion blur effect.",
                        "High motion blur is used in the scene.",
                        "The scene has a powerful motion blur effect.",
                        "Motion blur is set to high intensity in the scene.",
                        "The scene is enhanced with a strong motion blur effect.",
                        "High motion blur is present in the scene.",
                        "The scene is rendered with a strong motion blur effect."
                    ]
                },
                "extreme": {
                    "shutter_speed_min": 0.8,
                    "shutter_speed_max": 1.0,
                    "descriptions": [
                        "Extreme motion blur effect applied.",
                        "The scene has an intense motion blur effect.",
                        "Motion blur is set to extreme intensity.",
                        "The scene is rendered with an extreme motion blur effect.",
                        "Extreme motion blur is used in the scene.",
                        "The scene has an overwhelming motion blur effect.",
                        "Motion blur is set to extreme intensity in the scene.",
                        "The scene is enhanced with an extreme motion blur effect.",
                        "Extreme motion blur is present in the scene.",
                        "The scene is rendered with an extreme motion blur effect."
                    ]
                }
            }
        }
    }
}

Examples:

Example 1:
Prompt/caption: 

caption: A small wooden book with a lock, a teapot, text that says 'Geo Winter', and an axe are all in the scene. The text is behind the teapot, the axe is to the right of the teapot, and the book is behind both the axe and the teapot. The book moves backward slowly, the teapot moves forward slowly, the text slides left, and the axe moves forward. The camera follows the axe. The camera is angled right and level with the ground.  The scene has a slight blur effect. The scene is animated at a fast pace.

{
    "orientation_caption": "Camera orientation: Yaw: oblique right, Pitch: level with the ground.",
    "orientation": {
        "yaw": 317,
        "pitch": 1
    },
    "framing_caption": "Camera framing: Captures entire scene Set the fov of the camera to 74 degrees. (23.00 mm focal length)",
    "framing": {
        "fov": 74,
        "coverage_factor": 3.3787342191874163,
        "name": "extreme_wide"
    },
    "animation_caption": "Camera animation: Throughout the sequence, the camera follows a red, yellow, and black axe seamlessly. The scene transitions quickly with fast animation.",
    "animation": {
        "name": "crane_down",
        "keyframes": [
            {
                "CameraAnimationPivot": {
                    "position": [
                        0,
                        0,
                        2
                    ]
                }
            },
            {
                "CameraAnimationPivot": {
                    "position": [
                        0,
                        0,
                        0
                    ]
                }
            }
        ],
        "speed_factor": 1.3754731748375375
    },
    "postprocessing_caption": "Post-processing effects: The scene is rendered with a mild ambient occlusion effect. The scene has a slight motion blur effect.",
    "postprocessing": {
        "bloom": {
            "threshold": 0.9049127669422115,
            "intensity": 0.8086834007424443,
            "radius": 4.345852681550845,
            "type": "extreme"
        },
        "ssao": {
            "distance": 0.2675165509421571,
            "factor": 0.03757264803813665,
            "type": "low"
        },
        "ssrr": {
            "max_roughness": 0.674072407335585,
            "thickness": 0.7906668082985685,
            "type": "none"
        },
        "motionblur": {
            "shutter_speed": 0.1915879393539549,
            "type": "low"
        }
    },
}


Example 2:

Prompt/caption: A house with a roof is on the left and a skateboard ramp with a building is on the right. The house moves left and the skateboard ramp moves backward. The camera is positioned behind and to the right, tilted steeply downward, showing a zoomed-in view. The scene has subtle shadows and a slight ray tracing effect. The background is a Belfast Sunset with a grassy and rocky ground.

{
    "orientation_caption": "Camera orientation: Orientation: right back quarter, Angle: tilted down steeply.",
    "orientation": {
        "yaw": 214,
        "pitch": 69
    },
    "framing_caption": "Camera framing: Magnified view The camera has a 181 mm focal length.",
    "framing": {
        "fov": 11,
        "coverage_factor": 0.8407846352674209,
        "name": "extreme_closeup"
    },
    "animation_caption": "Camera animation:  Fast animation speed of 1.4x applied.",
    "animation": {
        "name": "zoom_in",
        "keyframes": [
            {
                "Camera": {
                    "angle_offset": 10
                }
            },
            {
                "Camera": {
                    "angle_offset": 0
                }
            }
        ],
        "speed_factor": 1.404395487155547
    },
    "postprocessing_caption": "Post-processing effects: The scene is enhanced with a gentle ambient occlusion effect. Low screen-space ray tracing effect applied.",
    "postprocessing": {
        "bloom": {
            "threshold": 0.8496130440000619,
            "intensity": 0.2220181839537102,
            "radius": 1.3457534686773542,
            "type": "medium"
        },
        "ssao": {
            "distance": 0.4896150467674612,
            "factor": 0.04480824053315857,
            "type": "low"
        },
        "ssrr": {
            "max_roughness": 0.09441042633095198,
            "thickness": 0.7098830677702582,
            "type": "low"
        },
        "motionblur": {
            "shutter_speed": 0.9803040919463857,
            "type": "extreme"
        }
    },
}

Example 3:
Prompt/caption: Show a stick with a blue and white handle, a blue crystal, and a blue and white bird, a light blue hat with white polka dots, a turquoise window with a blue shutter, a chicken in a yellow dress, and a house with trees, a pool, and cars. The chicken is to the right and behind the window. The window is in front of the hat. The stick is to the left of the hat. The hat is behind the window. The house is to the left of and behind the window. Camera follows the chicken as it moves. Camera is positioned low and far right. Use a very wide camera angle. The scene is set at night with cobblestone flooring, no bloom, and some motion blur.

{
    "orientation_caption": "Camera orientation: The camera is steeply downward, far back to the right.",
    "orientation": {
        "yaw": 225,
        "pitch": 40
    },
    "framing_caption": "Camera framing: Extremely wide coverage The camera has a 76 degree field of view. (22.00 mm focal length)",
    "framing": {
        "fov": 76,
        "coverage_factor": 3.8613973293159507,
        "name": "extreme_wide"
    },
    "animation_caption": "Camera animation: Capturing A low poly a chicken in a yellow dress in motion, the camera stays locked on target. The scene has a typical animation speed.",
    "animation": {
        "name": "orbit_left",
        "keyframes": [
            {
                "CameraAnimationRoot": {
                    "rotation": [
                        0,
                        0,
                        90
                    ]
                }
            },
            {
                "CameraAnimationRoot": {
                    "rotation": [
                        0,
                        0,
                        0
                    ]
                }
            }
        ],
        "speed_factor": 1.228949164388366
    },
    "postprocessing_caption": "Post-processing effects: No bloom is present in the scene. Medium motion blur is used in the scene.",
    "postprocessing": {
        "bloom": {
            "threshold": 0.95650677196721,
            "intensity": 0.017154654617546394,
            "radius": 6.005265759454469,
            "type": "none"
        },
        "ssao": {
            "distance": 0.7460410976901686,
            "factor": 0.7479792653337443,
            "type": "high"
        },
        "ssrr": {
            "max_roughness": 0.3143241035761323,
            "thickness": 4.2697366020954,
            "type": "high"
        },
        "motionblur": {
            "shutter_speed": 0.40866738731531493,
            "type": "medium"
        }
    },
}

You must include all attributes. Be sure to include keyframes and CameraAnimationRoot stuff. Go back and make sure you're not missing anything.
I already have the JSON generated below, just make sure that it's correct values for the orientation, framing, and postprocessing based on the caption.
JSON:
"""


def parse_gemini_json(raw_output: str) -> Optional[dict]:
    print(raw_output)
    """
    Parse the JSON output from the Gemini API.

    Args:
        raw_output (str): The raw output from the Gemini API.

    Returns:
        Optional[dict]: The parsed JSON output as a dictionary, or None if an error occurs.
    """

    try:
        if "```json" in raw_output:
            json_start = raw_output.index("```json") + 7
            json_end = raw_output.rindex("```")
            json_content = raw_output[json_start:json_end].strip()
        else:
            json_content = raw_output.strip()

        # Remove any leading or trailing commas
        json_content = json_content.strip(',')

        # If the content starts with a key (e.g., "objects":), wrap it in curly braces
        if json_content.strip().startswith('"') and ':' in json_content:
            json_content = "{" + json_content + "}"

        # Parse the JSON
        parsed_json = json.loads(json_content)
        
        return parsed_json
    except json.JSONDecodeError as e:
        print(f"Error parsing JSON: {str(e)}")
        return None


def setup_gemini():
    genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
    generation_config = {
        "temperature": 1,
        "top_p": 0.95,
        "top_k": 64,
        "max_output_tokens": 8192,
        "response_mime_type": "text/plain",
    }
    model = genai.GenerativeModel(
        model_name=MODEL,
        generation_config=generation_config,
    )
    return model


def generate_gemini(model, template, prompt):
    import time
    content = template + prompt

    max_retries = 3
    step = 30
    delay = 30

    for attempt in range(max_retries):
        try:
            chat_session = model.start_chat(history=[])
            response = chat_session.send_message(content)
            captions_content = response.text.strip()
            return captions_content
        except Exception as e:
            print(f"Error on attempt {attempt + 1}: {str(e)}")
            if attempt < max_retries - 1:
                print(f"Retrying in {delay} seconds...")
                time.sleep(delay)
                delay += step  # Exponential backoff
            else:
                print("Max retries reached. Returning None.")
                return None

    return None
```````

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/camera.py`:

```````py
import logging
import math

import bpy
from mathutils import Vector

import numpy as np
from scipy.spatial.transform import Rotation as R

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)


def rotate_points(points, angles):
    """
    Rotate points by given angles in degrees for (x, y, z) rotations.

    Args:
        points (np.ndarray): The points to rotate.
        angles (tuple): The angles to rotate by in degrees for (x, y, z) rotations.

    Returns:
        np.ndarray: The rotated points.
    """

    rotation = R.from_euler("xyz", angles, degrees=True)
    return np.array([rotation.apply(point) for point in points])


def compute_camera_distance(points, fov_deg):
    """
    Calculate the camera distance required to frame the bounding sphere of the points.

    Args:
        points (np.ndarray): The points to frame.
        fov_deg (float): The field of view in degrees.

    Returns:
        tuple: The camera distance, centroid of the points, and the radius of the bounding sphere.
    
    """
    # Calculate the center of the bounding sphere (use the centroid for simplicity)
    centroid = np.mean(points, axis=0)
    # Calculate the radius as the max distance from the centroid to any point
    radius = np.max(np.linalg.norm(points - centroid, axis=1))
    # Calculate the camera distance using the radius and the field of view
    fov_rad = math.radians(fov_deg)
    distance = radius / math.tan(fov_rad / 2)
    return distance, centroid, radius


def perspective_project(points, camera_distance, fov_deg, aspect_ratio=1.0):
    """
    Project points onto a 2D plane using a perspective projection considering the aspect ratio.

    Args:
        points (np.ndarray): The points to project.
        camera_distance (float): The distance of the camera from the origin.
        fov_deg (float): The field of view in degrees.
        aspect_ratio (float): The aspect ratio of the screen. Defaults to 1.0.
    
    Returns:
        np.ndarray: The screen space coordinates of the projected
    """
    screen_points = []
    fov_rad = math.radians(fov_deg)
    f = 1.0 / math.tan(fov_rad / 2)
    for point in points:
        # Translate point to camera's frame of reference (camera along the positive x-axis)
        p_cam = np.array([camera_distance - point[0], point[1], point[2]])
        # Apply perspective projection if the point is in front of the camera
        if p_cam[0] > 0:
            x = (p_cam[1] * f) / (p_cam[0] * aspect_ratio)  # Adjust x by aspect ratio
            y = p_cam[2] * f / p_cam[0]
            # Normalize to range [0, 1] for OpenGL screen space
            screen_x = (x + 1) / 2
            screen_y = (y + 1) / 2
            screen_points.append((screen_x, screen_y))
    return np.array(screen_points)


def create_camera_rig() -> bpy.types.Object:
    """
    Creates a camera rig consisting of multiple objects in Blender.

    Returns:
        dict: A dictionary containing the created objects:
            - camera_animation_root: The root object of the camera animation hierarchy.
            - camera_orientation_pivot_yaw: The yaw pivot object for camera orientation.
            - camera_orientation_pivot_pitch: The pitch pivot object for camera orientation.
            - camera_framing_pivot: The pivot object for camera framing.
            - camera_animation_pivot: The pivot object for camera animation.
            - camera_object: The camera object.
            - camera: The camera data.
    """
    camera_animation_root = bpy.data.objects.new("CameraAnimationRoot", None)
    bpy.context.scene.collection.objects.link(camera_animation_root)

    camera_orientation_pivot_yaw = bpy.data.objects.new(
        "CameraOrientationPivotYaw", None
    )
    camera_orientation_pivot_yaw.parent = camera_animation_root
    bpy.context.scene.collection.objects.link(camera_orientation_pivot_yaw)

    camera_orientation_pivot_pitch = bpy.data.objects.new(
        "CameraOrientationPivotPitch", None
    )
    camera_orientation_pivot_pitch.parent = camera_orientation_pivot_yaw
    bpy.context.scene.collection.objects.link(camera_orientation_pivot_pitch)

    camera_framing_pivot = bpy.data.objects.new("CameraFramingPivot", None)
    camera_framing_pivot.parent = camera_orientation_pivot_pitch
    bpy.context.scene.collection.objects.link(camera_framing_pivot)

    camera_animation_pivot = bpy.data.objects.new("CameraAnimationPivot", None)
    camera_animation_pivot.parent = camera_framing_pivot
    bpy.context.scene.collection.objects.link(camera_animation_pivot)

    camera = bpy.data.cameras.new("Camera")
    camera_object = bpy.data.objects.new("Camera", camera)

    # Rotate the Camera 90º
    camera_object.delta_rotation_euler = [1.5708, 0, 1.5708]
    camera_object.data.lens_unit = "FOV"

    camera_object.parent = camera_animation_pivot
    bpy.context.scene.collection.objects.link(camera_object)

    bpy.context.scene.camera = camera_object

    return {
        "camera_animation_root": camera_animation_root,
        "camera_orientation_pivot_yaw": camera_orientation_pivot_yaw,
        "camera_orientation_pivot_pitch": camera_orientation_pivot_pitch,
        "camera_framing_pivot": camera_framing_pivot,
        "camera_animation_pivot": camera_animation_pivot,
        "camera_object": camera_object,
        "camera": camera,
    }


def set_camera_settings(combination: dict) -> None:
    """
    Applies camera settings from a combination to the Blender scene.

    This function updates various camera settings including orientation, pivot adjustments, and
    framing based on the provided combination dictionary.

    Args:
        combination (dict): A dictionary containing camera settings including 'fov', 'animation',
                            and orientation details.
    Returns:
        None
    """
    camera = bpy.context.scene.objects["Camera"]
    camera_data = camera.data

    postprocessing = combination.get("postprocessing", {})

    # Apply bloom settings
    bloom_settings = postprocessing.get("bloom", {})
    threshold = bloom_settings.get("threshold", 0.8)
    intensity = bloom_settings.get("intensity", 0.5)
    radius = bloom_settings.get("radius", 5.0)
    bpy.context.scene.eevee.use_bloom = True
    bpy.context.scene.eevee.bloom_threshold = threshold
    bpy.context.scene.eevee.bloom_intensity = intensity
    bpy.context.scene.eevee.bloom_radius = radius

    # Apply SSAO settings
    ssao_settings = postprocessing.get("ssao", {})
    distance = ssao_settings.get("distance", 0.2)
    factor = ssao_settings.get("factor", 0.5)
    bpy.context.scene.eevee.use_gtao = True
    bpy.context.scene.eevee.gtao_distance = distance
    bpy.context.scene.eevee.gtao_factor = factor

    # Apply SSRR settings
    ssrr_settings = postprocessing.get("ssrr", {})
    max_roughness = ssrr_settings.get("max_roughness", 0.5)
    thickness = ssrr_settings.get("thickness", 0.1)
    bpy.context.scene.eevee.use_ssr = True
    bpy.context.scene.eevee.use_ssr_refraction = True
    bpy.context.scene.eevee.ssr_max_roughness = max_roughness
    bpy.context.scene.eevee.ssr_thickness = thickness

    # Apply motion blur settings
    motionblur_settings = postprocessing.get("motionblur", {})
    shutter_speed = motionblur_settings.get("shutter_speed", 0.5)
    bpy.context.scene.eevee.use_motion_blur = True
    bpy.context.scene.eevee.motion_blur_shutter = shutter_speed

    # Get the initial lens value from the combination
    initial_lens = combination["framing"]["fov"]

    # Get the first keyframe's angle_offset value, if available
    animation = combination["animation"]
    keyframes = animation["keyframes"]
    if (
        keyframes
        and "Camera" in keyframes[0]
        and "angle_offset" in keyframes[0]["Camera"]
    ):
        angle_offset = keyframes[0]["Camera"]["angle_offset"]
        camera_data.angle = math.radians(initial_lens + angle_offset)
    else:
        camera_data.angle = math.radians(initial_lens)

    orientation_data = combination["orientation"]
    orientation = {"pitch": orientation_data["pitch"], "yaw": orientation_data["yaw"]}

    # Rotate CameraOrientationPivotYaw by the Y
    camera_orientation_pivot_yaw = bpy.data.objects.get("CameraOrientationPivotYaw")
    camera_orientation_pivot_yaw.rotation_euler[2] = orientation["yaw"] * math.pi / 180

    # Rotate CameraOrientationPivotPitch by the X
    camera_orientation_pivot_pitch = bpy.data.objects.get("CameraOrientationPivotPitch")
    camera_orientation_pivot_pitch.rotation_euler[1] = (
        orientation["pitch"] * -math.pi / 180
    )

    # set the camera framerate to 30
    bpy.context.scene.render.fps = 30


def set_camera_animation(combination: dict, frame_interval: int, animation_length: int) -> None:
    """
    Applies the specified animation to the camera based on the keyframes from the camera_data.json file.
    The total animation frames are fixed to ensure consistent speed.

    Args:
        combination (dict): The combination dictionary containing animation data.

    Returns:
        None
    """
    animation = combination["animation"]
    speed_factor = animation.get("speed_factor", 1)/1.5
    keyframes = animation["keyframes"]
    adjusted_frame_interval = frame_interval * (animation_length / 100)

    for i, keyframe in enumerate(keyframes):
        for obj_name, transforms in keyframe.items():
            obj = bpy.data.objects.get(obj_name)
            if obj is None:
                raise ValueError(f"Object {obj_name} not found in the scene")
            frame = int(i * adjusted_frame_interval)
            for transform_name, value in transforms.items():
                if transform_name == "position":
                    obj.location = [coord * speed_factor for coord in value]
                    obj.keyframe_insert(data_path="location", frame=frame)
                elif transform_name == "rotation":
                    obj.rotation_euler = [
                        math.radians(angle * speed_factor) for angle in value
                    ]
                    obj.keyframe_insert(data_path="rotation_euler", frame=frame)
                elif transform_name == "scale":
                    obj.scale = [coord * speed_factor for coord in value]
                    obj.keyframe_insert(data_path="scale", frame=frame)
                elif transform_name == "angle_offset" and obj_name == "Camera":
                    camera_data = bpy.data.objects["Camera"].data
                    camera_data.angle = math.radians(
                        combination["framing"]["fov"] + value
                    )
                    camera_data.keyframe_insert(data_path="lens", frame=frame)

    bpy.context.scene.frame_set(0)
    

def position_camera(combination: dict, focus_object: bpy.types.Object) -> None:
    """
    Positions the camera based on the coverage factor and lens values.

    Args:
        combination (dict): The combination dictionary containing coverage factor and lens values.
        focus_object (bpy.types.Object): The object to focus the camera on.

    Returns:
        None
    """
    camera = bpy.context.scene.objects["Camera"]

    # Get the bounding box of the focus object in world space
    bpy.context.view_layer.update()
    
    bbox = [
        focus_object.matrix_world @ Vector(corner) for corner in focus_object.bound_box
    ]
    bbox_points = np.array([corner.to_tuple() for corner in bbox])

    # Rotate points as per the desired view angle if any
    # Assuming we want to compute this based on some predefined rotation angles
    rotation_angles = (45, 45, 45)  # Example rotation angles
    rotated_points = rotate_points(bbox_points, rotation_angles)

    # scale rotated_points by combination["framing"]["coverage_factor"]
    rotated_points *= combination["framing"]["coverage_factor"]

    # Calculate the camera distance to frame the rotated bounding box correctly
    fov_deg = combination["framing"][
        "fov"
    ]  # Get the FOV from combination or default to 45
    aspect_ratio = (
        bpy.context.scene.render.resolution_x / bpy.context.scene.render.resolution_y
    )

    camera_distance, centroid, radius = compute_camera_distance(
        rotated_points, fov_deg / aspect_ratio
    )

    # Set the camera properties
    camera.data.angle = math.radians(fov_deg)  # Set camera FOV
    if aspect_ratio >= 1:
        camera.data.sensor_fit = "HORIZONTAL"
    else:
        camera.data.sensor_fit = "VERTICAL"

    bbox = [
        focus_object.matrix_world @ Vector(corner) for corner in focus_object.bound_box
    ]
    bbox_min = min(bbox, key=lambda v: v.z)
    bbox_max = max(bbox, key=lambda v: v.z)

    # Calculate the height of the bounding box
    bbox_height = bbox_max.z - bbox_min.z

    # Position the camera based on the computed distance
    camera.location = Vector((camera_distance, 0, 0))  # Adjust this as needed


    # Set the position of the CameraAnimationRoot object to slightly above the focus object center, quasi-rule of thirds
    # bbox_height / 2 is the center of the bounding box, bbox_height / 1.66 is more aesthetically pleasing
    bpy.data.objects["CameraAnimationRoot"].location = focus_object.location + Vector(
        (0, 0, bbox_height/2)
    )

```````

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/distributed.py`:

```````py
import argparse
import json
import logging
import os
import time
from tqdm import tqdm
from typing import Dict

from distributask.distributask import Distributask

from .worker import run_job

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)

if __name__ == "__main__":

    def get_env_vars(path: str = ".env") -> Dict[str, str]:
        """Get the environment variables from the specified file."""
        env_vars = {}
        if not os.path.exists(path):
            return env_vars
        with open(path, "r") as f:
            for line in f:
                key, value = line.strip().split("=")
                env_vars[key] = value
        return env_vars

    def get_settings(args):
        env_vars = get_env_vars(".env")

        # Get settings from parsed argements or env, set defaults
        settings = {
            "start_index": args.start_index or int(env_vars.get("START_INDEX", 0)),
            "combinations_file": args.combinations_file
            or env_vars.get("COMBINATIONS_FILE", "combinations.json"),
            "end_index": args.end_index or int(env_vars.get("END_INDEX", 100)),
            "start_frame": args.start_frame or int(env_vars.get("START_FRAME", 0)),
            "end_frame": args.end_frame or int(env_vars.get("END_FRAME", 300)),
            "width": args.width or int(env_vars.get("WIDTH", 1280)),
            "height": args.height or int(env_vars.get("HEIGHT", 720)),
            "output_dir": args.output_dir or env_vars.get("OUTPUT_DIR", "./renders"),
            "hdri_path": args.hdri_path or env_vars.get("HDRI_PATH", "./backgrounds"),
            "max_price": args.max_price or float(env_vars.get("MAX_PRICE", 0.1)),
            "max_nodes": args.max_nodes or int(env_vars.get("MAX_NODES", 1)),
            "api_key": args.api_key or env_vars.get("VAST_API_KEY", ""),
            "redis_host": args.redis_host or env_vars.get("REDIS_HOST", "localhost"),
            "redis_port": args.redis_port or int(env_vars.get("REDIS_PORT", 6379)),
            "redis_user": args.redis_user or env_vars.get("REDIS_USER", ""),
            "redis_password": args.redis_password or env_vars.get("REDIS_PASSWORD", ""),
            "amazon_key_id": args.amazon_key_id or env_vars.get("AMAZON_KEY_ID", ""),
            "amazon_secret_key": args.amazon_secret_key
            or env_vars.get("AMAZON_SECRET_KEY", ""),
            "s3_bucket_name": args.s3_bucket_name or env_vars.get("S3_BUCKET_NAME", ""),
            "hf_token": args.hf_token or env_vars.get("HF_TOKEN", ""),
            "hf_repo_id": args.hf_repo_id or env_vars.get("HF_REPO_ID", ""),
            "broker_pool_limit": args.broker_pool_limit
            or int(env_vars.get("BROKER_POOL_LIMIT", 1)),
            "render_batch_size": args.render_batch_size
            or int(env_vars.get("RENDER_BATCH_SIZE", 1)),
            "inactivity_check_interval": args.inactivity_check or int(env_vars.get("INACTVITY_INTERVAL", 600)),
            "upload_destination": args.upload_dest or env_vars.get("UPLOAD_DEST", "s3")
        }

        # Load combinations from file
        with open(settings["combinations_file"], "r") as f:
            combinations = json.load(f)
            settings["combinations"] = combinations["combinations"]

        return settings

    def start_new_job(args):
        logger.info("Starting a new job...")
        settings = get_settings(args)

        # Override environment variables with provided arguments
        os.environ["REDIS_HOST"] = args.redis_host or settings["redis_host"]
        os.environ["REDIS_PORT"] = str(args.redis_port or settings["redis_port"])
        os.environ["REDIS_USER"] = args.redis_user or settings["redis_user"]
        os.environ["REDIS_PASSWORD"] = args.redis_password or settings["redis_password"]
        os.environ["HF_TOKEN"] = args.hf_token or settings["hf_token"]
        os.environ["HF_REPO_ID"] = args.hf_repo_id or settings["hf_repo_id"]

        job_config = {
            "max_price": settings["max_price"],
            "max_nodes": settings["max_nodes"],
            "start_index": settings["start_index"],
            "end_index": settings["end_index"],
            "combinations": settings["combinations"],
            "width": settings["width"],
            "height": settings["height"],
            "output_dir": settings["output_dir"],
            "hdri_path": settings["hdri_path"],
            "start_frame": settings["start_frame"],
            "end_frame": settings["end_frame"],
            "api_key": settings["api_key"],
            "hf_token": settings["hf_token"],
            "hf_repo_id": settings["hf_repo_id"],
            "redis_host": settings["redis_host"],
            "redis_port": settings["redis_port"],
            "redis_user": settings["redis_user"],
            "redis_password": settings["redis_password"],
            "amazon_key_id": settings["amazon_key_id"],
            "s3_bucket_name": settings["s3_bucket_name"],
            "amazon_secret_key": settings["amazon_secret_key"],
            "broker_pool_limit": settings["broker_pool_limit"],
            "render_batch_size": settings["render_batch_size"],
            "inactivity_check_interval": settings["inactivity_check_interval"],
            "upload_destination": settings["upload_destination"]
        }

        instance_env = {
            "VAST_API_KEY": settings["api_key"],
            "HF_TOKEN": settings["hf_token"],
            "HF_REPO_ID": settings["hf_repo_id"],
            "REDIS_HOST": settings["redis_host"],
            "REDIS_PORT": settings["redis_port"],
            "REDIS_USER": settings["redis_user"],
            "REDIS_PASSWORD": settings["redis_password"],
            "AWS_ACCESS_KEY_ID": settings["amazon_key_id"],
            "AWS_SECRET_ACCESS_KEY": settings["amazon_secret_key"],
            "S3_BUCKET_NAME": settings["s3_bucket_name"],
            "BROKER_POOL_LIMIT": settings["broker_pool_limit"],
        }

        print("*** JOB CONFIG")
        for key, value in job_config.items():
            if key != "combinations":
                print(f"{key}: {value}")

        distributask = Distributask(
            hf_repo_id=job_config["hf_repo_id"],
            hf_token=job_config["hf_token"],
            vast_api_key=job_config["api_key"],
            redis_host=job_config["redis_host"],
            redis_port=job_config["redis_port"],
            redis_username=job_config["redis_user"],
            redis_password=job_config["redis_password"],
            broker_pool_limit=job_config["broker_pool_limit"],
        )

        max_price = job_config["max_price"]
        max_nodes = job_config["max_nodes"]
        docker_image = "antbaez/simian-worker:latest"
        module_name = "simian.worker"

        # Rent and set up vastai nodes with docker image
        print("Searching for nodes...")
        num_nodes_avail = len(distributask.search_offers(max_price))
        print("Total nodes available: ", num_nodes_avail)

        rented_nodes = distributask.rent_nodes(
            max_price, max_nodes, docker_image, module_name, env_settings=instance_env
        )

        print("Total nodes rented: ", len(rented_nodes))

        distributask.register_function(run_job)

        while True:
            user_input = input("press r when workers are ready: ")
            if user_input == "r":
                break

        tasks = []

        batch_size = job_config["render_batch_size"]
        # Submit tasks to queue in batches
        for combination_index in range(
            job_config["start_index"],
            job_config["end_index"],
            batch_size,
        ):
            task = distributask.execute_function(
                "run_job",
                {
                    "combination_indeces": [
                        index
                        for index in range(
                            combination_index,
                            min(combination_index + batch_size, settings["end_index"]),
                        )
                    ],
                    "combinations": [
                        job_config["combinations"][index]
                        for index in range(
                            combination_index,
                            min(combination_index + batch_size, settings["end_index"]),
                        )
                    ],
                    "width": job_config["width"],
                    "height": job_config["height"],
                    "output_dir": job_config["output_dir"],
                    "hdri_path": job_config["hdri_path"],
                    "upload_dest": job_config["upload_destination"],
                    "start_frame": job_config["start_frame"],
                    "end_frame": job_config["end_frame"]
                },
            )
            tasks.append(task)

        # distributask.monitor_tasks(tasks, show_time_left=False)

        print("Tasks sent. Starting monitoring")
        inactivity_log = {node["instance_id"]: 0 for node in rented_nodes}

        start_time = time.time()
        with tqdm(total=len(tasks), unit="task") as pbar:
            while not all(task.ready() for task in tasks):

                current_tasks = sum([task.ready() for task in tasks])
                pbar.update(current_tasks - pbar.n)

                time.sleep(1)
                current_time = time.time()
                # check if node is inactive at set interval
                if current_time - start_time > settings["inactivity_check_interval"]:
                    start_time = time.time()
                    for node in rented_nodes:
                        # get log with api call
                        log_response = distributask.get_node_log(node)
                        if log_response:
                            # if "Task completed" in two consecutive logs, terminate node
                            try:
                                last_msg = log_response.text.splitlines()[-3:]
                                if (
                                    any("Task completed" in msg for msg in last_msg)
                                    and inactivity_log[node["instance_id"]] == 0
                                ):
                                    inactivity_log[node["instance_id"]] = 1
                                elif (
                                    any("Task completed" in msg for msg in last_msg)
                                    and inactivity_log[node["instance_id"]] == 1
                                ):
                                    distributask.terminate_nodes([node])
                                    print("node terminated")
                                else:
                                    inactivity_log[node["instance_id"]] == 0
                            except:
                                pass

        print("All tasks have been completed!")

    parser = argparse.ArgumentParser(description="Simian CLI")
    parser.add_argument("--start_index", type=int, help="Starting index for rendering")
    parser.add_argument(
        "--combinations-file", help="Path to the combinations JSON file"
    )
    parser.add_argument("--end_index", type=int, help="Ending index for rendering")
    parser.add_argument("--start_frame", type=int, help="Starting frame number")
    parser.add_argument("--end_frame", type=int, help="Ending frame number")
    parser.add_argument("--width", type=int, help="Rendering width in pixels")
    parser.add_argument("--height", type=int, help="Rendering height in pixels")
    parser.add_argument("--output_dir", help="Output directory")
    parser.add_argument("--hdri_path", help="HDRI path")
    parser.add_argument("--max_price", type=float, help="Maximum price per hour")
    parser.add_argument("--max_nodes", type=int, help="Maximum number of nodes")
    parser.add_argument("--api_key", help="Vast.ai API key")
    parser.add_argument("--redis_host", help="Redis host")
    parser.add_argument("--redis_port", type=int, help="Redis port")
    parser.add_argument("--redis_user", help="Redis user")
    parser.add_argument("--redis_password", help="Redis password")
    parser.add_argument("--hf_token", help="Hugging Face token")
    parser.add_argument("--hf_repo-id", help="Hugging Face repository ID")
    parser.add_argument("--amazon_key_id", help="amazon secret key id")
    parser.add_argument("--amazon_secret_key", help="amazon secret access key")
    parser.add_argument("--s3_bucket_name", help="amazon s3 bucket name")
    parser.add_argument(
        "--broker_pool_limit", type=int, help="Limit on redis pool size"
    )
    parser.add_argument(
        "--render_batch_size", type=int, help="Batch size of simian rendering"
    )
    parser.add_argument("--inactivity_check", type=int, help="The interval of checking and terminating nodes for inactivity in seconds"
    )
    parser.add_argument("--upload_dest", type=int, help="The desired destination of uploads at task completion"
    )
    args = parser.parse_args()

    start_new_job(args)

```````

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/transform.py`:

```````py
import logging
from math import radians, cos, sin
from typing import Dict, List, Union
import numpy as np

import bpy
import mathutils
from mathutils import Vector

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)


def degrees_to_radians(deg: float) -> float:
    """Convert degrees to radians.

    Args:
        deg (float): Angle in degrees.

    Returns:
        float: Angle in radians.
    """
    return radians(deg)


def compute_rotation_matrix(theta: float) -> List[List[float]]:
    """Compute the 2D rotation matrix for a given angle.

    Args:
        theta (float): Angle in radians.

    Returns:
        List[List[float]]: 2D rotation matrix.
    """
    return [[cos(theta), -sin(theta)], [sin(theta), cos(theta)]]


def apply_rotation(
    point: List[float], rotation_matrix: List[List[float]]
) -> List[Union[int, float]]:
    """Apply rotation matrix to a point and round to integer if close.

    Args:
        point (List[float]): 2D point as [x, y].
        rotation_matrix (List[List[float]]): 2D rotation matrix.

    Returns:
        List[Union[int, float]]: Rotated point with rounded coordinates.
    """
    rotated_point = np.dot(rotation_matrix, np.array(point))
    return [
        round(val) if abs(val - round(val)) < 1e-9 else val for val in rotated_point
    ]


def adjust_positions(objects: List[Dict], camera_yaw: float) -> List[Dict]:
    """Adjust the positions of objects based on the camera yaw.

    Args:
        objects (List[Dict]): List of object dictionaries.
        camera_yaw (float): Camera yaw angle in degrees.

    Returns:
        List[Dict]: List of object dictionaries with adjusted positions.
    """
    rotation_matrix = compute_rotation_matrix(radians(camera_yaw))
    lookup_table = {
        0: (-1, 1),
        1: (0, 1),
        2: (1, 1),
        3: (-1, 0),
        4: (0, 0),
        5: (1, 0),
        6: (-1, -1),
        7: (0, -1),
        8: (1, -1),
    }

    empty_objs = []
    for obj in objects:
        grid_x, grid_y = lookup_table[obj["placement"]]
        empty_obj = obj.copy()
        empty_obj["transformed_position"] = apply_rotation(
            [grid_x, grid_y], rotation_matrix
        )
        empty_objs.append(empty_obj)
    return empty_objs


def determine_relationships(objects: List[Dict], camera_yaw: float) -> List[str]:
    """Determine the spatial relationships between objects based on camera yaw.

    Args:
        objects (List[Dict]): List of object dictionaries.
        camera_yaw (float): Camera yaw angle in degrees.

    Returns:
        List[str]: List of relationship strings.
    """
    inverse_rotation_matrix = compute_rotation_matrix(radians(-camera_yaw))

    relationships = []
    for i, obj1 in enumerate(objects):
        for j, obj2 in enumerate(objects):
            if i != j:
                pos1 = apply_rotation(
                    obj1["transformed_position"], inverse_rotation_matrix
                )
                pos2 = apply_rotation(
                    obj2["transformed_position"], inverse_rotation_matrix
                )

                relationship = ""

                if pos2[1] > pos1[1]:
                    relationship = "to the left of"
                elif pos2[1] < pos1[1]:
                    relationship = "to the right of"

                if pos2[0] > pos1[0]:
                    relationship += " and behind"
                elif pos2[0] < pos1[0]:
                    relationship += " and in front of"

                if relationship:
                    # Remove trailing periods from object names
                    obj1_name = obj1['name'].rstrip('.')
                    obj2_name = obj2['name'].rstrip('.')

                    relationships.append(
                        f"{obj1_name} is {relationship} {obj2_name}."
                    )

    return relationships


def find_largest_length(objects: List[Dict[bpy.types.Object, Dict]]) -> float:
    """Find the largest dimension among the objects' bounding boxes.

    Args:
        objects (List[Dict[bpy.types.Object, Dict]]): List of object dictionaries.

    Returns:
        float: Largest dimension.
    """
    largest_dimension = 0
    for obj_dict in objects:
        obj = list(obj_dict.keys())[0]
        bpy.context.view_layer.update()
        bbox_corners = obj.bound_box
        width = (
            max(bbox_corners, key=lambda v: v[0])[0]
            - min(bbox_corners, key=lambda v: v[0])[0]
        )
        height = (
            max(bbox_corners, key=lambda v: v[1])[1]
            - min(bbox_corners, key=lambda v: v[1])[1]
        )
        current_max = max(width, height)
        largest_dimension = max(largest_dimension, current_max)

    return largest_dimension


def get_world_bounding_box_xy(obj: bpy.types.Object) -> List[Vector]:
    """Get the 2D bounding box corners of an object in world space.

    Args:
        obj (bpy.types.Object): Blender object.

    Returns:
        List[Vector]: List of 2D bounding box corners in world space.
    """
    world_bbox = [obj.matrix_world @ Vector(corner) for corner in obj.bound_box]
    min_x = min(corner.x for corner in world_bbox)
    max_x = max(corner.x for corner in world_bbox)
    min_y = min(corner.y for corner in world_bbox)
    max_y = max(corner.y for corner in world_bbox)
    corners_xy = [
        Vector((min_x, min_y, 0)),
        Vector((max_x, min_y, 0)),
        Vector((min_x, max_y, 0)),
        Vector((max_x, max_y, 0)),
    ]
    # logger.info(f"Object {obj.name} bounding box: {corners_xy}")
    return corners_xy


def check_overlap_xy(
    bbox1: List[Vector], bbox2: List[Vector], padding: float = 0.08
) -> bool:
    """Check if two 2D bounding boxes overlap with optional padding.

    Args:
        bbox1 (List[Vector]): First bounding box corners.
        bbox2 (List[Vector]): Second bounding box corners.
        padding (float, optional): Padding around the bounding boxes. Defaults to 0.08.

    Returns:
        bool: True if the bounding boxes overlap, False otherwise.
    """
    min1 = Vector(
        (min(corner.x for corner in bbox1), min(corner.y for corner in bbox1), 0)
    )
    max1 = Vector(
        (max(corner.x for corner in bbox1), max(corner.y for corner in bbox1), 0)
    )
    min2 = Vector(
        (min(corner.x for corner in bbox2), min(corner.y for corner in bbox2), 0)
    )
    max2 = Vector(
        (max(corner.x for corner in bbox2), max(corner.y for corner in bbox2), 0)
    )
    overlap = not (
        min1.x > max2.x + padding
        or max1.x < min2.x - padding
        or min1.y > max2.y + padding
        or max1.y < min2.y - padding
    )

    # logger.info(
    #     f"Checking overlap between {bbox1} and {bbox2} with padding {padding}: {overlap}"
    # )
    return overlap


def bring_objects_to_origin(objects):
    def move_object_and_children(obj, move_vector):
        obj.location += move_vector
        for child in obj.children:
            move_object_and_children(child, move_vector)

    for obj_dict in objects:
        obj = list(obj_dict.keys())[0]
        if obj.parent is not None:
            continue  # Skip children, they will be moved with their parent

        if obj.location.length < 0.01:
            continue

        step_size = min(obj.location.length / 10, 0.5)
        iterations = 0
        max_iterations = 1000

        while iterations < max_iterations:
            direction = Vector((-obj.location.x, -obj.location.y, 0)).normalized()
            move_vector = direction * step_size
            
            # Move the parent object and all its children
            move_object_and_children(obj, move_vector)
            
            bpy.context.view_layer.update()
            
            current_obj_bbox = get_world_bounding_box_xy(obj)
            collision = False

            for other_obj_dict in objects:
                other_obj = list(other_obj_dict.keys())[0]
                if other_obj != obj and other_obj.parent is None:
                    other_bbox = get_world_bounding_box_xy(other_obj)
                    if check_overlap_xy(current_obj_bbox, other_bbox):
                        collision = True
                        break

            if collision:
                # Revert the movement
                move_object_and_children(obj, -move_vector)
                bpy.context.view_layer.update()
                step_size *= 0.5
                if step_size < 0.001:
                    break
                continue

            if obj.location.length < 0.01:
                break

            iterations += 1
            step_size = min(obj.location.length / 10, 0.5)

    bpy.context.view_layer.update()


def place_objects_on_grid(
    objects: List[Dict[bpy.types.Object, Dict]], largest_length: float
) -> None:
    """Place objects on a grid based on their transformed positions, allowing stacking.

    Args:
        objects (List[Dict[bpy.types.Object, Dict]]): List of object dictionaries.
        largest_length (float): Largest dimension among the objects.
    """
    grid_heights = {}

    for obj_dict in objects:
        obj = list(obj_dict.keys())[0]
        transformed_position = obj_dict[obj]["transformed_position"]
        
        grid_x = round(transformed_position[0])
        grid_y = round(transformed_position[1])
        grid_pos = (grid_x, grid_y)

        current_height = grid_heights.get(grid_pos, 0)

        # Use the object's current z-location for the first object at this position
        if current_height == 0:
            z_position = obj.location.z
        else:
            z_position = current_height

        obj.location = Vector(
            (
                grid_x * largest_length,
                grid_y * largest_length,
                z_position
            )
        )

        # Update the height for the next object
        grid_heights[grid_pos] = z_position + obj.dimensions.z

    bpy.context.view_layer.update()
    if len(objects) > 1:
        bring_objects_to_origin(objects)


def create_mesh_from_vertices(vertices, name="Plane_POV"):
    mesh = bpy.data.meshes.new(name)
    obj = bpy.data.objects.new(name, mesh)
    bpy.context.collection.objects.link(obj)
    
    # Ensure we have 4 vertices
    if len(vertices) == 4:
        faces = [(0, 1, 2, 3)]
    else:
        faces = []

    mesh.from_pydata(vertices, [], faces)
    mesh.update()


def visualize_frustum(camera, vertices):
    curve_data = bpy.data.curves.new(name='FrustumLines', type='CURVE')
    curve_data.dimensions = '3D'
    curve_data.resolution_u = 2

    obj = bpy.data.objects.new('FrustumLines', curve_data)
    bpy.context.collection.objects.link(obj)

    polyline = curve_data.splines.new('POLY')
    polyline.points.add(len(vertices) + 1)  # Add points for each vertex plus one for the camera location

    polyline.points[0].co = (camera.location.x, camera.location.y, camera.location.z, 1)
    for i, vertex in enumerate(vertices):
        polyline.points[i + 1].co = (vertex.x, vertex.y, vertex.z, 1)


def get_plane_dimensions(plane):
    """Get the width and height of the plane."""
    bbox = [plane.matrix_world @ Vector(corner) for corner in plane.bound_box]
    min_x = min(v.x for v in bbox)
    max_x = max(v.x for v in bbox)
    min_y = min(v.y for v in bbox)
    max_y = max(v.y for v in bbox)

    width = max_x - min_x
    height = max_y - min_y
    return width, height


def get_camera_plane_vertices(camera, frame_number):
    scene = bpy.context.scene
    depsgraph = bpy.context.evaluated_depsgraph_get()
    cam_data = camera.data

    # Set the frame and update the scene
    bpy.context.scene.frame_set(frame_number)
    bpy.context.view_layer.update()

    cam_matrix_world = camera.matrix_world

    # Log camera settings
    # logger.info(f"Camera Focal Length: {cam_data.lens}")
    # logger.info(f"Camera Sensor Width: {cam_data.sensor_width}")
    # logger.info(f"Camera Sensor Height: {cam_data.sensor_height}")
    # logger.info(f"Camera Aspect Ratio: {cam_data.sensor_width / cam_data.sensor_height}")

    # Get the camera's frustum
    frame = cam_data.view_frame(scene=scene)

    # Convert the frame coordinates to world coordinates
    frame_world = [cam_matrix_world @ v for v in frame]

    # Get the stage's position
    stage = bpy.data.objects.get("Stage")
    if not stage:
        logger.error("Stage object not found!")
        return []

    stage_position = stage.location
    plane_point = stage_position

    # Calculate intersection of rays with the plane
    plane_vertices = []
    for v in frame_world:
        direction = v - cam_matrix_world.translation
        direction.normalize()

        result, location, normal, index, obj, matrix = scene.ray_cast(depsgraph, cam_matrix_world.translation, direction)

        # Get plane dimensions
        plane_width, plane_height = get_plane_dimensions(stage)
        max_distance = min(plane_width, plane_height) / 4  # Define a maximum reasonable distance based on plane size
        
        if result:
            # Flatten Z value to the plane height if above it
            if location.z > plane_point.z:
                location.z = plane_point.z
                # logger.warning(f"Flattened location to plane height: {location}")

            # Check if the location extends beyond the plane boundaries
            if (location - stage_position).length > max_distance:
                location = cam_matrix_world.translation + direction * (max_distance / 2)
                location.z = plane_point.z
                # logger.warning(f"Clamped location to half the maximum distance: {location}")
            
            # logger.info(f"Ray hit at: {location}")
            plane_vertices.append(location)
        else:
            # If the ray does not hit the plane, use a default position on the plane
            intersection_point = cam_matrix_world.translation + direction * 10.0  # Adjust as needed
            intersection_point.z = plane_point.z
            # logger.warning(f"Ray did not hit the plane, using default position: {intersection_point}")
            plane_vertices.append(intersection_point)

    # Ensure vertices form a rectangle
    if len(plane_vertices) == 4:
        plane_vertices = [plane_vertices[i] for i in [1, 0, 3, 2]]
    
    # logger.warning(f"Expected 4 plane vertices, got {len(plane_vertices)}")

    # logger.info(f"Plane vertices: {plane_vertices}")
    return plane_vertices


def visualize_plane_vertices(vertices):
    curve_data = bpy.data.curves.new(name='PlaneVerticesLines', type='CURVE')
    curve_data.dimensions = '3D'
    curve_data.resolution_u = 2

    obj = bpy.data.objects.new('PlaneVerticesLines', curve_data)
    bpy.context.collection.objects.link(obj)

    polyline = curve_data.splines.new('POLY')
    polyline.points.add(len(vertices) + 1)  # Add points for each vertex and close the loop

    for i, vertex in enumerate(vertices):
        polyline.points[i].co = (vertex.x, vertex.y, vertex.z, 1)
    polyline.points[-1].co = (vertices[0].x, vertices[0].y, vertices[0].z, 1)  # Close the loop


def draw_vector_from_camera(camera_obj):
    import bmesh
    bm = bmesh.new()
    layer = bm.verts.layers.float_vector.new('direction')
    cam_matrix = camera_obj.matrix_world
    cam_location = cam_matrix.translation
    frame = camera_obj.data.view_frame(scene=bpy.context.scene)
    
    for v in frame:
        world_coord = cam_matrix @ v
        direction = world_coord - cam_location
        vert = bm.verts.new(cam_location)
        vert[layer] = direction  # Store direction as custom data on the vertex
        
        # Create an edge to visualize the direction
        end_point = vert.co + direction.normalized() * 10  # Extend the direction
        end_vert = bm.verts.new(end_point)
        bm.edges.new([vert, end_vert])
    
    mesh = bpy.data.meshes.new("VectorVisual")
    bm.to_mesh(mesh)
    obj = bpy.data.objects.new("VectorVisualObj", mesh)
    bpy.context.collection.objects.link(obj)
    bm.free()


def apply_animation(objects, focus_obj, yaw, start_frame, end_frame, camera_follow):
    yaw_radians = radians(yaw)
    rotation_matrix = mathutils.Matrix.Rotation(yaw_radians, 4, 'Z')
    scene = bpy.context.scene
    camera = scene.camera 

    # Get camera plane vertices
    plane_vertices = get_camera_plane_vertices(camera, start_frame)
    # create_mesh_from_vertices(plane_vertices)
    # visualize_frustum(camera, plane_vertices)
    # visualize_plane_vertices(plane_vertices)

    # Calculate midpoints of each edge
    bottom_center = (plane_vertices[0] + plane_vertices[3]) / 2
    top_center = (plane_vertices[1] + plane_vertices[2]) / 2
    left_center = (plane_vertices[2] + plane_vertices[3]) / 2
    right_center = (plane_vertices[0] + plane_vertices[1]) / 2

    for obj_dict in objects:
        obj = list(obj_dict.keys())[0]
        properties = list(obj_dict.values())[0]
        movement = properties.get('movement')

        if not movement:
            continue

        # Determine initial placement based on movement direction
        if movement["direction"] == "right":
            initial_position = left_center
        elif movement["direction"] == "left":
            initial_position = right_center
        elif movement["direction"] == "forward":
            initial_position = bottom_center
        elif movement["direction"] == "backward":
            initial_position = top_center
        elif movement["direction"] == "up":
            initial_position = obj.location


        # Rotate direction vector according to yaw
        direction_vector = mathutils.Vector({
            "forward": (-1, 0, 0),
            "backward": (1, 0, 0),
            "right": (0, 1, 0),
            "left": (0, -1, 0),
            "up": (0, 0, 1)
        }[movement["direction"]])

        if movement["direction"] != "up":
            rotated_vector = rotation_matrix @ direction_vector
        else:
            rotated_vector = direction_vector
        step_vector = rotated_vector * movement["speed"]

        # Position object at initial location at the start frame
        scene.frame_set(start_frame)
        
        if not camera_follow:
            obj.location += initial_position - (step_vector * 4)
        
        obj.keyframe_insert(data_path="location", frame=start_frame)
        camera = bpy.data.objects["CameraAnimationRoot"]
        for frame in range(start_frame + 1, end_frame + 1):
            if obj == focus_obj and camera_follow:
                camera.location += step_vector
                camera.keyframe_insert(data_path="location", frame=frame)

            obj = list(obj_dict.keys())[0]
            obj.location += step_vector
            obj.keyframe_insert(data_path="location", frame=frame)
            bpy.context.view_layer.update()
```````

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/combiner.py`:

```````py
import logging
import json
import math
import os
import random
import argparse
from typing import Any, Dict, List, Optional
from mathutils import Vector
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn

from .transform import determine_relationships, adjust_positions

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)


def read_json_file(file_path: str) -> Dict[str, Any]:
    """
    Read a JSON file and return the data as a dictionary.

    Args:
        file_path (str): Path to the JSON file.

    Returns:
        Dict[str, Any]: Data from the JSON file.
    """
    with open(file_path, "r") as file:
        return json.load(file)


def parse_args() -> argparse.Namespace:
    """
    Parse command line arguments using argparse.

    Returns:
        argparse.Namespace: Parsed arguments.
    """
    parser = argparse.ArgumentParser(description="Generate random camera combinations.")
    parser.add_argument(
        "--count", type=int, default=10, help="Number of combinations to generate"
    )
    parser.add_argument(
        "--seed", type=int, default=None, help="Seed for the random number generator"
    )
    parser.add_argument(
        "--max_number_of_objects",
        type=int,
        default=5,
        help="Maximum number of objects to randomly select",
    )
    parser.add_argument(
        "--camera_file_path",
        type=str,
        default="data/camera_data.json",
        help="Path to the JSON file containing camera data",
    )
    parser.add_argument(
        "--object_data_path",
        type=str,
        default="data/object_data.json",
        help="Path to the JSON file containing object data",
    )
    parser.add_argument(
        "--texture_data_path",
        type=str,
        default="datasets/texture_data.json",
        help="Path to the JSON file containing texture data",
    )
    parser.add_argument(
        "--datasets_path",
        type=str,
        default="data/datasets.json",
        help="Path to the file which lists all the datasets to use",
    )
    parser.add_argument(
        "--cap3d_captions_path",
        type=str,
        default="datasets/cap3d_captions.json",
        help="Path to the JSON file containing captions data",
    )
    parser.add_argument(
        "--simdata_path",
        type=str,
        default="datasets",
        help="Path to the simdata directory",
    )
    parser.add_argument(
        "--output_path",
        type=str,
        default="combinations.json",
        help="Path to the output file",
    )
    parser.add_argument(
        "--stage_data_path",
        type=str,
        default="data/stage_data.json",
        help="Path to the JSON file containing stage data",
    )
    parser.add_argument(
        "--movement",
        action="store_true",
        help="Apply movement to all, none, or random objects."
    )
    parser.add_argument(
        "--ontop",
        action="store_true",
        help="Allow objects to be on top of each other."
    )
    parser.add_argument(
        "--camera_follow",
        action="store_true",
        help="Camera will follow specified object as it moves (for individual objects)."
    )
    parser.add_argument(
        "--random",
        action="store_true",
        help="Randomly apply movement, object stacking, and camera follow effects"
    )
    return parser.parse_args()


def generate_stage_captions(combination: Dict[str, Any]) -> List[str]:
    """
    Generate captions for the stage based on the combination data.

    Args:
        combination (Dict[str, Any]): Combination data.

    Returns:
        List[str]: List of stage captions.
    """
    current_dir = os.path.dirname(os.path.abspath(__file__))

    stage_data_path = os.path.join(current_dir, "../data/stage_data.json")
    stage_data = read_json_file(stage_data_path)

    background_prefix = random.choice(stage_data["background_names"])
    floor_prefix = random.choice(stage_data["material_names"])
    background_name = combination["background"]["name"]
    floor_material_name = combination["stage"]["material"]["name"]

    # remove all numbers and trim
    floor_material_name = "".join(
        [i for i in floor_material_name if not i.isdigit()]
    ).strip()
    background_name = (
        "".join([i for i in background_name if not i.isdigit()])
        .strip()
        .replace("#", "")
        .replace("_", " ")
        .replace("(", "")
        .replace(")", "")
    )
    caption_parts = [
        f"The {background_prefix} is {background_name}.",
        f"The {floor_prefix} is {floor_material_name}.",
    ]
    return caption_parts


def generate_orientation_caption(
    camera_data: Dict[str, Any], combination: Dict[str, Any]
) -> str:
    """
    Generate a caption for the camera orientation based on the combination data.

    Args:
        camera_data (Dict[str, Any]): Camera data.
        combination (Dict[str, Any]): Combination data.

    Returns:
        str: Orientation caption.
    """
    pitch_labels = camera_data["orientation"]["labels"]["pitch"]
    yaw_labels = camera_data["orientation"]["labels"]["yaw"]

    closest_pitch_label = min(
        pitch_labels.keys(),
        key=lambda x: abs(int(x) - int(combination["orientation"]["pitch"])),
    )
    closest_yaw_label = min(
        yaw_labels.keys(),
        key=lambda x: abs(int(x) - int(combination["orientation"]["yaw"])),
    )

    # Replace the placeholders in the camera text with the closest matching labels
    orientation_text = random.choice(camera_data["orientation"]["descriptions"])
    orientation_text = (
        orientation_text.replace(
            "<pitch>", random.choice(pitch_labels[closest_pitch_label])
        )
        .replace("<degrees>", str(combination["orientation"]["pitch"]))
        .replace("<yaw>", random.choice(yaw_labels[closest_yaw_label]))
        .replace("<degrees>", str(combination["orientation"]["yaw"]))
    )

    return orientation_text


def meters_to_feet_rounded(meters: float) -> int:
    """
    Convert meters to feet and round the result.

    Args:
        meters (float): Distance in meters.

    Returns:
        int: Distance in feet (rounded).
    """
    feet_per_meter = 3.28084
    return round(meters * feet_per_meter)


def generate_object_name_description_captions(
    combination: Dict[str, Any], object_data
) -> str:
    """
    Generate captions for object names and descriptions based on the combination data.

    Args:
        combination (Dict[str, Any]): Combination data.

    Returns:
        str: Object name and description captions.
    """
    object_name_descriptions = []
    for obj in combination["objects"]:
        object_description = obj["description"]

        object_scale = obj["scale"]
        scale_factor = object_scale["factor"]
        scale_name = object_scale["name_synonym"]

        object_name_description_relationship = random.choice(
            object_data["name_description_relationship"]
        )

        # Replace placeholders with actual values
        object_name_description_relationship = (
            object_name_description_relationship.replace("<name>", object_description)
            .replace("<size>", scale_name, 1)
        )

        random_metric_m = random.choice(["meters", "m", ""])
        size_in_meters = f"{scale_factor}{random_metric_m}"
        object_name_description_relationship = (
            object_name_description_relationship.replace(
                "<size_in_meters>", size_in_meters, 1
            )
        )

        random_metric_f = random.choice(["feet", "ft", ""])
        size_in_feet = f"{meters_to_feet_rounded(scale_factor)}{random_metric_f}"
        object_name_description_relationship = (
            object_name_description_relationship.replace(
                "<size_in_feet>", size_in_feet, 1
            )
        )

        object_name_descriptions.append(object_name_description_relationship)

    random.shuffle(object_name_descriptions)
    object_name_descriptions = " ".join(object_name_descriptions)

    return object_name_descriptions


def generate_relationship_captions(combination: Dict[str, Any]) -> List[str]:
    """
    Generate captions for object relationships based on the combination data.

    Args:
        combination (Dict[str, Any]): Combination data.

    Returns:
        List[str]: List of relationship captions.
    """
    threshold_relationships = len(combination["objects"])

    adjusted_objects = adjust_positions(
        combination["objects"], combination["orientation"]["yaw"]
    )

    camera_yaw = combination["orientation"]["yaw"]

    relationships = determine_relationships(adjusted_objects, camera_yaw)

    # Write relationships into the JSON file with combination["objects"]["relationships"]
    for i, obj in enumerate(combination["objects"]):
        obj.setdefault("relationships", [])
        if i < len(relationships):
            obj["relationships"] = relationships[i]

    selected_relationships = relationships
    if threshold_relationships < len(relationships):
        selected_relationships = random.sample(relationships, threshold_relationships)

    return selected_relationships


def add_movement_to_objects(objects, movement=False, max_speed=0.5):
    if movement:
        for obj in objects:
            direction = random.choice(["left", "right", "forward", "backward", "up"])
            speed = random.uniform(0.1, max_speed)/1.5
            obj["movement"] = {"direction": direction, "speed": speed}
    return objects


def generate_fov_caption(combination: Dict[str, Any]) -> str:
    """
    Generate a caption for the field of view (FOV) based on the combination data.

    Args:
        combination (Dict[str, Any]): Combination data.

    Returns:
        str: FOV caption.
    """
    
    fov_templates = {
        "degrees": [
            "The camera has a <fov> degree field of view.",
            "The camera has a <fov> degree FOV.",
            "The field of view is <fov> degrees.",
            "Set the fov of the camera to <fov> degrees.",
            "Set the FOV of the camera to <fov>°",
        ],
        "mm": [
            "The camera has a <mm> mm focal length.",
            "The focal length is <mm> mm.",
            "Set the focal length of the camera to <mm> mm.",
        ],
    }

    fov = combination["framing"]["fov"]

    # FOV is stored as degrees in the framing data
    fov_types = "degrees", "mm"

    # Select a random FOV type
    fov_type = random.choice(fov_types)

    # Select a random FOV template
    fov_template = random.choice(fov_templates[fov_type])

    # Replace the <fov> placeholder with the FOV value
    fov_template = fov_template.replace("<fov>", str(fov))

    # Convert FOV to focal length
    focal_length = int(35 / (2 * math.tan(math.radians(fov) / 2)))
    fov_caption = fov_template.replace("<mm>", str(focal_length))

    if fov_type == "degrees":
        fov_caption += f" ({focal_length:.2f} mm focal length)"

    return fov_caption


def generate_postprocessing_caption(combination: Dict[str, Any], camera_data) -> str:
    """
    Generate a caption for postprocessing based on the combination data.

    Args:
        combination (Dict[str, Any]): Combination data.
        camera_data (Dict[str, Any]): Camera data.

    Returns:
        str: Postprocessing caption.
    """
    postprocessing = combination["postprocessing"]
    caption_parts = []

    postprocessing_options = camera_data["postprocessing"]

    for key in postprocessing:
        post_type = postprocessing[key]["type"]
        if key == "bloom":
            bloom_caption = random.choice(
                postprocessing_options["bloom"]["types"][post_type]["descriptions"]
            )
            caption_parts.append(bloom_caption)
        elif key == "ssao":
            ssao_caption = random.choice(
                postprocessing_options["ssao"]["types"][post_type]["descriptions"]
            )
            caption_parts.append(ssao_caption)
        elif key == "ssrr":
            ssrr_caption = random.choice(
                postprocessing_options["ssrr"]["types"][post_type]["descriptions"]
            )
            caption_parts.append(ssrr_caption)
        elif key == "motionblur":
            motionblur_caption = random.choice(
                postprocessing_options["motionblur"]["types"][post_type]["descriptions"]
            )
            caption_parts.append(motionblur_caption)

    # Ensure we have at least one caption
    if not caption_parts:
        return "No significant post-processing effects applied."

    # Ensure at least 1-2 captions remain
    min_captions = min(2, len(caption_parts))
    if len(caption_parts) > min_captions:
        num_to_pop = random.randint(1, len(caption_parts) - min_captions)
        for _ in range(num_to_pop):
            random_index_to_remove = random.randint(0, len(caption_parts) - 1)
            caption_parts.pop(random_index_to_remove)

    return " ".join(caption_parts)


def generate_framing_caption(
    camera_data: Dict[str, Any], combination: Dict[str, Any]
) -> str:
    """
    Generate a caption for framing based on the camera data and combination data.
    Copy codeArgs:
        camera_data (Dict[str, Any]): Camera data.
        combination (Dict[str, Any]): Combination data.

    Returns:
        str: Framing caption.
    """
    framing = combination["framing"]
    framing_name = framing["name"]

    # Find the matching framing in camera_data
    matching_framing = next(
        (f for f in camera_data["framings"] if f["name"] == framing_name), None
    )

    if matching_framing:
        framing_description = random.choice(matching_framing["descriptions"])
        framing_description = framing_description.replace(
            "<fov>", str(framing["fov"])
        ).replace("<coverage_factor>", str(framing["coverage_factor"]))
        return framing_description
    else:
        return ""


def flatten_descriptions(descriptions: List[Any]) -> List[str]:
    """
    Flatten a list of descriptions, which may contain nested lists.
    Copy codeArgs:
        descriptions (List[Any]): A list of descriptions which may contain nested lists.

    Returns:
        List[str]: A flattened list of descriptions.
    """
    flat_list = []
    for item in descriptions:
        if isinstance(item, list):
            flat_list.extend(flatten_descriptions(item))
        else:
            flat_list.append(item)
    return flat_list


def speed_factor_to_percentage(speed_factor: float) -> str:
    """
    Convert speed factor to a percentage string.
    Copy codeArgs:
        speed_factor (float): Speed factor value.

    Returns:
        str: Speed factor as a percentage string.
    """
    rounded_speed_factor = round(speed_factor, 2)
    percentage = int(rounded_speed_factor * 100)
    return f"{percentage}%"


def generate_animation_captions(combination, camera_data) -> List[str]:
    logging.basicConfig(level=logging.DEBUG)
    animation_name = combination["animation"]["name"]
    speed_factor = combination["animation"]["speed_factor"]
    
    logging.debug(f"Searching for animation: {animation_name}")
    logging.debug(f"Animations in camera_data: {[anim['name'] for anim in camera_data.get('animations', [])]}")
    
    animation = next((anim for anim in camera_data['animations'] if anim['name'] == animation_name), None)
    
    if not animation or 'descriptions' not in animation:
        logging.error(f"No descriptions found for animation: {animation_name}")
        return [f"The camera performs a {animation_name} animation."]
    
    animation_description = random.choice(animation['descriptions'])
    
    speed_type = next(
        (t for t, v in camera_data['animation_speed']['types'].items() 
         if v['min'] <= speed_factor <= v['max']),
        None
    )
    
    if not speed_type:
        logging.warning(f"No matching speed type found for speed factor: {speed_factor}")
        speed_description = f"at a speed factor of {speed_factor:.2f}"
    else:
        speed_descriptions = camera_data['animation_speed']['types'][speed_type]['descriptions']
        
        # Handle potentially nested speed descriptions
        if isinstance(speed_descriptions[0], list):
            speed_descriptions = speed_descriptions[0]
        
        speed_description = random.choice(speed_descriptions)
        speed_description = speed_description.replace('<animation_speed_value>', f"{speed_factor:.2f}")
    
    result = [f"{animation_description} {speed_description}"]
    logging.debug(f"Generated caption: {result}")
    return result


def generate_movement_captions(combination: Dict[str, Any], object_data) -> List[str]:
    """
    Generate captions for object movement based on the combination data.

    Args:
        combination (Dict[str, Any]): Combination data.
        object_data (Dict[str, Any]): Object data including movement templates and speed descriptions.

    Returns:
        List[str]: List of movement captions.
    """

    if 'movement_description_relationship' not in object_data:
        return []

    object_movement_data = object_data['movement_description_relationship']
    object_movement_speed_words = object_data['movement_speed_description']

    movement_captions = []
    for obj in combination['objects']:
        if 'movement' in obj:
            speed = obj['movement']['speed']
            if speed <= 0.25:
                speed_words = object_movement_speed_words['0.25']
            else:
                speed_words = object_movement_speed_words['0.5']

            speed_description = random.choice(speed_words)

            template = random.choice(object_movement_data)
            movement_description = template.replace('<object>', obj['name'])
            movement_description = movement_description.replace('<movement>', obj['movement']['direction'])
            movement_description = movement_description.replace('<speed>', f'{speed:.2f}')

            if '<speed_description>' in movement_description:
                movement_description = movement_description.replace('<speed_description>', speed_description)

            movement_captions.append(movement_description)

    return movement_captions


def generate_ontop_captions(combination: Dict[str, Any], ontop_data, object_data) -> List[str]:
    """
    Generate captions for objects being on top of each other based on the combination data.

    Args:
        combination (Dict[str, Any]): Combination data.
        ontop_data (str): Flag indicating whether to allow objects on top of each other.
        object_data (Dict[str, Any]): Object data containing ontop description relationships.

    Returns:
        List[str]: List of ontop captions.
    """
    if not ontop_data or 'ontop_description_relationship' not in object_data:
        return []

    ontop_captions = []
    placement_stacks = {}

    object_ontop_captions = object_data['ontop_description_relationship']

    # Maintain the original order of objects
    for obj in combination["objects"]:
        placement = obj["placement"]
        if placement not in placement_stacks:
            placement_stacks[placement] = []
        placement_stacks[placement].append(obj)

    # Generate captions for objects at the same placement
    for placement, objects in placement_stacks.items():
        if len(objects) > 1:
            for i in range(len(objects) - 1):
                below_obj = objects[i]
                above_obj = objects[i + 1]
                
                caption_template = random.choice(object_ontop_captions)
                
                # Always describe from bottom to top to maintain consistency
                caption = caption_template.replace("<object1>", above_obj['name']).replace("<object2>", below_obj['name'])
                
                ontop_captions.append(caption)

    return ontop_captions


def generate_camerafollow_captions(combination: Dict[str, Any], camera_data) -> List[str]:
    """
    Generate captions for camera following objects based on the combination data.

    Args:
        combination (Dict[str, Any]): Combination data.
        camera_data (Dict[str, Any]): Camera data containing camera follow options. 

    Returns:
        List[str]: List of camera follow captions.
    """
    if 'camera_follow' not in camera_data:
        return []
    
    camera_follow_options = camera_data['camera_follow']
    camera_follow_captions = []
    for obj in combination['objects']:
        if 'camera_follow' in obj:
            caption = random.choice(camera_follow_options)
            caption = caption.replace('<object>', obj['name'])
            camera_follow_captions.append(caption)        
    return camera_follow_captions
    

def generate_postprocessing(camera_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Generate postprocessing settings based on the camera data.
    Copy codeArgs:
        camera_data (Dict[str, Any]): Camera data.

    Returns:
        Dict[str, Any]: Postprocessing settings.
    """
    postprocessing = {}

    bloom_data = camera_data["postprocessing"]["bloom"]

    bloom_threshold = random.uniform(
        bloom_data["threshold_min"], bloom_data["threshold_max"]
    )
    bloom_intensity = random.uniform(
        bloom_data["intensity_min"], bloom_data["intensity_max"]
    )
    bloom_radius = random.uniform(bloom_data["radius_min"], bloom_data["radius_max"])

    bloom_type = "none"
    bloom_types = bloom_data["types"]
    for t in bloom_types.keys():
        if (
            bloom_intensity >= bloom_types[t]["intensity_min"]
            and bloom_intensity <= bloom_types[t]["intensity_max"]
        ):
            bloom_type = t
            break

    postprocessing["bloom"] = {
        "threshold": bloom_threshold,
        "intensity": bloom_intensity,
        "radius": bloom_radius,
        "type": bloom_type,
    }

    ssao_data = camera_data["postprocessing"]["ssao"]
    ssao_distance = random.uniform(ssao_data["distance_min"], ssao_data["distance_max"])
    ssao_factor = random.uniform(ssao_data["factor_min"], ssao_data["factor_max"])

    ssao_type = "none"
    for t in ssao_data["types"].keys():
        if (
            ssao_factor >= ssao_data["types"][t]["factor_min"]
            and ssao_factor <= ssao_data["types"][t]["factor_max"]
        ):
            ssao_type = t
            break

    postprocessing["ssao"] = {
        "distance": ssao_distance,
        "factor": ssao_factor,
        "type": ssao_type,
    }

    ssrr_data = camera_data["postprocessing"]["ssrr"]
    ssrr_max_roughness = random.uniform(
        ssrr_data["min_max_roughness"], ssrr_data["max_max_roughness"]
    )
    ssrr_thickness = random.uniform(
        ssrr_data["min_thickness"], ssrr_data["max_thickness"]
    )

    ssrr_type = "none"
    for t in ssrr_data["types"].keys():
        if (
            ssrr_max_roughness >= ssrr_data["types"][t]["max_roughness_min"]
            and ssrr_max_roughness <= ssrr_data["types"][t]["max_roughness_max"]
        ):
            ssrr_type = t
            break

    postprocessing["ssrr"] = {
        "max_roughness": ssrr_max_roughness,
        "thickness": ssrr_thickness,
        "type": ssrr_type,
    }

    motionblur_data = camera_data["postprocessing"]["motionblur"]
    motionblur_shutter_speed = random.uniform(
        motionblur_data["shutter_speed_min"], motionblur_data["shutter_speed_max"]
    )

    motionblur_type = "none"
    for t in motionblur_data["types"].keys():
        if (
            motionblur_shutter_speed >= motionblur_data["types"][t]["shutter_speed_min"]
            and motionblur_shutter_speed
            <= motionblur_data["types"][t]["shutter_speed_max"]
        ):
            motionblur_type = t
            break

    postprocessing["motionblur"] = {
        "shutter_speed": motionblur_shutter_speed,
        "type": motionblur_type,
    }
    return postprocessing


def generate_orientation(
    camera_data: Dict[str, Any],
    objects: List[Dict[str, Any]],
    background: Dict[str, Any],
) -> Dict[str, int]:
    """
    Generate camera orientation based on the camera data, objects, and background.
    Copy codeArgs:
        camera_data (Dict[str, Any]): Camera data.
        objects (List[Dict[str, Any]]): List of objects in the scene.
        background (Dict[str, Any]): Background information.

    Returns:
        Dict[str, int]: Camera orientation.
    """
    orientation_data = camera_data["orientation"]

    # Roll a number between orientation['yaw_min'] and orientation['yaw_max']
    yaw = random.randint(orientation_data["yaw_min"], orientation_data["yaw_max"])
    pitch = random.randint(orientation_data["pitch_min"], orientation_data["pitch_max"])

    # Check if the camera is going to be occluded by the objects
    # If so, re-roll the orientation until a non-occluded orientation is found
    while True:
        occluded = False
        for obj in objects[1:]:
            # Calculate the vector from the camera to the object
            # calculate the position of the camera on the unit circle from yaw
            camera_x = math.cos(math.radians(yaw))
            camera_y = math.sin(math.radians(yaw))

            camera_vector = [camera_x, camera_y, 0]

            # get the position of the object
            object_position = obj["transformed_position"]

            # normalize the object position, account for division by 0
            object_position = [
                a / max(1e-6, sum([b**2 for b in object_position])) ** 0.5
                for a in object_position
            ]

            # if the magnitude of the object position is 0, ignore it
            if sum([a**2 for a in object_position]) < 0.001:
                continue

            # dot product of the camera vector and the object position
            dot_product = sum([a * b for a, b in zip(camera_vector, object_position)])

            # calculate an angle padding on both sides of the object to use as a threshold
            angle = math.radians(15)
            # set the threshold to the dot product - cos of the angle
            threshold = math.cos(angle)

            if dot_product > threshold:
                occluded = True
                break

        if not occluded:
            break

        # Re-roll the orientation if occluded and try again
        yaw = random.randint(orientation_data["yaw_min"], orientation_data["yaw_max"])
        pitch = random.randint(
            orientation_data["pitch_min"], orientation_data["pitch_max"]
        )

    orientation = {
        "yaw": int(yaw),
        "pitch": int(pitch),
    }

    return orientation


def generate_framing(camera_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Generate camera framing based on the camera data.
    Copy codeArgs:
        camera_data (Dict[str, Any]): Camera data.

    Returns:
        Dict[str, Any]: Camera framing.
    """
    # Get the min_fov and max_fov across all framings
    fov_min = min([f["fov_min"] for f in camera_data["framings"]])
    fov_max = max([f["fov_max"] for f in camera_data["framings"]])

    # Randomly roll an FOV value between FOV_min and FOV_max
    fov = int(random.uniform(fov_min, fov_max))

    # Find the corresponding framing
    framing = None
    for f in camera_data["framings"]:
        if f["fov_min"] <= fov <= f["fov_max"]:
            framing = f
            break

    # Derive a coverage_factor between coverage_factor_min and coverage_factor_max
    coverage_factor = random.uniform(
        framing["coverage_factor_min"], framing["coverage_factor_max"]
    )

    framing = {
        "fov": fov,
        "coverage_factor": coverage_factor,
        "name": framing["name"],
    }
    return framing


def generate_animation(camera_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Generate camera animation based on the camera data.
    Copy codeArgs:
        camera_data (Dict[str, Any]): Camera data.

    Returns:
        Dict[str, Any]: Camera animation.
    """
    animation = random.choice(camera_data["animations"])
    animation = animation.copy()
    animation["speed_factor"] = random.uniform(0.5, 2.0)
    animation.pop("descriptions", None)
    return animation


def generate_background(
    background_dict, background_names, background_weights
) -> Dict[str, Any]:
    """
    Generate a random background.

    Args:
        background_dict: Background data.
        background_names: List of background names.
        background_weigh ts: List of background

    Returns:
        Dict[str, Any]: Generated background.
    """
    chosen_background = random.choices(background_names, weights=background_weights)[0]
    # Get the keys from the chosen background
    background_keys = list(background_dict[chosen_background].keys())
    background_id = random.choice(background_keys)
    bg = background_dict[chosen_background][background_id]

    background = {
        "name": bg["name"],
        "url": bg["url"],
        "id": background_id,
        "from": chosen_background,
    }

    return background


def calculate_transformed_positions(combination):
    print("Input to calculate_transformed_positions:")
    print(json.dumps(combination, indent=2))

    # Extract objects and orientation from the combination
    objects = combination.get('objects', [])
    orientation = combination.get('orientation', {})
    
    if 'orientation' not in combination:
        raise KeyError("'orientation' key is missing from the combination")
    
    if 'yaw' not in orientation:
        raise KeyError("'yaw' key is missing from the orientation")

    yaw = orientation['yaw']

    # Adjust positions based on the camera's yaw
    adjusted_objects = adjust_positions(objects, yaw)

    # Update the objects in the combination with their transformed positions
    for original_obj, adjusted_obj in zip(objects, adjusted_objects):
        original_obj['transformed_position'] = adjusted_obj['transformed_position']

    return combination


def generate_stage(texture_data) -> Dict[str, Any]:
    """
    Generate a random stage.

    Args:
        texture_data: Texture data.

    Returns:
        Dict[str, Any]: Generated stage.
    """
    texture_names = list(texture_data.keys())
    texture_weights = [len(texture_data[name]["maps"]) for name in texture_names]
    chosen_texture = random.choices(texture_names, weights=texture_weights)[0]
    maps = texture_data[chosen_texture]["maps"]

    material = {
        "name": texture_data[chosen_texture]["name"],
        "maps": maps,
    }
    stage = {
        "material": material,
        "uv_scale": [random.uniform(0.8, 1.2), random.uniform(0.8, 1.2)],
        "uv_rotation": random.uniform(0, 360),
    }
    return stage


def add_camera_follow(objects, camera_follow):
    """
    Add camera follow to objects.

    Args:
        objects: List of objects.
        camera_follow: Camera follow flag.

    Returns:
        List[Dict[str, Any]]: List of objects with camera follow.
    """
    if camera_follow:  
        # how many objects in array
        num_objects = len(objects)
        # randomly select an object to follow
        random_index = random.randint(0, num_objects - 1)
        objects[random_index]["camera_follow"] = {"follow": True}

    return objects


def generate_object_list(objects):
    return ", ".join([obj["description"] for obj in objects])


def generate_combinations(
    camera_data: Dict[str, Any],
    count: int,
    seed: Optional[int],
    dataset_names: List[str],
    dataset_weights: List[int],
    object_data: Dict[str, Any],
    dataset_dict: Dict[str, Any],
    captions_data: Dict[str, Any],
    background_dict: Dict[str, Any],
    background_names: List[str],
    background_weights: List[int],
    texture_data: Dict[str, Any],
    movement: bool = False,  
    max_speed: float = 0.5,
    ontop_data: bool = False,
    camera_follow: bool = False,
    random_flag: bool = False,
) -> Dict[str, Any]:
    if seed is None:
        seed = -1
    random.seed(seed)

    combinations = []

    for i in range(count):
        combination = {"index": i}

        if random_flag:
            movement = random.choice([True, False])
            ontop = random.choice([True, False])
            camera_follow = random.choice([True, False])
            max_speed = random.uniform(0.1, 0.5)

        # Generate objects
        combination["objects_caption"] = "Object caption:"
        objects = generate_objects(
            object_data, dataset_names, dataset_weights, dataset_dict, captions_data, ontop_data
        )
        combination["objects"] = objects
        object_list = generate_object_list(objects)
        object_list_intro = object_data["object_list_intro"]
        intro = random.choice(object_list_intro)
        combination["objects_caption"] = intro.replace("<object_list>", object_list)

        # Generate background
        combination["background_caption"] = "Scene background:"
        background = generate_background(
            background_dict, background_names, background_weights
        )
        combination["background"] = background
        combination["background_caption"] += f" The landscape is {background['name']}."

        # Calculate transformed positions
        adjusted_objects = adjust_positions(objects, random.randint(0, 360))
        for obj, adjusted_obj in zip(objects, adjusted_objects):
            obj["transformed_position"] = adjusted_obj["transformed_position"]

        # Generate orientation and framing
        combination["orientation_caption"] = "Camera orientation:"
        orientation = generate_orientation(camera_data, objects, background)
        framing = generate_framing(camera_data)
        combination["orientation"] = orientation

        combination["framing_caption"] = "Camera framing:"
        combination["framing"] = framing

        # Generate animation
        combination["animation_caption"] = "Camera animation:"
        animation = generate_animation(camera_data)
        combination["animation"] = animation

        # Generate stage
        combination["stage_caption"] = "Scene stage:"
        stage = generate_stage(texture_data)
        combination["stage"] = stage

        # Generate postprocessing
        combination["postprocessing_caption"] = "Post-processing effects:"
        postprocessing = generate_postprocessing(camera_data)
        combination["postprocessing"] = postprocessing

        # Add movement to objects
        if movement:
            objects = add_movement_to_objects(objects, movement, max_speed)
        else:
            combination["no_movement"] = True

        # Add camera follow
        if camera_follow:
            objects = add_camera_follow(objects, camera_follow)

        # Generate captions
        caption_parts = []

        # Object captions
        object_name_descriptions = generate_object_name_description_captions(
            combination, object_data
        )
        caption_parts.append(object_name_descriptions)

        # Relationship captions
        scene_relationship_description = generate_relationship_captions(combination)
        scene_relationship_description_str = " ".join(scene_relationship_description)
        caption_parts.append(scene_relationship_description_str)
        combination["objects_caption"] += scene_relationship_description_str
        # Ontop captions
        ontop_captions = generate_ontop_captions(combination, ontop_data, object_data)
        caption_parts.extend(ontop_captions)
        combination["objects_caption"] += " " + " ".join(ontop_captions)
        # Camera follow captions
        camerafollow_captions = generate_camerafollow_captions(combination, camera_data)
        caption_parts.extend(camerafollow_captions)
        combination["animation_caption"] += " " + " ".join(camerafollow_captions)
        # Movement captions
        movement_captions = generate_movement_captions(combination, object_data)
        caption_parts.extend(movement_captions)
        combination["objects_caption"] += " " + " ".join(movement_captions)

        # Orientation caption
        orientation_text = generate_orientation_caption(camera_data, combination)
        caption_parts.append(orientation_text)
        combination["orientation_caption"] += " " + orientation_text

        # Framing caption
        framing_caption = generate_framing_caption(camera_data, combination)
        caption_parts.append(framing_caption)
        combination["framing_caption"] += " " + framing_caption
        # FOV caption
        fov_caption = generate_fov_caption(combination)
        caption_parts.append(fov_caption)
        combination["framing_caption"] += " " + fov_caption

        # Postprocessing caption
        postprocessing_caption = generate_postprocessing_caption(combination, camera_data)
        caption_parts.append(postprocessing_caption)
        combination["postprocessing_caption"] += " " + postprocessing_caption

        # Stage captions
        stage_captions = generate_stage_captions(combination)
        caption_parts.extend(stage_captions)
        combination["stage_caption"] += " " + " ".join(stage_captions)

        # Animation captions
        animation_captions = generate_animation_captions(combination, camera_data)
        caption_parts.extend(animation_captions)
        combination["animation_caption"] += " " + " ".join(animation_captions)

        # Generate overall caption
        combination["caption"] = " ".join(caption_parts).strip()

        combinations.append(combination)

    data = {"seed": seed, "count": count, "combinations": combinations}

    return data


def generate_objects(
    object_data, dataset_names, dataset_weights, dataset_dict, captions_data, ontop_data
) -> List[Dict[str, Any]]:
    """
    Generate a list of random objects.

    Args:
        object_data (Dict[str, Any]): Object data.
        dataset_names (List[str]): List of dataset names.
        dataset_weights (List[int]): List of dataset weights.
        dataset_dict (Dict[str, Any]): Dataset dictionary.
        captions_data (Dict[str, Any]): Captions data.
        ontop_data (str): Flag indicating whether to allow objects on top of each other.

    Returns:
        List[Dict[str, Any]]: List of generated objects.
    """
    chosen_dataset = "cap3d"

    if chosen_dataset not in dataset_dict:
        raise KeyError(f"Dataset '{chosen_dataset}' not found in dataset_dict")

    # Randomly generate max_number_of_objects
    max_number_of_objects = parse_args().max_number_of_objects
    number_of_objects = random.randint(1, max_number_of_objects)

    object_scales = object_data["scales"]

    # Scale values
    scale_values = [scale["factor"] for scale in object_scales.values()]

    # Create simple triangular distribution based on scale_values
    len_scale_values = len(scale_values)
    mid_point = len_scale_values // 2
    if len_scale_values % 2 == 0:
        weights = [i + 1 for i in range(mid_point)] + [
            mid_point - i for i in range(mid_point)
        ]
    else:
        weights = (
            [i + 1 for i in range(mid_point)]
            + [mid_point + 1]
            + [mid_point - i for i in range(mid_point)]
        )

    total_weight = sum(weights)
    normalized_weights = [w / total_weight for w in weights]

    objects = []
    positions_taken = set()
    for i in range(number_of_objects):
        object_uid = random.choice(dataset_dict[chosen_dataset])
        object_description = captions_data[object_uid]
        object_description = captions_data[object_uid].rstrip('.')  # Remove trailing period
        
        scale_choice = random.choices(
            list(object_scales.items()), weights=normalized_weights, k=1
        )[0]
        scale_key = scale_choice[0]
        scale_value = scale_choice[1]

        scale = {
            "factor": scale_value["factor"],
            "name": scale_key,
            "name_synonym": random.choice(scale_value["names"]),
        }

        if i == 0:
            placement = 4  # Ensure the first object is always placed at position 4
            positions_taken.add(placement)
        else:
            possible_positions = [
                pos for pos in range(0, 9) if pos not in positions_taken or ontop_data
            ]
            placement = random.choice(possible_positions)
            positions_taken.add(placement)

        object = {
            "name": object_description,  # Use the description directly
            "uid": object_uid,
            "description": object_description,  # Use the description directly
            "placement": placement,
            "from": chosen_dataset,
            "scale": scale,
        }

        objects.append(object)

    return objects


if __name__ == "__main__":
    console = Console()
    logging.basicConfig(level=logging.INFO, format='%(message)s')
    logging.getLogger().propagate = False
    args = parse_args()

    simverse_ascii = """

 ░▒▓███████▓▒░▒▓█▓▒░▒▓██████████████▓▒░░▒▓█▓▒░░▒▓█▓▒░▒▓████████▓▒░▒▓███████▓▒░ ░▒▓███████▓▒░▒▓████████▓▒░ 
░▒▓█▓▒░      ░▒▓█▓▒░▒▓█▓▒░░▒▓█▓▒░░▒▓█▓▒░▒▓█▓▒░░▒▓█▓▒░▒▓█▓▒░      ░▒▓█▓▒░░▒▓█▓▒░▒▓█▓▒░      ░▒▓█▓▒░        
░▒▓█▓▒░      ░▒▓█▓▒░▒▓█▓▒░░▒▓█▓▒░░▒▓█▓▒░░▒▓█▓▒▒▓█▓▒░░▒▓█▓▒░      ░▒▓█▓▒░░▒▓█▓▒░▒▓█▓▒░      ░▒▓█▓▒░        
 ░▒▓██████▓▒░░▒▓█▓▒░▒▓█▓▒░░▒▓█▓▒░░▒▓█▓▒░░▒▓█▓▒▒▓█▓▒░░▒▓██████▓▒░ ░▒▓███████▓▒░ ░▒▓██████▓▒░░▒▓██████▓▒░   
       ░▒▓█▓▒░▒▓█▓▒░▒▓█▓▒░░▒▓█▓▒░░▒▓█▓▒░ ░▒▓█▓▓█▓▒░ ░▒▓█▓▒░      ░▒▓█▓▒░░▒▓█▓▒░      ░▒▓█▓▒░▒▓█▓▒░        
       ░▒▓█▓▒░▒▓█▓▒░▒▓█▓▒░░▒▓█▓▒░░▒▓█▓▒░ ░▒▓█▓▓█▓▒░ ░▒▓█▓▒░      ░▒▓█▓▒░░▒▓█▓▒░      ░▒▓█▓▒░▒▓█▓▒░        
░▒▓███████▓▒░░▒▓█▓▒░▒▓█▓▒░░▒▓█▓▒░░▒▓█▓▒░  ░▒▓██▓▒░  ░▒▓████████▓▒░▒▓█▓▒░░▒▓█▓▒░▒▓███████▓▒░░▒▓████████▓▒░ 
                                                                                                          
    """

    console.print(simverse_ascii)

    # Load only cap3d dataset
    cap3d_data_path = args.cap3d_captions_path
    cap3d_data = read_json_file(cap3d_data_path)
    dataset_dict = {"cap3d": list(cap3d_data.keys())}

    tasks = [
        ("Loading object data", args.object_data_path),
        ("Loading stage data", args.stage_data_path),
        ("Loading texture data", args.texture_data_path),
        ("Loading camera data", args.camera_file_path)
    ]

    data_dict = {}

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console
    ) as progress:
        task_load = progress.add_task("[cyan]Processing tasks...", total=len(tasks))

        for task, path in tasks:
            data_dict[path] = read_json_file(path)
            progress.update(task_load, advance=1)

        object_data = data_dict[args.object_data_path]
        captions_data = read_json_file(args.cap3d_captions_path)
        stage_data = data_dict[args.stage_data_path]
        texture_data = data_dict[args.texture_data_path]
        camera_data = data_dict[args.camera_file_path]
        movement_data = args.movement
        ontop_data = args.ontop
        speed = 1.0
        camera_follow = args.camera_follow
        random_flag = args.random 

        backgrounds = read_json_file(args.datasets_path)["backgrounds"]
        background_dict = {}
        bg_task = progress.add_task("[cyan]Loading backgrounds...", total=len(backgrounds))

        for bg in backgrounds:
            bg_path = os.path.join(args.simdata_path, bg + ".json")
            if os.path.exists(bg_path):
                background_data = read_json_file(bg_path)
                background_dict[bg] = background_data
            progress.update(bg_task, advance=1)

    background_names = list(background_dict.keys())
    background_weights = [len(background_dict[name]) for name in background_names]

    combinations = generate_combinations(
        camera_data,
        args.count,
        args.seed,
        ["cap3d"],  # Use only the cap3d dataset
        [1],  # Weight for the cap3d dataset
        object_data,
        dataset_dict,
        captions_data,
        background_dict,
        background_names,
        background_weights,
        texture_data,
        movement_data,
        speed,
        ontop_data,
        camera_follow,
        random_flag
    )

    # Write to JSON file
    with open(args.output_path, "w") as f:
        json.dump(combinations, f, indent=4)

    console.print(f"[bold green]✓ Combinations have been successfully written to {args.output_path}")

```````

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/postprocessing.py`:

```````py
import bpy
from typing import Any


def setup_compositor_for_black_and_white(context: bpy.types.Context) -> None:
    """
    Sets up the compositor for a black and white effect using Blender's Eevee engine.
    Assumes that the context provided is valid and that the scene uses Eevee.

    Args:
        context (bpy.types.Context): The Blender context.

    Returns:
        None
    """
    # Ensure the use of nodes in the scene's compositing.
    scene = context.scene
    scene.use_nodes = True
    tree = scene.node_tree

    # Create necessary nodes
    render_layers = tree.nodes.new(type="CompositorNodeRLayers")
    hue_sat = tree.nodes.new(type="CompositorNodeHueSat")
    composite = tree.nodes.new(
        type="CompositorNodeComposite"
    )  # Ensure there's a composite node

    # Position nodes
    render_layers.location = (-300, 0)
    hue_sat.location = (100, 0)
    composite.location = (300, 0)

    # Configure Hue Sat node for desaturation
    hue_sat.inputs["Saturation"].default_value = (
        0  # Reduce saturation to zero to get grayscale
    )
    # increase contrast
    hue_sat.inputs["Value"].default_value = 2.0

    # Link nodes
    links = tree.links
    links.new(render_layers.outputs["Image"], hue_sat.inputs["Image"])
    links.new(
        hue_sat.outputs["Image"], composite.inputs["Image"]
    )  # Direct output to the composite node


def setup_compositor_for_cel_shading(context: bpy.types.Context) -> None:
    """
    Sets up the compositor for a cel shading effect using Blender's Eevee engine.
    The node setup is based on the theory of using normal and diffuse passes to create
    a stylized, non-photorealistic look with subtle transitions between shadows and highlights.
    Assumes the context provided is valid and that the scene uses Eevee.

    Args:
        context (bpy.types.Context): The Blender context.

    Returns:
        None
    """
    # Ensure the use of nodes in the scene's compositing
    scene: bpy.types.Scene = context.scene
    scene.use_nodes = True
    tree: bpy.types.NodeTree = scene.node_tree

    # Create necessary nodes for compositing
    # Render Layers Node: Provides access to the render passes
    # Normal Node: Converts the normal pass into a usable format
    # Color Ramp Nodes: Control the shading and highlight areas
    # Mix RGB Nodes: Combine the shading and highlight with the diffuse color
    # Composite Node: The final output node
    render_layers: Any = tree.nodes.new(type="CompositorNodeRLayers")
    normal_node: Any = tree.nodes.new(type="CompositorNodeNormal")
    color_ramp_shadow: Any = tree.nodes.new(type="CompositorNodeValToRGB")
    color_ramp_highlight: Any = tree.nodes.new(type="CompositorNodeValToRGB")
    mix_rgb_shadow: Any = tree.nodes.new(type="CompositorNodeMixRGB")
    mix_rgb_highlight: Any = tree.nodes.new(type="CompositorNodeMixRGB")
    alpha_over: Any = tree.nodes.new(
        type="CompositorNodeAlphaOver"
    )  # Add an Alpha Over node
    composite: Any = tree.nodes.new(type="CompositorNodeComposite")

    # Configure Mix RGB nodes
    # Multiply blend mode for shadows to darken the diffuse color slightly
    # Overlay blend mode for highlights to create subtle bright highlights
    # Reference: https://docs.blender.org/manual/en/latest/compositing/types/color/mix.html
    mix_rgb_shadow.blend_type = "MULTIPLY"
    mix_rgb_highlight.blend_type = "OVERLAY"
    mix_rgb_shadow.use_clamp = True
    mix_rgb_highlight.use_clamp = True

    # Configure Shadow Color Ramp
    color_ramp_shadow.color_ramp.interpolation = "EASE"
    color_ramp_shadow.color_ramp.elements[0].position = 0.5
    color_ramp_shadow.color_ramp.elements[1].position = 0.8
    color_ramp_shadow.color_ramp.elements[0].color = (0.5, 0.5, 0.5, 1)  # Mid Gray
    color_ramp_shadow.color_ramp.elements[1].color = (1, 1, 1, 1)  # White

    # Configure Highlight Color Ramp
    color_ramp_highlight.color_ramp.interpolation = "EASE"
    color_ramp_highlight.color_ramp.elements[0].position = 0.8
    color_ramp_highlight.color_ramp.elements[1].position = 0.95
    color_ramp_highlight.color_ramp.elements[0].color = (0, 0, 0, 1)  # Black
    color_ramp_highlight.color_ramp.elements[1].color = (1, 1, 1, 1)  # White

    # Adjust the Mix RGB nodes
    mix_rgb_shadow.blend_type = "MULTIPLY"
    mix_rgb_shadow.inputs[0].default_value = 1.0  # Reduce the shadow intensity

    mix_rgb_highlight.blend_type = (
        "SCREEN"  # Change to 'SCREEN' for better highlight blending
    )
    mix_rgb_highlight.inputs[0].default_value = 0.5  # Reduce the highlight intensity

    # Link nodes
    links: Any = tree.links
    # Connect Normal pass to Normal node for shading information
    links.new(render_layers.outputs["Normal"], normal_node.inputs["Normal"])
    # Connect Normal node to Shadow Color Ramp for shadow intensity control
    links.new(normal_node.outputs["Dot"], color_ramp_shadow.inputs["Fac"])
    # Connect Normal node to Highlight Color Ramp for highlight intensity control
    links.new(normal_node.outputs["Dot"], color_ramp_highlight.inputs["Fac"])
    # Connect Shadow Color Ramp to Mix RGB Shadow node for shadow color blending
    links.new(color_ramp_shadow.outputs["Image"], mix_rgb_shadow.inputs[2])
    # Connect Diffuse pass to Mix RGB Shadow node as the base color
    links.new(render_layers.outputs["Image"], mix_rgb_shadow.inputs[1])
    # Connect Mix RGB Shadow to Mix RGB Highlight for combining shadows with the base color
    links.new(mix_rgb_shadow.outputs["Image"], mix_rgb_highlight.inputs[1])
    # Connect Highlight Color Ramp to Mix RGB Highlight for adding highlights
    links.new(color_ramp_highlight.outputs["Image"], mix_rgb_highlight.inputs[2])
    # Connect Mix RGB Highlight to Composite node for final output
    links.new(
        mix_rgb_highlight.outputs["Image"], alpha_over.inputs[2]
    )  # Plug the cel-shaded result into the foreground input of Alpha Over
    links.new(
        render_layers.outputs["Env"], alpha_over.inputs[1]
    )  # Plug the environment pass into the background input of Alpha Over
    links.new(
        alpha_over.outputs["Image"], composite.inputs["Image"]
    )  # Plug the Alpha Over output into the Composite node


def setup_compositor_for_depth(context: bpy.types.Context) -> None:
    """
    Sets up the compositor for rendering a depth map using Blender's Eevee engine.
    Assumes that the context provided is valid and that the scene uses Eevee.

    Args:
        context (bpy.types.Context): The Blender context.

    Returns:
        None
    """
    # Ensure the use of nodes in the scene's compositing.
    scene: bpy.types.Scene = context.scene
    scene.use_nodes = True
    tree: bpy.types.NodeTree = scene.node_tree
    # Create necessary nodes
    render_layers: Any = tree.nodes.new(type="CompositorNodeRLayers")
    normalize: Any = tree.nodes.new(type="CompositorNodeNormalize")
    composite: Any = tree.nodes.new(
        type="CompositorNodeComposite"
    )  # Ensure there's a composite node

    # Position nodes
    render_layers.location = (-300, 0)
    normalize.location = (0, 0)
    composite.location = (300, 0)

    # Enable the Depth pass in the View Layer properties
    view_layer: bpy.types.ViewLayer = context.view_layer
    view_layer.use_pass_z = True

    # Link nodes
    links: Any = tree.links
    links.new(render_layers.outputs["Depth"], normalize.inputs["Value"])
    links.new(
        normalize.outputs["Value"], composite.inputs["Image"]
    )  # Direct output to the composite node


effects = {
    "black_and_white": setup_compositor_for_black_and_white,
    "cel_shading": setup_compositor_for_cel_shading,
    "depth": setup_compositor_for_depth,
}


def enable_effect(context: bpy.types.Context, effect_name: str) -> None:
    """
    Enables the cel shading effect in the compositor.

    Args:
        context (bpy.types.Context): The Blender context.
        effect_name (str): The name of the effect to enable.

    Returns:
        None
    """
    # Make sure compositor use is turned on
    context.scene.render.use_compositing = True
    context.scene.use_nodes = True
    tree: bpy.types.NodeTree = context.scene.node_tree

    # Clear existing nodes
    for node in tree.nodes:
        tree.nodes.remove(node)

    # Enable Normal Pass and Diffuse Pass
    # These passes provide the necessary information for shading and color
    # Reference: https://docs.blender.org/manual/en/latest/render/layers/passes.html
    view_layer: bpy.types.ViewLayer = context.view_layer
    view_layer.use_pass_normal = True
    view_layer.use_pass_diffuse_color = True
    view_layer.use_pass_environment = True  # Enable the environment pass
    view_layer.use_pass_z = True

    # get the function for the effect
    effect_function: Any = effects.get(effect_name)

    # Set up the nodes for cel shading
    effect_function(context)

```````

`/Users/ericsheen/Desktop/DeepAI_Research/Simverse/simian/vendor/objaverse.py`:

```````py
"""A package for downloading and processing Objaverse."""

import glob
import gzip
import json
import logging
import multiprocessing
import os
import urllib.request
import warnings
from typing import Any, Dict, List, Optional, Tuple

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)


BASE_PATH = os.path.join(os.path.expanduser("~"), ".objaverse")

__version__ = "0.1.7"
_VERSIONED_PATH = os.path.join(BASE_PATH, "hf-objaverse-v1")


def load_annotations(uids: Optional[List[str]] = None) -> Dict[str, Any]:
    """Load the full metadata of all objects in the dataset.

    Args:
        uids: A list of uids with which to load metadata. If None, it loads
        the metadata for all uids.

    Returns:
        A dictionary mapping the uid to the metadata.
    """
    metadata_path = os.path.join(_VERSIONED_PATH, "metadata")
    object_paths = _load_object_paths()
    dir_ids = (
        set(object_paths[uid].split("/")[1] for uid in uids)
        if uids is not None
        else [f"{i // 1000:03d}-{i % 1000:03d}" for i in range(160)]
    )
    out = {}
    for i_id in dir_ids:
        json_file = f"{i_id}.json.gz"
        local_path = os.path.join(metadata_path, json_file)
        if not os.path.exists(local_path):
            hf_url = f"https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/{i_id}.json.gz"
            # wget the file and put it in local_path
            os.makedirs(os.path.dirname(local_path), exist_ok=True)
            urllib.request.urlretrieve(hf_url, local_path)
        with gzip.open(local_path, "rb") as f:
            data = json.load(f)
        if uids is not None:
            data = {uid: data[uid] for uid in uids if uid in data}
        out.update(data)
        if uids is not None and len(out) == len(uids):
            break
    return out


def _load_object_paths() -> Dict[str, str]:
    """Load the object paths from the dataset.

    The object paths specify the location of where the object is located
    in the Hugging Face repo.

    Returns:
        A dictionary mapping the uid to the object path.
    """
    object_paths_file = "object-paths.json.gz"
    local_path = os.path.join(_VERSIONED_PATH, object_paths_file)
    if not os.path.exists(local_path):
        hf_url = f"https://huggingface.co/datasets/allenai/objaverse/resolve/main/{object_paths_file}"
        # wget the file and put it in local_path
        os.makedirs(os.path.dirname(local_path), exist_ok=True)
        urllib.request.urlretrieve(hf_url, local_path)
    with gzip.open(local_path, "rb") as f:
        object_paths = json.load(f)
    return object_paths


def load_uids() -> List[str]:
    """Load the uids from the dataset.

    Returns:
        A list of uids.
    """
    return list(_load_object_paths().keys())


def _download_object(
    uid: str,
    object_path: str,
    total_downloads: float,
    start_file_count: int,
) -> Tuple[str, str]:
    """Download the object for the given uid.

    Args:
        uid: The uid of the object to load.
        object_path: The path to the object in the Hugging Face repo.

    Returns:
        The local path of where the object was downloaded.
    """
    local_path = os.path.join(_VERSIONED_PATH, object_path)
    tmp_local_path = os.path.join(_VERSIONED_PATH, object_path + ".tmp")
    hf_url = (
        f"https://huggingface.co/datasets/allenai/objaverse/resolve/main/{object_path}"
    )
    # wget the file and put it in local_path
    os.makedirs(os.path.dirname(tmp_local_path), exist_ok=True)
    urllib.request.urlretrieve(hf_url, tmp_local_path)

    os.rename(tmp_local_path, local_path)

    files = glob.glob(os.path.join(_VERSIONED_PATH, "glbs", "*", "*.glb"))
    # logger.info(
    #     f"Downloaded {len(files) - start_file_count}/{total_downloads} objects",
    # )

    return uid, local_path


def load_objects(uids: List[str], download_processes: int = 1) -> Dict[str, str]:
    """Return the path to the object files for the given uids.

    If the object is not already downloaded, it will be downloaded.

    Args:
        uids: A list of uids.
        download_processes: The number of processes to use to download the objects.

    Returns:
        A dictionary mapping the object uid to the local path of where the object
        downloaded.
    """
    object_paths = _load_object_paths()
    out = {}
    if download_processes == 1:
        uids_to_download = []
        for uid in uids:
            if uid.endswith(".glb"):
                uid = uid[:-4]
            if uid not in object_paths:
                warnings.warn(f"Could not find object with uid {uid}. Skipping it.")
                continue
            object_path = object_paths[uid]
            local_path = os.path.join(_VERSIONED_PATH, object_path)
            if os.path.exists(local_path):
                out[uid] = local_path
                continue
            uids_to_download.append((uid, object_path))
        if len(uids_to_download) == 0:
            return out
        start_file_count = len(
            glob.glob(os.path.join(_VERSIONED_PATH, "glbs", "*", "*.glb"))
        )
        for uid, object_path in uids_to_download:
            uid, local_path = _download_object(
                uid, object_path, len(uids_to_download), start_file_count
            )
            out[uid] = local_path
    else:
        args = []
        for uid in uids:
            if uid.endswith(".glb"):
                uid = uid[:-4]
            if uid not in object_paths:
                warnings.warn(f"Could not find object with uid {uid}. Skipping it.")
                continue
            object_path = object_paths[uid]
            local_path = os.path.join(_VERSIONED_PATH, object_path)
            if not os.path.exists(local_path):
                args.append((uid, object_paths[uid]))
            else:
                out[uid] = local_path
        if len(args) == 0:
            return out
        # logger.info(
        #     f"starting download of {len(args)} objects with {download_processes} processes"
        # )
        start_file_count = len(
            glob.glob(os.path.join(_VERSIONED_PATH, "glbs", "*", "*.glb"))
        )
        args_list = [(*arg, len(args), start_file_count) for arg in args]
        with multiprocessing.Pool(download_processes) as pool:
            r = pool.starmap(_download_object, args_list)
            for uid, local_path in r:
                out[uid] = local_path
    return out


def load_lvis_annotations() -> Dict[str, List[str]]:
    """Load the LVIS annotations.

    If the annotations are not already downloaded, they will be downloaded.

    Returns:
        A dictionary mapping the LVIS category to the list of uids in that category.
    """
    hf_url = "https://huggingface.co/datasets/allenai/objaverse/resolve/main/lvis-annotations.json.gz"
    local_path = os.path.join(_VERSIONED_PATH, "lvis-annotations.json.gz")
    os.makedirs(os.path.dirname(local_path), exist_ok=True)
    if not os.path.exists(local_path):
        urllib.request.urlretrieve(hf_url, local_path)
    with gzip.open(local_path, "rb") as f:
        lvis_annotations = json.load(f)
    return lvis_annotations

```````